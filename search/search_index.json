{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"INFO-I590 Data Visualization (Spring 2025)","text":""},{"location":"#instructor-contact-information","title":"Instructor Contact Information","text":""},{"location":"#course-instructor","title":"Course Instructor","text":"<p>Filipi Nascimento Silva Email: filsilva@iu.edu Classes: Mondays and Wednesdays from 3:55 PM to 5:10 PM Office Hours: During regular lecture times &amp; TBD</p>"},{"location":"#teaching-assistants-tas","title":"Teaching Assistants (TAs)","text":"<p>The AIs are available as day &amp; time, contact, and location (or zoom link). Please see their schedules and contact information below.</p> <ul> <li>Shalmoli Ghosh \u2013 See Canvas     Office hours: See Canvas     Zoom: See Canvas</li> <li>Anindya Mondal \u2013 See Canvas     Office hours: See Canvas     Zoom: See Canvas</li> <li>Weihong Qi \u2013 See Canvas     Office hours: See Canvas     Zoom: See Canvas</li> <li>Vincent Wong \u2013 See Canvas     Office hours: See Canvas     Zoom: See Canvas</li> </ul>"},{"location":"#resources","title":"Resources","text":"<p>Github repository: https://github.com/filipinascimento/dataviz Slack Channel: https://join.slack.com/t/luddy-dataviz/signup</p>"},{"location":"#course-overview","title":"Course Overview","text":"<p>Welcome to our exciting journey into the world of Data Visualization! This course is designed to provide you with a solid understanding of visualization fundamentals, emphasizing practical skills and real-world applications through Python and Javascript.</p> <p>Visualizing data is an essential skill for researchers, data scientists, analysts, journalists, and professionals in various fields dealing with information. It is an important tool for understanding complex datasets and making data-driven decisions. Also, it can play an important role in troubleshooting issues within complex data analysis pipelines or AI models. Furthermore, data visualization serves as a powerful medium for communicating data-driven insights and narratives with colleagues or broader audiences.</p> <p>This course is heavily based on Prof. Yong-Yeol \u201cYY\u201d Ahn's Data Visualization course (http://yongyeol.com/).</p>"},{"location":"#communication","title":"Communication","text":"<p>We all get more email than we can read every day. If you email the instructor concerning this course, it is VITAL that you begin the subject line with \"I590:\" and follow this tag with a meaningful subject line (e.g., \u201cI590: questions about project\u201d); otherwise your message may not get read as soon as you might hope.</p> <p>We will use GitHub for all course materials, assignments, and for the final project. Slack will be used for communication, discussions, and to provide feedback on assignments and projects. We encourage you to actively participate in discussions, ask questions, and share your thoughts and ideas. Please be respectful and considerate of others' opinions and ideas, also do not post your personal information or sensitive data in the Slack channel.</p> <p>Slack Channel for the course: https://join.slack.com/t/luddy-dataviz/signup</p> <p>Canvas and Email also work for communication but with a certain delay. We encourage you to use Slack for faster communication.</p> <p>If you have suggestions, criticism or feedback on improving the course, please feel free to share them with us. You can use Slack or use the anonymous feedback form: https://forms.gle/24fNybMVmDGBJi9A8</p>"},{"location":"#objectives","title":"Objectives","text":"<p>By the end of this course, you are expected to be able to:</p> <ul> <li>Prepare and manipulate basic data types, such as numerical, categorical, and textual data.</li> <li>Explain and summarize data using descriptive statistics.</li> <li>Analyze data using exploratory visualization techniques.</li> <li>Critically analyze and improve visualizations based on principles such as human perception, design, visualization techniques, technology, and ethics.</li> <li>Understand how visualizations can be misleading or misrepresent data.</li> <li>Use ethical considerations when creating and deploying visualizations, such as fairness, accuracy, transparency, accessibility and diversity.</li> <li>Use modern libraries and tools for creating interactive visualizations.</li> <li>Integrate visualization into data analysis and machine learning pipelines.</li> <li>Prepare narrative visualizations to communicate data-driven insights and stories.</li> <li>Create and deploy interactive visualizations to the web.</li> </ul> <p>You will showcase your learned skills by undertaking a course project, in which you will develop a visualization to reveal insights from real-world datasets. This project will require detailed documentation of each step involved in its development, from initial concept to final execution.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>In this course, we will primarily use Python for data analysis and visualization tasks. Thus, you are required to have a good understanding of algorithms and practical experience with Python. We also expect you to have some level of familiarity with web technologies (HTML, CSS, and JavaScript), which will be essential for creating and deploying interactive visualizations. You are encouraged to also have a basic understanding of statistics and probability, as well as notions of 2D geometry and linear algebra.</p> <p>For self-assessment, please visit the following link: http://bit.ly/dvizselfassess (created by YY Ahn). Contact the instructor if you are uncertain about your background.</p>"},{"location":"#course-structure","title":"Course structure","text":"<p>Each week, we'll explore different topics in Data Visualization, starting from the basics and gradually moving to more advanced concepts. The course is designed to be hands-on, with a mix of theory, practical exercises, and projects.</p> <p>Here's a tentative outline of the course for Fall 2024:</p> <ul> <li> <p>Week 1 (Jan 13 and Jan 15): Introduction to Data Visualization  </p> <ul> <li>Overview of Data Visualization  </li> <li>The Importance of Data Visualization  </li> <li>Historical overview  </li> <li>Course summary and expectations  </li> <li>Famous visualizations and their impact</li> </ul> </li> <li> <p>Week 2 (Jan 22): Prerequisites and Recap of Fundamentals  </p> <ul> <li>Demonstration of visualizations, tools, and libraries  </li> <li>Git and Github  </li> <li>Python basics (Jupyter, Pandas)  </li> <li>Simple statistics  </li> <li>Modern Javascript, HTML, and CSS basics  </li> <li>Setup of a web development environment  </li> <li>Canvas and SVG  </li> <li>Basics of 2D computer graphics, geometry, and affine transformations</li> </ul> </li> <li> <p>Week 3 (Jan 27 and Jan 29): Principles of Data Visualization  </p> <ul> <li>Human perception and cognition  </li> <li>Gestalt principles  </li> <li>Visual encoding  </li> <li>Data visualization principles  </li> <li>Color perception, theory and representations  </li> <li>Ethical considerations in data visualization  </li> <li>Bad vs Good visualizations and common misconceptions</li> </ul> </li> <li> <p>Week 4 (Feb 3 and Feb 5): Data types and Exploratory Data Analysis  </p> <ul> <li>Data types and data structures  </li> <li>Data cleaning and preprocessing  </li> <li>Manipulation of data  </li> <li>Description and summarization of data  </li> <li>Histograms  </li> <li>Box plots and variants  </li> <li>File formats for visualizations  </li> <li>Exporting visualizations to design tools</li> </ul> </li> <li> <p>Week 5 (Feb 10 and Feb 12): Distributions, scales and axes  </p> <ul> <li>Typical distributions  </li> <li>Kernel density estimation  </li> <li>Power-law distribution  </li> <li>Linear, Logarithmic and Time scales  </li> <li>Line plots  </li> <li>Axes, ticks and labels</li> </ul> </li> <li> <p>Week 6 (Feb 17 and Feb 19): Mapping data to 2D  </p> <ul> <li>Scatter and bubble plots  </li> <li>Heatmaps  </li> <li>Color scales  </li> <li>2D histograms, contour and density plots  </li> <li>Which chart, color, scale, map, *, to use?  </li> <li>Introduction to d3.js  </li> <li>Project idea discussions and matchmaking</li> </ul> </li> <li> <p>Week 7 (Feb 24 and Feb 26): Multidimensional data I  </p> <ul> <li>Parallel coordinates and radar chart  </li> <li>Scatter matrices and multi-panel plots  </li> <li>Are 3D plots the solution?  </li> <li>Principles of dimensionality reduction  </li> <li>Principal component analysis</li> </ul> </li> <li> <p>Week 8 (Mar 3 and Mar 5): Multidimensional data II  </p> <ul> <li>Visualizing high-dimensional data  </li> <li>t-SNE and UMAP  </li> <li>Clustering</li> </ul> </li> <li> <p>Week 9 (Mar 10 and Mar 12): Geospatial data  </p> <ul> <li>Map projections  </li> <li>Choropleth maps  </li> <li>Density projection and caveats  </li> <li>Geodesic and great-circle distances  </li> <li>Routes  </li> <li>Midterm project proposal evaluation</li> </ul> </li> <li> <p>Week 10 (Mar 16 - Mar 23): Spring break (no classes)</p> </li> <li> <p>Week 11 (Mar 24 and Mar 26): Text and embeddings  </p> <ul> <li>Preprocessing text  </li> <li>Word clouds and variations  </li> <li>Word prevalence plots  </li> <li>Words and Text embedding (e.g., word2vec, BERT, etc.)  </li> <li>Other types of embedding (e.g., images, etc.)</li> </ul> </li> <li> <p>Week 12 (Mar 31 and Apr 2): Network visualization  </p> <ul> <li>Network visualization  </li> <li>Node-link diagrams  </li> <li>Graph layout algorithms  </li> <li>Visualizing social media  </li> <li>Project checkpoint and discussions</li> </ul> </li> <li> <p>Week 13 (Apr 7 and Apr 9): Interactive visualizations  </p> <ul> <li>Importance of interactivity  </li> <li>Types of interactions  </li> <li>Demonstration d3.js for interactive visualizations  </li> <li>Deploying visualizations to the web  </li> <li>Building interactive visualizations with d3.js</li> </ul> </li> <li> <p>Week 14 (Apr 14 and Apr 16): Deconstructing and reconstructing visualizations with d3.js  </p> <ul> <li>The building blocks of visualizations  </li> <li>Customizing visualizations with d3.js  </li> <li>Guest lecture</li> </ul> </li> <li> <p>Week 15 (Apr 21 and Apr 23): Project hackday and presentations</p> </li> <li> <p>Week 16 (Apr 28 and Apr 30): Project presentations week</p> </li> <li> <p>Week 17 (May 5 or May 7): Final Exam</p> </li> </ul>"},{"location":"#project","title":"Project","text":"<p>You can choose your own project topic individually or as a team. If you're working as a team, we will assist with team formation if needed. It's strongly recommended that you discuss your topic with the instructor and TAs for guidance.</p>"},{"location":"#1-project-proposal-midterm","title":"1. Project Proposal (Midterm)","text":"<p>Midway through the course, you must submit a short 1-page project proposal. This will outline: - Your chosen topic. - The problem or question you aim to address. - Your planned approach for creating visualizations.</p> <p>The proposal is flexible (you can change your mind later) and intended to help you refine your ideas and receive feedback before proceeding further.</p>"},{"location":"#2-final-report","title":"2. Final Report","text":"<p>A detailed report that: - Outlines your findings. - Clearly explains the process you followed to create your visualizations. - Demonstrates your understanding of visualization methods and your ability to use them effectively for visual data presentations.</p>"},{"location":"#3-code","title":"3. Code","text":"<p>A GitHub repository containing the code developed and/or used for the project: - Should provide a README file with description. - Code should be at least minimally runnable and with instructions. - Data or pointers to the datasets employed should be provided (if the data cannot be shared, its format should be described). - Commented code and organization in a reusable package can provide bonus points.</p>"},{"location":"#4-presentation","title":"4. Presentation","text":"<p>A presentation of your work, which includes: - Duration: 5\u201310 minutes, followed by a short Q&amp;A session. - Note: The exact presentation time will depend on the total number of teams formed.</p>"},{"location":"#recommended-books-and-resources","title":"Recommended books and resources","text":"<p>Here are some highly recommended books and resources on Data Visualization and general Data Science with Python:</p> <ol> <li>Fundamentals of Data Visualization by Claus O. Wilke (available online at https://clauswilke.com/dataviz/): A comprehensive guide to the theory and practice of data visualization.</li> <li>The Visual Display of Quantitative Information (2nd ed.) by E.R. Tufte: A classic book on data visualization principles and techniques.</li> <li>\"Python Data Science Handbook\" by Jake VanderPlas: A comprehensive guide to using Python for data analysis, manipulation, and visualization.</li> <li>D3 Tips and Tricks v7.x by Malcolm Maclean \u2013 https://leanpub.com/d3-t-and-t-v7 (Free online book, or pay what you want)</li> <li>D3 Tutorial updated by Danny Yang (forked from Square's original tutorial): Online tutorial on d3.js. https://yangdanny97.github.io/blog/2022/08/07/d3-resources</li> <li>\"Data Science from Scratch\" by Joel Grus: A great introduction to Data Science fundamentals using Python.</li> <li>\"Python for Data Analysis\" by Wes McKinney (the creator of pandas): A practical guide to using Python for data analysis, manipulation, and visualization.</li> <li>\"Introduction to Machine Learning with Python\" by Andreas C. M\u00fcller &amp; Sarah Guido: A practical approach to learning machine learning with Python.</li> <li>Kaggle: Participate in competitions or explore datasets for practical experience (https://www.kaggle.com/)</li> <li>Awesome Public Datasets: For a huge list of public datasets for practice and projects (https://github.com/awesomedata/awesome-public-datasets)</li> <li>Visual Complexity: Mapping patterns of information by Manuel Lima: A book on the visualization of complex networks and systems.</li> </ol> <p>Wait for more resources to be added to this list or suggest your own!</p>"},{"location":"#course-materials","title":"Course Materials","text":"<p>Here's what you can find in our repository: - Python Jupyter Notebooks: Interactive notebooks with code, explanations, and exercises. - PDF Presentations: Slides covering key concepts and examples. - Assignments: Python notebook assignments to apply what you've learned. - Datasets: A collection of datasets used in our materials, including links to Kaggle datasets for hands-on practice. - Additional Resources: Links to further reading and external resources.</p> <p>Most of these materials will be available when the course starts.</p>"},{"location":"#grading","title":"Grading","text":"<p>Grades will not be curved and will use the following scale.</p> <pre><code>93.00 - 100%  = A\n90.00 - 92.99% = A-\n87.00 - 89.99% = B+\n83.00 - 86.99% = B\n80.00 - 82.99% = B-\n77.00 - 79.99% = C+\n73.00 - 76.99% = C\n70.00 - 72.99% = C-\n67.00 - 69.99% = D+\n63.00 - 66.99% = D\n60.00 - 62.99% = D-\n&lt;60.00       = F\n</code></pre> <p>You will be evaluated based on performance in engagement (participation or attendance), assignments, final project and the final exam. The final grade will be calculated as follows:</p> <ul> <li>20% - Attendance/Engagement</li> <li>20% - Assignments</li> <li>30% - Final project</li> <li>30% - Final exam</li> </ul> <p>Some assignments may give bonus points for the final grade. Extra credits will be given based on engagement in class and online, such as asking questions, helping others, and contributing to the course materials.</p>"},{"location":"#writing-assistance","title":"Writing Assistance","text":"<p>Writing reports is a large part of the class. In addition to excellent content, there are high expectations for the quality of the writing (organization, clarity, grammar, etc.). For free help at any phase of the writing process\u2014from brainstorming to polishing the final draft\u2014call Writing Tutorial Services (WTS, pronounced \u201cwits\u201d) at 812-855-6738 for an appointment. When you visit WTS, you\u2019ll find a tutor who is a sympathetic and helpful reader of your prose. To be assured of an appointment with the tutor who will know most about your class, please call in advance.</p> <p>WTS, in the new Learning Commons on the first floor of the West Tower of Wells Library, is open Monday-Thursday 10:00 a.m. to 8:00 p.m. and Friday 10:00 a.m. to 5:00 p.m. WTS tutors are also available for walk-in appointments in the Academic Support Centers in Briscoe, Forest, and Teter residence halls, in the Neal-Marshall Black Culture Center, at La Casa, and at the Groups Scholars Program Office in Maxwell. Call WTS or check our Web site for hours.</p> <p>https://wts.indiana.edu/</p>"},{"location":"#attendance-and-engagement","title":"Attendance and Engagement","text":"<p>You will receive a full engagement grade if you attend all classes. In addition, you can earn extra points by actively participating: asking questions, helping classmates, and contributing to course materials; both in class and online. Attendance will be tracked via sign-in sheets, Top Hat, or random spot-check questions.</p> <p>Absences</p> <ul> <li>If you need to be absent for any reason (health issues, unexpected events, travel, etc.), please let me know by email.</li> <li>While life happens and missing a few classes may be unavoidable, your engagement grade may be lowered if you have too many absences and little to no participation or engagement records.</li> </ul> <p>Catching Up &amp; Special Cases</p> <ul> <li>All materials will be made available online, and the TAs are here to help you stay on track.</li> <li>The instructor will do his best to accommodate special situations (like illness, family emergencies, travel) by providing recordings or other resources\u2014just let him know, and we will figure out a plan.</li> </ul> <p>Religious Observances</p> <p>Indiana University respects the right of all students to observe their religious holidays. Accordingly, course directors are to make reasonable accommodation, upon request, for such observances. It is the responsibility of the students involved to notify their course directors in a timely manner concerning their need for such accommodation. In this class, please send me e-mail or visit me in office hours to notify me of such a situation at least a few days in advance of the event. See full details at: https://bulletin.indiana.edu/policies/religious-observances.html</p>"},{"location":"#policies","title":"Policies","text":""},{"location":"#general-policies","title":"General policies","text":"<p>(Copied from Prof. YY Ahn's course)</p> <ol> <li>Be honest. Don\u2019t be a cheater.    Your assignments and papers should be your own work. If you find useful resources for your assignments, share them and cite them. If your friends helped you, acknowledge them. You should feel free to discuss both online and offline (except for the exam), but do not show your code directly. Any cases of academic misconduct (cheating, fabrication, plagiarism, etc.) will be reported to the School and the Dean of Students, following the standard procedure. Cheating is not cool.</li> <li>You have the responsibility of backing up all your data and code.    Always back up your code and data. You should at least use Google Drive or Dropbox at the minimum. You can also use cloud services like Google Colaboratory. Ideally, learn version control systems and use https://github.iu.edu/ or https://github.com/. Loss of data, code, or papers (e.g., due to malfunction of your laptop) is not an acceptable excuse for delayed or missing submission.</li> <li>Disabilities.    Every attempt will be made to accommodate qualified students with disabilities (e.g., mental health, learning, chronic health, physical, hearing, vision, neurological, etc.). You must have established your eligibility for support services through Disability Services for Students. Note that services are confidential, may take time to put into place, and are not retroactive. Captions and alternate media for print materials may take three or more weeks to get produced. Please contact Disability Services for Students at http://disabilityservices.indiana.edu/ or 812-855-7578 as soon as possible if accommodations are needed. The office is located on the third floor, west tower, of the Wells Library (Room W302). Walk-ins are welcome 8 AM to 5 PM, Monday through Friday. You can also locate a variety of campus resources for students and visitors who need assistance at https://accessibility.iu.edu/ada/requesting-accommodations/for-students/index.html.</li> <li>Bias-based incidents.    Any act of discrimination or harassment based on race, ethnicity, religious affiliation, gender, gender identity, sexual orientation, or disability can be reported to biasincident@indiana.edu or to the Dean of Students Office at (812) 855-8188.</li> <li> <p>Sexual misconduct and Title IX.    Title IX and IU\u2019s Sexual Misconduct Policy prohibit sexual misconduct in any form, including sexual harassment, sexual assault, stalking, and dating and domestic violence. If you have experienced sexual misconduct, or know someone who has, you can use university resources:  </p> <ul> <li>a) The Sexual Assault Crisis Services (SACS) at (812) 855-8900 (counseling services)  </li> <li>b) Confidential Victim Advocates (CVA) at (812) 856-2469 (advocacy and advice services)  </li> <li>c) IU Health Center at (812) 855-4011 (health and medical services)</li> </ul> </li> <li> <p>If you have any mental health issues, don\u2019t hesitate to contact IU\u2019s Counseling and Psychological Services, which provides free counseling sessions. Also, please contact Disability Services for Students at http://disabilityservices.indiana.edu/ or 812-855-7578 as soon as possible if accommodations are needed.</p> </li> </ol>"},{"location":"#policy-on-the-use-of-generative-ai-and-llms","title":"Policy on the Use of Generative AI and LLMs","text":"<p>In accordance with IU\u2019s Generative AI Policies, you have the instructor permission to use generative AI (GAI) and large language models (LLMs), including GPT, Gemini, Llama, Copilot, and similar tools, in this course as long as you do so responsibly and transparently. This includes tasks such as code completion, brainstorming, or improving the clarity of your text. However, these tools are not recommended for fully automating assignments or drafting entire project reports. Keep in mind that the quality of GAI output can be unreliable, and you remain fully accountable for any inaccuracies, biases, or offensive content you submit.</p> <ul> <li>Disclosure: If GAI usage played a substantial role in shaping your work, you must clearly acknowledge it. Provide a brief description of how you used the tool, and if it was essential to your final result, include a copy of the prompt(s).</li> <li>Citation Example (no particular format required beyond basic transparency):</li> </ul> <p>OpenAI. (2024). ChatGPT (Mar 14 version) [Large language model]. https://chat.openai.com/chat</p> <ul> <li>Verification: For code generated by AI, make sure it does exactly what you intend. You must be able to explain its logic and functionality; \u201cblindly trusting\u201d AI outputs is strongly discouraged.</li> <li>Limitations: No GAI tools may be used during exams or project presentations. Doing so will be considered a violation of course policies.</li> <li>Integrity: Use of GAI without proper acknowledgment can be treated as plagiarism or cheating.</li> </ul> <p>Note: There is currently no IU-approved or otherwise reliable method to detect AI-generated content. Some GAI systems can produce excessively verbose or uniquely structured text, but this is not consistent or guaranteed. We rely on your honesty and accurate self-reporting regarding the use of these tools.</p> <p>In short, while GAI can streamline your workflow, it is not a substitute for human critical thinking. Leverage these tools thoughtfully, verify their outputs, and be prepared to justify your methods.</p>"},{"location":"#special-thanks","title":"Special Thanks","text":"<ul> <li>YY Ahn designed the course and kindly shared the materials.</li> <li>Francisco Alfaro helped the migration with mkdocs. </li> </ul>"},{"location":"__init__/","title":"init","text":""},{"location":"communication/","title":"Communication","text":""},{"location":"communication/#channels","title":"Channels","text":"<p>We will have Canvas as the primary and official channel, but also have Slack as a more casual/quick communication channel. </p> <p>You need to use IU email or Canvas for any communication that contains your personal information or specific grade.  However, I may be able to see Slack messages more quickly (I'm often behind my emails). </p> <p>So I'd encourage to use Slack as the default, but then Canvas/Email for anything personal or sensitive. Even when you communicate via Canvas, you can still ping me on Slack to ensure that I see it quickly. </p>"},{"location":"communication/#how-to-get-your-answers-quickly-and-effectively","title":"How to get your answers quickly and effectively","text":""},{"location":"communication/#not-so-effective","title":"Not so effective","text":"<ol> <li> <p>Sending DMs to a single TA or only to the instructor individually: You are less likely to get a timely response and you may need to send multiple messages to multiple people and then coordinate them. What could be a single message in a group DM can become dozens of scattered messages. Unless it is very personal/sensitive, avoid doing this. </p> </li> <li> <p>Using Canvas/email as the primary channel: This is totally fine given that it is our primary communication channel. However, please note that the response may be much slower given that I am usually swamped with emails and cannot respond quickly. I can see Slack messages more often. </p> </li> </ol>"},{"location":"communication/#more-effective","title":"More effective","text":"<ol> <li> <p>Post in <code>#q-and-a</code> channel: this will maximize the number of people who can see and answer your question (the instructor, TAs, and other students). </p> </li> <li> <p>Create a group DM with all AIs &amp; the instructor: This again will make sure that everyone in the team see your message and everyone is on the same page. </p> </li> </ol>"},{"location":"communication/#how-to-ask-a-good-question","title":"How to ask a good question?","text":"<p>The quality and promptness of answers you get will largely depend on the quality of your question! </p> <p>provide enough context. For instance, if you encounter an error, you may want to show us, for instance, the error message, what you were doing, screenshots, what you did to solve the issue, your hypothesis about why it is happening, and the notebook file itself, and so on. Actually, one of the best ways to solve an issue is trying to ask a really good question properly.</p> <p>Show the screenshot. Since it is impossible to remember every single quiz question or parts of the assignments, not having the context right there means: open a browser, go to canvas website, type login password, wait for the duo login, navigate to the course, navigate to the module, open the right question, and read it. It creates a huge unnecessary friction. If you\u2019re looking at the question (or any text), just copy &amp; paste it. </p> <p>Don\u2019t summon, just ask your question. </p> <ul> <li>Don't: \"Hello professor? Can I ask a question?\" - Of course you can ask a question! \ud83d\ude0a You can just ask! </li> <li>Do: \"Could you clarify the following quiz question? Here's the screenshot of the quiz and it is module X review quiz. I thought that the answer was X but it was Y. blah blah ...\"</li> </ul>"},{"location":"tutorials/git_basics/","title":"GIT Basics","text":"<p>Git is a distributed version control system that allows you to track changes in your code, collaborate with others, and revert to previous versions if needed. This tutorial covers the basics of Git, including installation, configuration, creating a local repository, working with remotes, branching, merging, and more.</p> <p>In this tutorial, you will learn how to: - Install and set up Git on your computer. - Create a local repository and make your first commit. - Create a remote repository on GitHub and connect it to your local repo. - Clone an existing repository to your local machine. - Work with branches, merging, and rebasing. - Fork a repository on GitHub and keep your fork in sync with the original project.</p>"},{"location":"tutorials/git_basics/#1-installing-and-setting-up-git","title":"1. Installing and Setting Up Git","text":""},{"location":"tutorials/git_basics/#install-git","title":"Install Git","text":"<ul> <li>Windows: Download Git for Windows and install. You can use Git Bash or Command Prompt after installation.</li> <li>macOS: Git often comes pre-installed. If not, install Xcode Command Line Tools (<code>xcode-select --install</code>) or get the official installer.</li> <li>Linux: Install via your package manager, e.g. <code>sudo apt-get install git</code> (Debian/Ubuntu) or <code>sudo dnf install git</code> (Fedora).</li> </ul>"},{"location":"tutorials/git_basics/#configure-git-all-platforms","title":"Configure Git (All Platforms)","text":"<p>After installation, set your name and email for commits: <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n</code></pre> You can check your configuration with: <pre><code>git config --list\n</code></pre></p>"},{"location":"tutorials/git_basics/#2-starting-a-local-repository","title":"2. Starting a Local Repository","text":"<p>You can create a new folder on your computer and turn it into a Git repository: <pre><code>mkdir my-new-project\ncd my-new-project\ngit init\n</code></pre> - <code>git init</code> creates a hidden <code>.git</code> folder that tracks your code changes.</p>"},{"location":"tutorials/git_basics/#first-commit","title":"First Commit","text":"<ol> <li>Create a file, e.g. <code>README.md</code>.</li> <li>Add content to the file (e.g., \u201cThis is my new project\u201d).</li> <li> <p>Stage and commit it:    <pre><code>git add README.md\ngit commit -m \"Initial commit\"\n</code></pre></p> </li> <li> <p><code>git add</code> tells Git which files you want to include in the next commit.  </p> </li> <li><code>git commit -m \"message\"</code> creates a snapshot of the staged changes with a brief message.</li> </ol>"},{"location":"tutorials/git_basics/#3-creating-a-remote-repository-on-github","title":"3. Creating a Remote Repository on GitHub","text":"<p>You can do this in two main ways:</p>"},{"location":"tutorials/git_basics/#method-a-creating-on-githubs-website","title":"Method A: Creating on GitHub\u2019s Website","text":"<ol> <li>Go to github.com and log in.  </li> <li>Click the \u201cNew\u201d button or \u201c+\u201d \u2192 \u201cNew repository.\u201d  </li> <li>Name your repository (e.g., <code>my-new-project</code>) and optionally add a description.  </li> <li>Leave it public or private, depending on your needs.  </li> <li>Do not initialize with a README if you already have one locally.  </li> <li>Click \u201cCreate repository.\u201d  </li> <li>GitHub will display instructions with a remote URL (something like <code>https://github.com/YourUsername/my-new-project.git</code>).</li> </ol> <p>Connect your local repo to this remote: <pre><code>git remote add origin https://github.com/YourUsername/my-new-project.git\ngit branch -M main\ngit push -u origin main\n</code></pre> - <code>git remote add origin ...</code> links your local repo to the remote. - <code>git push -u origin main</code> uploads (pushes) your local main branch to GitHub and sets <code>origin/main</code> as the default remote branch to track.</p>"},{"location":"tutorials/git_basics/#method-b-creating-a-repo-from-the-command-line-github-cli","title":"Method B: Creating a Repo from the Command Line (GitHub CLI)","text":"<p>If you have the GitHub CLI installed: <pre><code>gh repo create my-new-project --public --source=. --remote=origin --push\n</code></pre> - This automates the repo creation on GitHub and sets up the remote connection.</p>"},{"location":"tutorials/git_basics/#4-cloning-an-existing-repository","title":"4. Cloning an Existing Repository","text":"<p>If you want to get a copy of an existing repo from GitHub (or another service): <pre><code>git clone https://github.com/SomeoneElse/some-repo.git\ncd some-repo\n</code></pre> - This downloads the repository to your local machine. - You\u2019ll have a <code>.git</code> folder ready, so you can start making commits right away.</p>"},{"location":"tutorials/git_basics/#5-basic-git-workflow","title":"5. Basic Git Workflow","text":"<p>Once your local repository is set up and connected to a remote, the daily workflow typically looks like this:</p> <ol> <li> <p>Pull the latest changes to ensure your local copy is up to date:    <pre><code>git pull origin main\n</code></pre>    (Substitute <code>main</code> with the branch you\u2019re working on if needed.)</p> </li> <li> <p>Make changes (edit files, add new files, etc.).</p> </li> <li> <p>Stage changes:    <pre><code>git add .\n</code></pre>    or specify file names individually: <pre><code>git add my_file.py\n</code></pre></p> </li> <li> <p>Commit with a descriptive message:    <pre><code>git commit -m \"Add new feature X\"\n</code></pre></p> </li> <li> <p>Push your local commits to GitHub:    <pre><code>git push origin main\n</code></pre></p> </li> </ol>"},{"location":"tutorials/git_basics/#6-branching-and-merging","title":"6. Branching and Merging","text":""},{"location":"tutorials/git_basics/#creating-and-switching-to-a-branch","title":"Creating and Switching to a Branch","text":"<p>A branch is a separate line of development to safely work on new features or experiments.</p> <p><pre><code>git checkout -b my-feature\n# or:\n# git switch -c my-feature\n</code></pre> - <code>checkout -b</code> creates and switches you to the new branch. - Make your changes, then commit them as usual (<code>git add</code>, <code>git commit -m \"...\"</code>).</p>"},{"location":"tutorials/git_basics/#merging-a-branch-into-main","title":"Merging a Branch into <code>main</code>","text":"<p>After you finish the feature and it\u2019s tested, you merge it back into <code>main</code>: 1. Switch back to <code>main</code>:    <pre><code>git checkout main\n# or git switch main\n</code></pre> 2. Merge the feature branch:    <pre><code>git merge my-feature\n</code></pre> 3. Push the updated <code>main</code> to the remote:    <pre><code>git push origin main\n</code></pre></p> <p>If you run into merge conflicts, Git will tell you which files have conflicting changes. Resolve them manually in your text editor by keeping or adjusting the lines you need, then stage and commit again.</p>"},{"location":"tutorials/git_basics/#7-forking-a-repository","title":"7. Forking a Repository","text":"<p>Forking is a common way to contribute to someone else\u2019s project:</p> <ol> <li>On GitHub, go to the repository you want to fork.  </li> <li>Click the Fork button in the top-right corner.  </li> <li>GitHub creates a copy under your account, e.g., <code>github.com/YourUsername/some-repo</code>.  </li> <li>Clone your fork to your local machine:    <pre><code>git clone https://github.com/YourUsername/some-repo.git\n</code></pre></li> <li>Make changes, commit, and push to your fork:    <pre><code>git add .\ngit commit -m \"Fix typo / Add feature\"\ngit push origin main\n</code></pre></li> <li>Open a Pull Request on the original repository to propose your changes.</li> </ol>"},{"location":"tutorials/git_basics/#8-getting-updates-from-upstream-keeping-your-fork-in-sync","title":"8. Getting Updates from \u201cUpstream\u201d (Keeping Your Fork in Sync)","text":"<p>Often, you\u2019ll want to keep your local fork up to date with the original project (\u201cupstream\u201d):</p> <ol> <li>Add the original repository as an upstream remote:    <pre><code>git remote add upstream https://github.com/OriginalAuthor/some-repo.git\n</code></pre></li> <li>Fetch updates from upstream:    <pre><code>git fetch upstream\n</code></pre></li> <li>Merge changes into your local <code>main</code> branch:    <pre><code>git checkout main\ngit merge upstream/main\n</code></pre></li> <li>Push those changes back to your fork on GitHub:    <pre><code>git push origin main\n</code></pre></li> </ol>"},{"location":"tutorials/git_basics/#9-rebase-optional-advanced-topic","title":"9. Rebase (Optional Advanced Topic)","text":"<p>Rebasing is an alternative to merging that applies your commits on top of the latest changes from another branch, resulting in a cleaner, more linear history. It\u2019s optional but can be very helpful in maintaining a tidy commit log.</p> <ul> <li>Example (rebasing your feature branch onto the updated <code>main</code>):   <pre><code># Make sure your main is up to date:\ngit checkout main\ngit pull origin main\n\n# Switch to your feature branch:\ngit checkout my-feature\n\n# Rebase feature branch on top of the new main:\ngit rebase main\n</code></pre></li> <li>If there are conflicts, resolve them and then continue the rebase:   <pre><code>git add .\ngit rebase --continue\n</code></pre></li> <li>Finally, push your rebased branch. You may need <code>--force</code> if you\u2019ve rewritten commit history:   <pre><code>git push origin my-feature --force\n</code></pre> For beginners, merges are typically enough. Rebasing is optional and recommended once you\u2019re comfortable with Git basics, especially if the project enforces a rebased (linear) history.</li> </ul>"},{"location":"tutorials/git_basics/#10-summary-of-common-commands","title":"10. Summary of Common Commands","text":"<ul> <li>Initialization: <code>git init</code></li> <li>Clone a repo: <code>git clone &lt;repo_url&gt;</code></li> <li>View status: <code>git status</code></li> <li>Stage changes: <code>git add .</code> (or file names)</li> <li>Commit: <code>git commit -m \"Message\"</code></li> <li>Push: <code>git push origin &lt;branch&gt;</code></li> <li>Pull: <code>git pull origin &lt;branch&gt;</code></li> <li>Check branches: <code>git branch</code></li> <li>Create/switch branch: <code>git checkout -b my-branch</code> or <code>git switch -c my-branch</code></li> <li>Merge: <code>git merge &lt;branch&gt;</code></li> <li>Rebase (advanced): <code>git rebase main</code></li> <li>Fork (on GitHub) \u2192 <code>git remote add upstream &lt;original_repo&gt;</code> \u2192 <code>git fetch upstream</code> \u2192 <code>git merge upstream/main</code></li> </ul>"},{"location":"tutorials/google_colab/","title":"Google Colab Basics","text":"In\u00a0[\u00a0]: Copied! <pre>from google.colab import drive\nimport pandas as pd\n\ndrive.mount('/content/drive')\n</pre> from google.colab import drive import pandas as pd  drive.mount('/content/drive') In\u00a0[\u00a0]: Copied! <pre># Update the file path as necessary\nfile_path = '/content/drive/MyDrive/data/dataset.csv'\n\ntry:\n    df = pd.read_csv(file_path)\n    print(\"Data loaded successfully. Here are the first few rows:\")\n    print(df.head())\nexcept FileNotFoundError:\n    print(f\"Error: The file at {file_path} was not found. Please verify the path.\")\nexcept Exception as e:\n    print(\"An error occurred while loading the file:\", str(e))\n</pre> # Update the file path as necessary file_path = '/content/drive/MyDrive/data/dataset.csv'  try:     df = pd.read_csv(file_path)     print(\"Data loaded successfully. Here are the first few rows:\")     print(df.head()) except FileNotFoundError:     print(f\"Error: The file at {file_path} was not found. Please verify the path.\") except Exception as e:     print(\"An error occurred while loading the file:\", str(e)) In\u00a0[\u00a0]: Copied! <pre>!pip install seaborn\n</pre> !pip install seaborn In\u00a0[\u00a0]: Copied! <pre>import seaborn as sns\n# example usage:\nsns.lineplot(x=[1, 2, 3, 4], y=[1, 4, 9, 16])\n</pre> import seaborn as sns # example usage: sns.lineplot(x=[1, 2, 3, 4], y=[1, 4, 9, 16])"},{"location":"tutorials/google_colab/#google-colab-basics","title":"Google Colab Basics\u00b6","text":"<p>Welcome to the Data Viz course! This notebook will help you get started with Google Colab, an interactive, cloud-based Jupyter environment. If you already have access to some notebooks, simply click on the Open in Colab links provided in your course materials. This is one of the options for running the course notebooks.</p>"},{"location":"tutorials/google_colab/#using-google-colab","title":"Using Google Colab\u00b6","text":"<p>One option for this course is to use Google Colab.</p> <p>Google Colab allows you to write and execute Python code on remote servers. This means:</p> <ul> <li><p>No need to install Python or any libraries on your local machine.</p> </li> <li><p>Access to powerful computing resources directly from the browser.</p> </li> <li><p>Easy collaboration by sharing notebooks with peers.</p> </li> </ul> <p>While Google Colab offers a convenient cloud-based environment, keep in mind the following limitations:</p> <ul> <li><p>Sessions are temporary and may disconnect after a period of inactivity.</p> </li> <li><p>There are restrictions on GPU/TPU usage and RAM.</p> </li> <li><p>The runtime environment resets after disconnecting, which means unsaved work is lost.</p> </li> <li><p>Dependency management can be challenging if specific package versions are required.</p> </li> <li><p>You always need to install and import the necessary libraries at the beginning of each notebook.</p> </li> </ul> <p>For more details, check out the Google Colab Documentation.</p>"},{"location":"tutorials/google_colab/#1-setting-up-the-colab-environment","title":"1. Setting Up the Colab Environment\u00b6","text":"<p>To begin:</p> <ul> <li><p>Open your Colab notebook from Google Colab.</p> </li> <li><p>You can also use the Open in Colab button provided by your course materials.</p> </li> <li><p>Once your notebook opens, you'll see this guide and can start running cells immediately.</p> </li> </ul>"},{"location":"tutorials/google_colab/#2-mounting-google-drive","title":"2. Mounting Google Drive\u00b6","text":"<p>Many datasets in this course are stored on Google Drive. The cell below shows how to mount your drive:</p> <ol> <li><p>Run the cell.</p> </li> <li><p>Click on the link that appears.</p> </li> <li><p>Log into your Google account and authorize the access.</p> </li> <li><p>Copy and paste the provided authorization code back into the notebook.</p> </li> </ol>"},{"location":"tutorials/google_colab/#3-importing-data-from-google-drive","title":"3. Importing Data from Google Drive\u00b6","text":"<p>After mounting your drive, you can access your data files. For example, to load a CSV file called <code>dataset.csv</code> located in <code>MyDrive/data/</code>, update the file path accordingly and run the cell below.</p> <p>This example includes a simple error handling block in case the file isn't found.</p>"},{"location":"tutorials/google_colab/#4-installing-libraries","title":"4. Installing Libraries\u00b6","text":"<p>Google Colab comes with many popular libraries pre-installed. However, if you need to install additional packages, you can do so using <code>!pip install</code>.</p> <p>For example, to install the <code>seaborn</code> library, run the cell below:</p>"},{"location":"tutorials/google_colab/#importing-installed-libraries","title":"Importing Installed Libraries\u00b6","text":"<p>After installing a library, you can import it in your notebook as usual:</p>"},{"location":"tutorials/google_colab/#5-next-steps","title":"5. Next Steps\u00b6","text":"<p>Now that you've set up your environment:</p> <ul> <li><p>Explore the notebooks provided in the course.</p> </li> <li><p>Try running additional code cells and modifying examples.</p> </li> <li><p>Consider experimenting with different datasets and visualizations.</p> </li> <li><p>If you encounter issues, refer to the course documentation or ask for help.</p> </li> </ul> <p>Enjoy your journey with Usable AI and happy coding!</p>"},{"location":"tutorials/javascript_basics/","title":"Javascript Tutorial","text":"<p>[Under construction]</p>"},{"location":"tutorials/jupyter_basics/","title":"Jupyter Lab","text":"<p>This guide walks you through using Jupyter Lab for your Data Viz projects. It is assumed that your environment is already set up with the necessary packages. If you are using conda and prepared the environment as described in Setup Guide, this is the <code>dataviz</code> conda environment.</p>"},{"location":"tutorials/jupyter_basics/#1-launching-jupyter-lab","title":"1. Launching Jupyter Lab","text":""},{"location":"tutorials/jupyter_basics/#using-conda-environment","title":"Using Conda Environment","text":"<p>If you're using the <code>dataviz</code> conda environment: 1. Open your terminal. Remember that if you installed MiniForge on Windows, you should use the Miniforge Prompt from the Start Menu. 2. Activate your environment:     <pre><code>conda activate dataviz\n</code></pre> 3. Launch Jupyter Lab by running:     <pre><code>jupyter lab\n</code></pre></p>"},{"location":"tutorials/jupyter_basics/#using-other-environments","title":"Using Other Environments","text":"<p>If you have set up your environment with virtualenv or another tool: 1. Activate your virtual environment (e.g., for virtualenv):     <pre><code>source path/to/your/virtualenv/bin/activate\n</code></pre> 2. Launch Jupyter Lab:     <pre><code>jupyter lab\n</code></pre></p> <p>Note: The command is the same regardless of which environment management tool you use, as long as all required packages are installed.</p>"},{"location":"tutorials/jupyter_basics/#accessing-jupyter-lab-from-a-browser","title":"Accessing Jupyter Lab from a Browser","text":"<p>Jupyter Lab will open in your default web browser. If it doesn't, you can normally access it at <code>http://localhost:8888/lab</code>.</p>"},{"location":"tutorials/jupyter_basics/#2-navigating-the-jupyter-lab-interface","title":"2. Navigating the Jupyter Lab Interface","text":"<p>When Jupyter Lab starts, you'll see: - File Browser: Browse directories and open notebooks, scripts, and other files. - Launcher: Easily open new notebooks, terminals, text files, and consoles. - Tabs and Panels: Organize multiple documents side by side.</p> <p>Spend time exploring the layout to customize your workflow.</p>"},{"location":"tutorials/jupyter_basics/#3-creating-and-using-notebooks","title":"3. Creating and Using Notebooks","text":""},{"location":"tutorials/jupyter_basics/#creating-a-new-notebook","title":"Creating a New Notebook","text":"<ol> <li>Click on the Launcher tab or choose File &gt; New &gt; Notebook from the top menu.</li> <li>Select the desired kernel. By default, it may pick the Python version installed in your environment.</li> </ol>"},{"location":"tutorials/jupyter_basics/#running-cells","title":"Running Cells","text":"<ul> <li>Insert Code: Click on a cell to insert code.</li> <li>Run Cell: Press <code>Shift + Enter</code> to execute a cell.</li> <li>Insert Markdown: Change cell type to Markdown to add headings, explanations, or notes.</li> </ul>"},{"location":"tutorials/jupyter_basics/#useful-shortcuts","title":"Useful Shortcuts","text":"<ul> <li>Run current cell: Shift + Enter</li> <li>Insert new cell below: B</li> <li>Insert new cell above: A</li> <li>Save Notebook: Ctrl + S (Cmd + S on macOS)</li> </ul>"},{"location":"tutorials/jupyter_basics/#4-managing-kernels-and-environments","title":"4. Managing Kernels and Environments","text":""},{"location":"tutorials/jupyter_basics/#switching-kernels","title":"Switching Kernels","text":"<p>If the notebook isn\u2019t using the desired kernel: 1. Click on Kernel &gt; Change Kernel... in the menu. 2. Select the correct kernel corresponding to your environment. 3. Confirm if the kernel has the same name as your environment (e.g., <code>Python (dataviz)</code> if you are using our conda installation).</p>"},{"location":"tutorials/jupyter_basics/#adding-new-kernels-optional","title":"Adding New Kernels (optional)","text":"<p>For custom environments, you might need to manually add a kernel using <code>ipykernel</code>: 1. Install <code>ipykernel</code> in your environment if it's not already installed.     <pre><code>pip install ipykernel\n</code></pre> 2. Add the kernel:     <pre><code>python -m ipykernel install --user --name my_env --display-name \"Python (my_env)\"\n</code></pre> 3. Restart Jupyter Lab and select the new kernel from Kernel &gt; Change Kernel...</p>"},{"location":"tutorials/jupyter_basics/#5-executing-code-and-interrupting-the-kernel","title":"5. Executing code and interrupting the Kernel","text":"<p>You can execute code cells by pressing <code>Shift + Enter</code>. If you need to stop the execution of a cell or interrupt the kernel, you can use the following shortcuts: - Interrupt Kernel: Press <code>I</code> twice. You can also click on the stop button in the toolbar. This will stop the execution of the current cell but keep the kernel running. - Restart Kernel: Go to the Kernel menu and select Restart Kernel. Restarting the kernel will clear all variables and outputs.</p>"},{"location":"tutorials/jupyter_basics/#6-advanced-features","title":"6. Advanced Features","text":""},{"location":"tutorials/jupyter_basics/#extensions-and-plugins","title":"Extensions and Plugins","text":"<p>Jupyter Lab supports a rich ecosystem of extensions: - Variable Inspector: Monitor variables in your notebook. - Git Integration: Track changes and version control your projects. - Code Formatter: Automatically format your code with tools like Black.</p> <p>You can explore available extensions at the Jupyter Lab Extensions Repository.</p>"},{"location":"tutorials/jupyter_basics/#terminal-and-text-editor","title":"Terminal and Text Editor","text":"<ul> <li>Terminal: Open an integrated terminal from File &gt; New &gt; Terminal.</li> <li>Text Editor: Edit plain text or configuration files directly within Jupyter Lab.</li> </ul>"},{"location":"tutorials/jupyter_basics/#customizing-settings","title":"Customizing Settings","text":"<p>Access the Advanced Settings Editor under the Settings menu to customize themes, shortcuts, and other configurations.</p>"},{"location":"tutorials/jupyter_basics/#7-tips-for-effective-use","title":"7. Tips for Effective Use","text":"<ul> <li>Regularly Save: Use keyboard shortcuts to save frequently. (<code>Ctrl + S</code> or <code>Cmd + S on macOS</code>)</li> <li>Organize Code into Functions: Keep your notebook clean by modularizing your code.</li> <li>Version Control Notebooks: Back up your notebooks using Git.</li> <li>Explore Commands: Use the command palette (<code>Ctrl + Shift + C</code> or <code>Cmd + Shift + C on macOS</code>) to discover powerful quick commands.</li> <li>Utilize Markdown Cells: Enhance your notebooks by using Markdown cells for documentation and notes.</li> <li>Keep Descriptive Titles: Use clear and descriptive titles for your notebooks to make them easily identifiable.</li> </ul>"},{"location":"tutorials/jupyter_basics/#7-troubleshooting","title":"7. Troubleshooting","text":""},{"location":"tutorials/jupyter_basics/#common-issues","title":"Common Issues","text":"<ul> <li>Kernel Not Found: Check if the environment corresponding to the kernel is active and all dependencies are installed.</li> <li>Slow Performance: Close unnecessary tabs and clear outputs when working with large datasets.</li> <li>Extension Issues: Disable or update extensions if you experience unexpected behavior.</li> </ul>"},{"location":"tutorials/jupyter_basics/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Refer to the Jupyter Lab Documentation for more details.</li> <li>Community Forums: Engage in community discussions on forums like Stack Overflow for troubleshooting help.</li> <li>Instructor and AIs: Reach out to your instructor or AIs for assistance with specific issues.</li> </ul>"},{"location":"tutorials/jupyter_basics/#conclusion","title":"Conclusion","text":"<p>Jupyter Lab is a versatile tool that supports interactive coding, combining code execution, rich text elements, and visualizations in a unified interface. Whether using the <code>dataviz</code> conda environment or another setup, mastering Jupyter Lab will enhance your productivity in this course and beyond.</p> <p>Happy coding!</p>"},{"location":"tutorials/numpy_basics/","title":"Numpy basics","text":"In\u00a0[\u00a0]: Copied! <pre># Install numpy using pip\n!pip install numpy\n</pre> # Install numpy using pip !pip install numpy <p>This downloads and installs the NumPy package from the Python Package Index (PyPI). The <code>as np</code> alias is a common convention that makes it easier to reference NumPy throughout your code.</p> In\u00a0[\u00a0]: Copied! <pre># Importing NumPy\nimport numpy as np\n</pre> # Importing NumPy import numpy as np In\u00a0[\u00a0]: Copied! <pre># From Python Lists\narr_list = [1, 2, 3, 4]\narr_np = np.array(arr_list)\nprint(arr_np)       # [1 2 3 4]\nprint(arr_np.dtype) # e.g., int64\n</pre> # From Python Lists arr_list = [1, 2, 3, 4] arr_np = np.array(arr_list) print(arr_np)       # [1 2 3 4] print(arr_np.dtype) # e.g., int64  In\u00a0[\u00a0]: Copied! <pre># Using Built-in Functions\nzeros_arr = np.zeros((2, 3))       # 2x3 array of zeros\nones_arr = np.ones(5)              # 1D array of ones\nrange_arr = np.arange(0, 10, 2)    # [0, 2, 4, 6, 8]\nlin_arr = np.linspace(0, 1, 5)     # [0., 0.25, 0.5, 0.75, 1.]\n\nprint(zeros_arr)\nprint(ones_arr)\nprint(range_arr)\nprint(lin_arr)\n</pre> # Using Built-in Functions zeros_arr = np.zeros((2, 3))       # 2x3 array of zeros ones_arr = np.ones(5)              # 1D array of ones range_arr = np.arange(0, 10, 2)    # [0, 2, 4, 6, 8] lin_arr = np.linspace(0, 1, 5)     # [0., 0.25, 0.5, 0.75, 1.]  print(zeros_arr) print(ones_arr) print(range_arr) print(lin_arr)  In\u00a0[\u00a0]: Copied! <pre># Arithmetic\nx = np.array([1, 2, 3])\ny = np.array([10, 20, 30])\n\nprint(x + y)       # [11 22 33]\nprint(x * 2)       # [2 4 6]\nprint(x * y)       # [10 40 90]\n</pre> # Arithmetic x = np.array([1, 2, 3]) y = np.array([10, 20, 30])  print(x + y)       # [11 22 33] print(x * 2)       # [2 4 6] print(x * y)       # [10 40 90]  In\u00a0[\u00a0]: Copied! <pre># Array Statistics\narr = np.array([1, 2, 3, 4, 5])\nprint(arr.mean())  # 3.0\nprint(arr.sum())   # 15\nprint(arr.max())   # 5\n</pre> # Array Statistics arr = np.array([1, 2, 3, 4, 5]) print(arr.mean())  # 3.0 print(arr.sum())   # 15 print(arr.max())   # 5  In\u00a0[\u00a0]: Copied! <pre># Reshape\nmat = np.arange(1, 7)   # [1 2 3 4 5 6]\nmat_2d = mat.reshape(2, 3)\nprint(mat_2d)\n</pre> # Reshape mat = np.arange(1, 7)   # [1 2 3 4 5 6] mat_2d = mat.reshape(2, 3) print(mat_2d)  In\u00a0[\u00a0]: Copied! <pre># Slicing\n# 1D slicing\narr = np.array([10, 20, 30, 40, 50])\nprint(arr[1:3])       # [20 30]\n\n# 2D slicing\nmat_2d = np.array([[1, 2, 3],\n                   [4, 5, 6]])\nprint(mat_2d[0, 1])   # 2  (row 0, col 1)\nprint(mat_2d[:, 0])   # [1 4] (all rows, col 0)\n</pre> # Slicing # 1D slicing arr = np.array([10, 20, 30, 40, 50]) print(arr[1:3])       # [20 30]  # 2D slicing mat_2d = np.array([[1, 2, 3],                    [4, 5, 6]]) print(mat_2d[0, 1])   # 2  (row 0, col 1) print(mat_2d[:, 0])   # [1 4] (all rows, col 0)  In\u00a0[\u00a0]: Copied! <pre># Fancy Indexing\narr = np.array([10, 20, 30, 40, 50])\nidx = np.array([2, 0, 3])  # positions to pick\nprint(arr[idx])            # [30 10 40]\n</pre> # Fancy Indexing arr = np.array([10, 20, 30, 40, 50]) idx = np.array([2, 0, 3])  # positions to pick print(arr[idx])            # [30 10 40]  In\u00a0[\u00a0]: Copied! <pre># Boolean Masking\narr = np.array([10, 20, 30, 40, 50])\nmask = arr &gt; 25\nprint(mask)        # [False False  True  True  True]\nprint(arr[mask])   # [30 40 50]\n</pre> # Boolean Masking arr = np.array([10, 20, 30, 40, 50]) mask = arr &gt; 25 print(mask)        # [False False  True  True  True] print(arr[mask])   # [30 40 50]  In\u00a0[\u00a0]: Copied! <pre>a = np.array([[1, 2, 3],\n              [4, 5, 6]])\nb = np.array([10, 20, 30])\n\nprint(a + b)\n# [[11 22 33]\n#  [14 25 36]]\n</pre> a = np.array([[1, 2, 3],               [4, 5, 6]]) b = np.array([10, 20, 30])  print(a + b) # [[11 22 33] #  [14 25 36]]  In\u00a0[\u00a0]: Copied! <pre># Random numbers\nrand_arr = np.random.rand(3, 2)    # uniform in [0,1)\nrandn_arr = np.random.randn(3, 2)  # normal distribution (mean=0, std=1)\n\n# Inspect shape\nprint(rand_arr)\nprint(rand_arr.shape)  # (3, 2)\n</pre> # Random numbers rand_arr = np.random.rand(3, 2)    # uniform in [0,1) randn_arr = np.random.randn(3, 2)  # normal distribution (mean=0, std=1)  # Inspect shape print(rand_arr) print(rand_arr.shape)  # (3, 2)  In\u00a0[\u00a0]: Copied! <pre># Your solution:\n# 1. Create a 1D NumPy array of the first 10 positive integers\n\n# 2. Create a 2x5 array of zeros\n\n# 3. Create a 4x4 identity matrix\n</pre> # Your solution: # 1. Create a 1D NumPy array of the first 10 positive integers  # 2. Create a 2x5 array of zeros  # 3. Create a 4x4 identity matrix  In\u00a0[\u00a0]: Copied! <pre># Your solution:\narr = np.array([5, 10, 15, 20, 25])\n\n# 1. Multiply each element by 3\n\n# 2. Subtract 5 from each element\n\n# 3. Compute the sum of all elements\n</pre> # Your solution: arr = np.array([5, 10, 15, 20, 25])  # 1. Multiply each element by 3  # 2. Subtract 5 from each element  # 3. Compute the sum of all elements  In\u00a0[\u00a0]: Copied! <pre># Your solution:\n\n# 1. Create a 1D array of integers from 1 to 12\n\n# 2. Reshape it into a 3x4 matrix\n\n# 3. Extract the element in the 2nd row, 3rd column\n\n# 4. Extract the first column as a 1D array\n</pre> # Your solution:  # 1. Create a 1D array of integers from 1 to 12  # 2. Reshape it into a 3x4 matrix  # 3. Extract the element in the 2nd row, 3rd column  # 4. Extract the first column as a 1D array  In\u00a0[\u00a0]: Copied! <pre># Your solution:\narr = np.array([100, 200, 300, 400, 500])\nindices = np.array([4, 0, 2])\n\n# 1. Use fancy indexing to extract elements\n\n# 2. Rearrange the extracted elements into descending order\n</pre> # Your solution: arr = np.array([100, 200, 300, 400, 500]) indices = np.array([4, 0, 2])  # 1. Use fancy indexing to extract elements  # 2. Rearrange the extracted elements into descending order  In\u00a0[\u00a0]: Copied! <pre># Your solution:\narr = np.array([1, 4, 7, 10, 13, 16])\n\n# 1. Create a boolean mask for elements greater than 8\n\n# 2. Use the mask to extract those elements\n\n# 3. Compute the mean of the extracted elements\n</pre> # Your solution: arr = np.array([1, 4, 7, 10, 13, 16])  # 1. Create a boolean mask for elements greater than 8  # 2. Use the mask to extract those elements  # 3. Compute the mean of the extracted elements  In\u00a0[\u00a0]: Copied! <pre># Your solution:\na = np.array([[1, 2, 3], [4, 5, 6]])\nb = np.array([10, 20, 30])\n\n# 1. Add `b` to each row of `a`\n\n# 2. Multiply each element of `a` by 2\n\n# 3. Compute the sum of all elements\n</pre> # Your solution: a = np.array([[1, 2, 3], [4, 5, 6]]) b = np.array([10, 20, 30])  # 1. Add `b` to each row of `a`  # 2. Multiply each element of `a` by 2  # 3. Compute the sum of all elements  In\u00a0[\u00a0]: Copied! <pre># Your solution:\n\n# 1. Generate a 3x3 array of random values uniformly distributed between 0 and 1\n\n# 2. Generate a 3x3 array of random values drawn from a standard normal distribution\n\n# 3. Compute the mean and standard deviation of each array\n</pre> # Your solution:  # 1. Generate a 3x3 array of random values uniformly distributed between 0 and 1  # 2. Generate a 3x3 array of random values drawn from a standard normal distribution  # 3. Compute the mean and standard deviation of each array"},{"location":"tutorials/numpy_basics/#numpy-tutorial","title":"NumPy Tutorial\u00b6","text":"<p>This should give you a grasp on NumPy\u2019s essential operations, including fancy indexing and boolean masking.</p>"},{"location":"tutorials/numpy_basics/#1-installation-and-import","title":"1. Installation and Import\u00b6","text":"<ol> <li>Install NumPy (if you haven\u2019t already):</li> </ol>"},{"location":"tutorials/numpy_basics/#2-creating-arrays","title":"2. Creating Arrays\u00b6","text":"<p>NumPy arrays are the core data structure for fast numerical computation in Python. They\u2019re more efficient than standard Python lists for large-scale numerical operations.</p>"},{"location":"tutorials/numpy_basics/#3-basic-operations","title":"3. Basic Operations\u00b6","text":"<p>NumPy arrays allow you to perform element-wise arithmetic with concise syntax, which is much faster than using regular Python lists in a loop.</p>"},{"location":"tutorials/numpy_basics/#4-reshaping-and-indexing","title":"4. Reshaping and Indexing\u00b6","text":""},{"location":"tutorials/numpy_basics/#5-fancy-indexing-and-boolean-masking","title":"5. Fancy Indexing and Boolean Masking\u00b6","text":""},{"location":"tutorials/numpy_basics/#6-broadcasting","title":"6. Broadcasting\u00b6","text":""},{"location":"tutorials/numpy_basics/#7-random-and-useful-utilities","title":"7. Random and Useful Utilities\u00b6","text":""},{"location":"tutorials/numpy_basics/#8-conclusion-and-next-steps","title":"8. Conclusion and Next Steps\u00b6","text":"<ol> <li><p>Array Creation &amp; Reshaping: Master these to handle initial data processing.</p> </li> <li><p>Indexing &amp; Slicing: Use these techniques to quickly extract or modify parts of your dataset.</p> </li> <li><p>Fancy Indexing &amp; Boolean Masking: Powerful shortcuts for rearranging or filtering data based on conditions.</p> </li> <li><p>Broadcasting: Simplifies arithmetic with mismatched shapes.</p> </li> <li><p>Random Utilities: Ideal for simulations, data augmentation, or test scenarios.</p> </li> </ol>"},{"location":"tutorials/numpy_basics/#questions","title":"Questions\u00b6","text":"<p>Solve the following exercises to practice NumPy basics. Follow the instructions and fill in the blanks where indicated.</p> <p>There is no need to submit this notebook; it's for your practice only.</p>"},{"location":"tutorials/numpy_basics/#1-array-creation-and-initialization","title":"1. Array Creation and Initialization\u00b6","text":"<p>Task:</p> <ol> <li><p>Create a 1D NumPy array of the first 10 positive integers.</p> </li> <li><p>Create a 2x5 array of zeros.</p> </li> <li><p>Create a 4x4 identity matrix.</p> </li> </ol>"},{"location":"tutorials/numpy_basics/#2-array-operations","title":"2. Array Operations\u00b6","text":"<p>Task:</p> <p>Given the array <code>arr = np.array([5, 10, 15, 20, 25])</code>, do the following:</p> <ol> <li><p>Multiply each element by 3.</p> </li> <li><p>Subtract 5 from each element.</p> </li> <li><p>Compute the sum of all elements in the resulting array.</p> </li> </ol>"},{"location":"tutorials/numpy_basics/#3-reshaping-and-indexing","title":"3. Reshaping and Indexing\u00b6","text":"<p>Task:</p> <ol> <li><p>Create a 1D array of integers from 1 to 12.</p> </li> <li><p>Reshape it into a 3x4 matrix.</p> </li> <li><p>Extract the element in the 2nd row, 3rd column.</p> </li> <li><p>Extract the first column as a 1D array.</p> </li> </ol>"},{"location":"tutorials/numpy_basics/#4-fancy-indexing","title":"4. Fancy Indexing\u00b6","text":"<p>Task:</p> <p>Given <code>arr = np.array([100, 200, 300, 400, 500])</code> and <code>indices = np.array([4, 0, 2])</code>:</p> <ol> <li><p>Use fancy indexing to extract the elements at positions defined by <code>indices</code>.</p> </li> <li><p>Rearrange the extracted elements into descending order.</p> </li> </ol>"},{"location":"tutorials/numpy_basics/#5-boolean-masking","title":"5. Boolean Masking\u00b6","text":"<p>Task:</p> <p>Given <code>arr = np.array([1, 4, 7, 10, 13, 16])</code>:</p> <ol> <li><p>Create a boolean mask for elements greater than 8.</p> </li> <li><p>Use the mask to extract those elements.</p> </li> <li><p>Compute the mean of the extracted elements.</p> </li> </ol>"},{"location":"tutorials/numpy_basics/#6-broadcasting","title":"6. Broadcasting\u00b6","text":"<p>Task:</p> <p>Given the 2D array <code>a = np.array([[1, 2, 3], [4, 5, 6]])</code>:</p> <ol> <li><p>Add the 1D array <code>b = np.array([10, 20, 30])</code> to each row of <code>a</code>.</p> </li> <li><p>Multiply each element of <code>a</code> by 2.</p> </li> <li><p>Compute the sum of all elements in the resulting array.</p> </li> </ol>"},{"location":"tutorials/numpy_basics/#7-random-array-utilities","title":"7. Random Array Utilities\u00b6","text":"<p>Task:</p> <ol> <li><p>Generate a 3x3 array of random values uniformly distributed between 0 and 1.</p> </li> <li><p>Generate a 3x3 array of random values drawn from a standard normal distribution.</p> </li> <li><p>Compute the mean and standard deviation of each array.</p> </li> </ol>"},{"location":"tutorials/pandas_basics/","title":"Pandas basics","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install pandas\n</pre> !pip install pandas <p>Import pandas in Python:</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd  In\u00a0[\u00a0]: Copied! <pre>data = {\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"Age\": [25, 30, 35],\n    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"]\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n</pre> data = {     \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],     \"Age\": [25, 30, 35],     \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"] }  df = pd.DataFrame(data) print(df)  In\u00a0[\u00a0]: Copied! <pre>data_list = [\n    {\"Name\": \"Alice\",   \"Age\": 25, \"City\": \"New York\"},\n    {\"Name\": \"Bob\",     \"Age\": 30, \"City\": \"Los Angeles\"},\n    {\"Name\": \"Charlie\", \"Age\": 35, \"City\": \"Chicago\"}\n]\ndf2 = pd.DataFrame(data_list)\nprint(df2)\n</pre> data_list = [     {\"Name\": \"Alice\",   \"Age\": 25, \"City\": \"New York\"},     {\"Name\": \"Bob\",     \"Age\": 30, \"City\": \"Los Angeles\"},     {\"Name\": \"Charlie\", \"Age\": 35, \"City\": \"Chicago\"} ] df2 = pd.DataFrame(data_list) print(df2)  In\u00a0[\u00a0]: Copied! <pre>df_csv = pd.read_csv(\"my_data.csv\")      # from CSV\ndf_excel = pd.read_excel(\"my_data.xlsx\") # from Excel\n\n#Replace `\"my_data.csv\"` with your actual file path or URL.\n</pre> df_csv = pd.read_csv(\"my_data.csv\")      # from CSV df_excel = pd.read_excel(\"my_data.xlsx\") # from Excel  #Replace `\"my_data.csv\"` with your actual file path or URL. In\u00a0[\u00a0]: Copied! <pre>print(df.head())       # First 5 rows (use df.head(10) for first 10)\nprint(df.tail())       # Last 5 rows\nprint(df.shape)        # (rows, columns)\nprint(df.columns)      # List of column names\nprint(df.info())       # Summary of the DataFrame (types, non-null counts)\nprint(df.describe())   # Basic statistics for numeric columns\n</pre> print(df.head())       # First 5 rows (use df.head(10) for first 10) print(df.tail())       # Last 5 rows print(df.shape)        # (rows, columns) print(df.columns)      # List of column names print(df.info())       # Summary of the DataFrame (types, non-null counts) print(df.describe())   # Basic statistics for numeric columns  In\u00a0[\u00a0]: Copied! <pre># Dot notation (for simple column names without spaces/special chars)\nprint(df.Age)\n\n# Bracket notation\nprint(df[\"Age\"])\n</pre> # Dot notation (for simple column names without spaces/special chars) print(df.Age)  # Bracket notation print(df[\"Age\"])  In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame({\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"Dave\"],\n    \"Age\": [25, 30, 35, 28],\n    \"City\": [\"NY\", \"LA\", \"Chicago\", \"Seattle\"]\n}, index=[\"row1\", \"row2\", \"row3\", \"row4\"])  # custom index labels\n\n# Using .loc (label-based)\nprint(df.loc[\"row2\"])            # Entire row labeled 'row2'\nprint(df.loc[\"row2\", \"Age\"])     # Specific cell (row2, Age)\nprint(df.loc[\"row1\":\"row3\"])     # Slice multiple rows by label\nprint(df.loc[:, [\"Name\", \"City\"]]) # All rows, only these columns\n\n# Using .iloc (integer-based)\nprint(df.iloc[1])                # 2nd row (since indexing starts at 0)\nprint(df.iloc[1, 1])             # Cell in row index=1, col index=1\nprint(df.iloc[0:2])              # Rows 0 to 1\nprint(df.iloc[:, [0, 2]])        # All rows, columns 0 and 2\n</pre> df = pd.DataFrame({     \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"Dave\"],     \"Age\": [25, 30, 35, 28],     \"City\": [\"NY\", \"LA\", \"Chicago\", \"Seattle\"] }, index=[\"row1\", \"row2\", \"row3\", \"row4\"])  # custom index labels  # Using .loc (label-based) print(df.loc[\"row2\"])            # Entire row labeled 'row2' print(df.loc[\"row2\", \"Age\"])     # Specific cell (row2, Age) print(df.loc[\"row1\":\"row3\"])     # Slice multiple rows by label print(df.loc[:, [\"Name\", \"City\"]]) # All rows, only these columns  # Using .iloc (integer-based) print(df.iloc[1])                # 2nd row (since indexing starts at 0) print(df.iloc[1, 1])             # Cell in row index=1, col index=1 print(df.iloc[0:2])              # Rows 0 to 1 print(df.iloc[:, [0, 2]])        # All rows, columns 0 and 2  In\u00a0[\u00a0]: Copied! <pre># Show only rows where Age &gt; 28\nmask = df[\"Age\"] &gt; 28\nolder_than_28 = df[mask]\nprint(older_than_28)\n</pre> # Show only rows where Age &gt; 28 mask = df[\"Age\"] &gt; 28 older_than_28 = df[mask] print(older_than_28)  In\u00a0[\u00a0]: Copied! <pre># People older than 25 AND living in NY\ndf_filtered = df[(df[\"Age\"] &gt; 25) &amp; (df[\"City\"] == \"NY\")]\nprint(df_filtered)\n</pre> # People older than 25 AND living in NY df_filtered = df[(df[\"Age\"] &gt; 25) &amp; (df[\"City\"] == \"NY\")] print(df_filtered)  In\u00a0[\u00a0]: Copied! <pre>df.loc[\"row1\", \"Age\"] = 26\nprint(df)\n</pre> df.loc[\"row1\", \"Age\"] = 26 print(df)  In\u00a0[\u00a0]: Copied! <pre>df.iloc[0, 1] = 27\nprint(df)\n</pre> df.iloc[0, 1] = 27 print(df)  In\u00a0[\u00a0]: Copied! <pre># Increase everyone's Age by 1\ndf[\"Age\"] = df[\"Age\"] + 1\nprint(df)\n</pre> # Increase everyone's Age by 1 df[\"Age\"] = df[\"Age\"] + 1 print(df)  In\u00a0[\u00a0]: Copied! <pre>print(df[\"Age\"].mean())  # Average age\nprint(df[\"Age\"].max())   # Max age\nprint(df[\"Age\"].min())   # Min age\n</pre> print(df[\"Age\"].mean())  # Average age print(df[\"Age\"].max())   # Max age print(df[\"Age\"].min())   # Min age  In\u00a0[\u00a0]: Copied! <pre>city_counts = df[\"City\"].value_counts()\nprint(city_counts)\n</pre> city_counts = df[\"City\"].value_counts() print(city_counts)  In\u00a0[\u00a0]: Copied! <pre>data = {\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"Dave\"],\n    \"Age\": [25, 30, 35, 28],\n    \"City\": [\"NY\", \"LA\", \"NY\", \"LA\"],\n    \"Salary\": [70000, 80000, 120000, 95000]\n}\ndf = pd.DataFrame(data)\n\n# Group by 'City' and calculate mean Salary\ngrouped = df.groupby(\"City\")[\"Salary\"].mean()\nprint(grouped)\n</pre> data = {     \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"Dave\"],     \"Age\": [25, 30, 35, 28],     \"City\": [\"NY\", \"LA\", \"NY\", \"LA\"],     \"Salary\": [70000, 80000, 120000, 95000] } df = pd.DataFrame(data)  # Group by 'City' and calculate mean Salary grouped = df.groupby(\"City\")[\"Salary\"].mean() print(grouped)  In\u00a0[\u00a0]: Copied! <pre>df_left = pd.DataFrame({\n    \"PersonID\": [1, 2, 3],\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"]\n})\n\ndf_right = pd.DataFrame({\n    \"PersonID\": [1, 2, 4],\n    \"City\": [\"NY\", \"LA\", \"Houston\"]\n})\n\nmerged_df = pd.merge(df_left, df_right, on=\"PersonID\", how=\"inner\")\nprint(merged_df)\n</pre> df_left = pd.DataFrame({     \"PersonID\": [1, 2, 3],     \"Name\": [\"Alice\", \"Bob\", \"Charlie\"] })  df_right = pd.DataFrame({     \"PersonID\": [1, 2, 4],     \"City\": [\"NY\", \"LA\", \"Houston\"] })  merged_df = pd.merge(df_left, df_right, on=\"PersonID\", how=\"inner\") print(merged_df)  In\u00a0[\u00a0]: Copied! <pre># If columns in the two DataFrames have different names:\npd.merge(df_left, df_right, left_on=\"PersonID\", right_on=\"ID\")\n</pre> # If columns in the two DataFrames have different names: pd.merge(df_left, df_right, left_on=\"PersonID\", right_on=\"ID\")  In\u00a0[\u00a0]: Copied! <pre># 1. Create a DataFrame from a dictionary of lists.\n</pre> # 1. Create a DataFrame from a dictionary of lists.  In\u00a0[\u00a0]: Copied! <pre># 2. Load a CSV file and inspect its first few rows.\n</pre> # 2. Load a CSV file and inspect its first few rows.  In\u00a0[\u00a0]: Copied! <pre># 3. Filter rows where a numeric column exceeds a threshold.\n</pre> # 3. Filter rows where a numeric column exceeds a threshold.  In\u00a0[\u00a0]: Copied! <pre># 4. Perform a group-by operation and calculate the mean of another column.\n</pre> # 4. Perform a group-by operation and calculate the mean of another column.  In\u00a0[\u00a0]: Copied! <pre># 5. Merge two DataFrames on a common key.\n</pre> # 5. Merge two DataFrames on a common key."},{"location":"tutorials/pandas_basics/#pandas-tutorial","title":"Pandas Tutorial\u00b6","text":"<p>A comprehensive yet beginner-friendly tutorial on pandas, a popular Python library for data manipulation and analysis.</p> <p>We will cover:</p> <ul> <li><p>Creating and loading data into a pandas <code>DataFrame</code>.</p> </li> <li><p>Basic indexing, merging, grouping, and computing statistics.</p> </li> <li><p>Modifying data with <code>.loc</code>, <code>.iloc</code>, and using functions like <code>value_counts()</code>.</p> </li> </ul>"},{"location":"tutorials/pandas_basics/#1-installation-and-import","title":"1. Installation and Import\u00b6","text":"<p>Install pandas (if not already installed):</p>"},{"location":"tutorials/pandas_basics/#2-creating-dataframes","title":"2. Creating DataFrames\u00b6","text":"<p>A DataFrame is the core data structure in pandas\u2014think of it like a table with rows and columns. You can create one from various sources.</p>"},{"location":"tutorials/pandas_basics/#21-from-a-dictionary-of-lists","title":"2.1. From a Dictionary of Lists\u00b6","text":""},{"location":"tutorials/pandas_basics/#22-from-a-list-of-dictionaries","title":"2.2. From a List of Dictionaries\u00b6","text":""},{"location":"tutorials/pandas_basics/#23-from-csv-or-excel","title":"2.3. From CSV or Excel\u00b6","text":"<p>Pandas makes it easy to read data from common file types:</p>"},{"location":"tutorials/pandas_basics/#3-basic-data-inspection","title":"3. Basic Data Inspection\u00b6","text":"<p>After creating or loading a DataFrame, you\u2019ll often want to inspect it:</p>"},{"location":"tutorials/pandas_basics/#4-selecting-and-indexing-data","title":"4. Selecting and Indexing Data\u00b6","text":"<p>Pandas offers multiple ways to select or filter data within a DataFrame.</p>"},{"location":"tutorials/pandas_basics/#41-dot-notation-bracket-notation","title":"4.1. Dot Notation / Bracket Notation\u00b6","text":""},{"location":"tutorials/pandas_basics/#42-row-selection-with-loc-and-iloc","title":"4.2. Row Selection with <code>.loc</code> and <code>.iloc</code>\u00b6","text":"<ul> <li><p><code>.loc</code> selects rows and columns by label.</p> </li> <li><p><code>.iloc</code> selects rows and columns by integer position.</p> </li> </ul>"},{"location":"tutorials/pandas_basics/#5-filtering-rows","title":"5. Filtering Rows\u00b6","text":""},{"location":"tutorials/pandas_basics/#boolean-masking","title":"Boolean Masking\u00b6","text":"<p>You can create a boolean condition that returns <code>True/False</code> for each row, then use that mask to filter the DataFrame.</p>"},{"location":"tutorials/pandas_basics/#multiple-conditions","title":"Multiple Conditions\u00b6","text":"<p>Use bitwise operators <code>&amp;</code> (AND), <code>|</code> (OR), and <code>~</code> (NOT):</p>"},{"location":"tutorials/pandas_basics/#6-changing-values","title":"6. Changing Values\u00b6","text":""},{"location":"tutorials/pandas_basics/#61-assigning-with-loc","title":"6.1. Assigning with <code>.loc</code>\u00b6","text":""},{"location":"tutorials/pandas_basics/#62-assigning-with-iloc","title":"6.2. Assigning with <code>.iloc</code>\u00b6","text":""},{"location":"tutorials/pandas_basics/#63-vectorized-assignments","title":"6.3. Vectorized Assignments\u00b6","text":""},{"location":"tutorials/pandas_basics/#7-calculating-simple-statistics-and-value-counts","title":"7. Calculating Simple Statistics and Value Counts\u00b6","text":""},{"location":"tutorials/pandas_basics/#71-simple-statistics","title":"7.1. Simple Statistics\u00b6","text":""},{"location":"tutorials/pandas_basics/#72-value_counts","title":"7.2. <code>value_counts()</code>\u00b6","text":""},{"location":"tutorials/pandas_basics/#8-grouping-and-aggregation","title":"8. Grouping and Aggregation\u00b6","text":"<p><code>.groupby()</code> allows you to split data into groups based on some criteria, apply functions to each group, and combine results.</p>"},{"location":"tutorials/pandas_basics/#9-merging-joining-dataframes","title":"9. Merging / Joining DataFrames\u00b6","text":""},{"location":"tutorials/pandas_basics/#91-the-merge-method","title":"9.1. The <code>merge()</code> Method\u00b6","text":""},{"location":"tutorials/pandas_basics/#92-joins-on-different-column-names","title":"9.2. Joins on Different Column Names\u00b6","text":""},{"location":"tutorials/pandas_basics/#10-exercises","title":"10. Exercises\u00b6","text":"<ol> <li><p>Create a DataFrame from a dictionary of lists with at least three columns.</p> </li> <li><p>Load a CSV file into a DataFrame and inspect its first few rows.</p> </li> <li><p>Filter rows where a numeric column exceeds a certain threshold.</p> </li> <li><p>Perform a group-by operation and calculate the mean of another column.</p> </li> <li><p>Merge two DataFrames on a common key.</p> </li> </ol>"},{"location":"tutorials/python_basics/","title":"Tutorial Python","text":"<p>Are you still new to Python? Or your python skill is a bit rusty?</p> <p>Then, feel free to use this to learn or refresh Python basics! You don't have to submit this notebook. It is just for your own reference.</p> <p>We begin by focusing on constructing and executing Python scripts. This fundamental skill allows you to instruct your computer to perform a multitude of tasks. As you master this technique, you will be able to transform abstract ideas into executable programs, opening up a world of possibilities for automation and efficient problem-solving.</p> <p>Moving forward, we will dive into loops and conditional statements. These powerful constructs enable your programs to dynamically react to varying scenarios and diverse data inputs. Leveraging these tools allows for the creation of responsive and adaptive code, which is essential for tackling complex challenges.</p> <p>Towards the end of the module, we'll explore the concept of reusable functions. Crafting well-structured functions not only enhances the modularity and reusability of your code, but also simplifies maintenance. This skill is paramount for reducing redundancy, streamlining debugging, and ensuring that your code remains clear and adaptable.</p> <p>We will also cover file I/O, loading data, list comprehensions, lambda functions, and installing packages.</p> <p>You can comment in Python using <code>#</code>. Comments are ignored by the Python interpreter.</p> <p>To define a variable, assign a value to a variable name. Python will automatically detect the data type. The syntax is <code>variable_name = value</code>. You can print a variable using the <code>print(variable_name)</code> function.</p> In\u00a0[\u00a0]: Copied! <pre># This is a comment and will be ignored by the interpreter\na_variable = \"the value\"\n# This is another comment and will also be ignored by the interpreter\nprint(a_variable)\n</pre> # This is a comment and will be ignored by the interpreter a_variable = \"the value\" # This is another comment and will also be ignored by the interpreter print(a_variable)  <p>Variables can be reassigned with a different value and data type. For example, you can change an integer to a string:</p> In\u00a0[\u00a0]: Copied! <pre>a_variable = 10\nprint(a_variable)  # Output: 10\na_variable = \"Now I'm a string!\"\nprint(a_variable)  # Output: Now I'm a string!\n</pre> a_variable = 10 print(a_variable)  # Output: 10 a_variable = \"Now I'm a string!\" print(a_variable)  # Output: Now I'm a string!  <p>Q: Create two variables and then swap their values. Print the variables before and after the swap.</p> <ul> <li>For example if <code>a = 1</code> and <code>b = 2</code>, after the swap <code>a = 2</code> and <code>b = 1</code>.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  In\u00a0[\u00a0]: Copied! <pre>print(\"Hello, World!\")  # Output: Hello, World!\n\n# Example of print with concatenation\na_planet = \"Earth\"\nprint(\"Hello from \" + a_planet + \"!\")\n# Output: Hello from Earth!\n\n# Example of print with comma separated values\na_planet = \"Mars\"\nanother_planet = \"Jupiter\"\nprint(\"Hello from\", a_planet, \"and\", another_planet + \"!\")\n# Output: Hello from Mars and Jupiter!\n</pre> print(\"Hello, World!\")  # Output: Hello, World!  # Example of print with concatenation a_planet = \"Earth\" print(\"Hello from \" + a_planet + \"!\") # Output: Hello from Earth!  # Example of print with comma separated values a_planet = \"Mars\" another_planet = \"Jupiter\" print(\"Hello from\", a_planet, \"and\", another_planet + \"!\") # Output: Hello from Mars and Jupiter!   <p>Another way to format text is using the f-string method. You can use f-string by adding an <code>f</code> before the string and then use curly braces <code>{}</code> to insert variables.</p> In\u00a0[\u00a0]: Copied! <pre># Example of f-string print\na_planet = \"Earth\"\nanother_planet = \"Mars\"\nprint(f\"Hello from {a_planet} and {another_planet}!\")  # Output: Hello from Earth and Mars!\n</pre> # Example of f-string print a_planet = \"Earth\" another_planet = \"Mars\" print(f\"Hello from {a_planet} and {another_planet}!\")  # Output: Hello from Earth and Mars!  In\u00a0[\u00a0]: Copied! <pre># Declaring an integer and a float variable\nx = 10 \ny = 3.0\nk = x + y + 100\n\n# The value stored in the variable can be inspected by using print statement. \n# Type of a variable var can be checked by calling type(var) \nprint(\"The value of x is\", x, \"and it is of type\", type(x))\n\n# f-strings can be used to write more elegant print statement. \nprint(f\"The value of y is {y} and it is of type {type(y)}\")\nprint(f\"The value of k is {k} and it is of type {type(k)}\")\n\n# casting int to float\nprint(f\"x is of type {type(x)}\")\nx = float(x)\nprint(f\"x is of type {type(x)} after casting\")\n</pre> # Declaring an integer and a float variable x = 10  y = 3.0 k = x + y + 100  # The value stored in the variable can be inspected by using print statement.  # Type of a variable var can be checked by calling type(var)  print(\"The value of x is\", x, \"and it is of type\", type(x))  # f-strings can be used to write more elegant print statement.  print(f\"The value of y is {y} and it is of type {type(y)}\") print(f\"The value of k is {k} and it is of type {type(k)}\")  # casting int to float print(f\"x is of type {type(x)}\") x = float(x) print(f\"x is of type {type(x)} after casting\")  <p>Numbers can also be written in different formats. For example, you can use scientific notation: <code>z = 1.23e-4</code> (equivalent to 0.000123).</p> In\u00a0[\u00a0]: Copied! <pre># Numbers can also be written in scientific notation\nx = 1.05e6\nprint(f\"x is {x} and it is of type {type(x)}\")\n\n# You can also use _ to make the number more readable\nx = 1_000_000\nprint(f\"x is {x} and it is of type {type(x)}\")\n</pre> # Numbers can also be written in scientific notation x = 1.05e6 print(f\"x is {x} and it is of type {type(x)}\")  # You can also use _ to make the number more readable x = 1_000_000 print(f\"x is {x} and it is of type {type(x)}\")  <p>You can perform arithmetic operations on numbers. The basic arithmetic operators are <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>//</code> (integer division), and <code>%</code> (modulo).</p> In\u00a0[\u00a0]: Copied! <pre># Arithmetic operators\n\n# Addition\nz = x + y\nprint(f\"Adding x and y gives {z}\")\n\n# Subtraction\nz = x - y\nprint(f\"Subtracting y from x gives {z}\")\n\n# Multiplication\nz = x * y\nprint(f\"Multiplying x and y gives {z}\")\n\n# Division\nz = x / y \nprint(f\"x divided by y gives {z}\") \n\n# Floor Division\nz = x // y \nprint(f\"x divided by y gives {z} as quotient\") \n\n# Modulus Operator\nz = x % y \nprint(f\"x divided by y gives {z} as reminder\")\n\n# Exponentiation\nz = x ** y\nprint(f\"x raised to y gives {z}\") \n\n# self increment by 1\nx = x + 1\n# This is equivalent to x += 1\nprint(f\"x + 1 gives {x}\")\n</pre> # Arithmetic operators  # Addition z = x + y print(f\"Adding x and y gives {z}\")  # Subtraction z = x - y print(f\"Subtracting y from x gives {z}\")  # Multiplication z = x * y print(f\"Multiplying x and y gives {z}\")  # Division z = x / y  print(f\"x divided by y gives {z}\")   # Floor Division z = x // y  print(f\"x divided by y gives {z} as quotient\")   # Modulus Operator z = x % y  print(f\"x divided by y gives {z} as reminder\")  # Exponentiation z = x ** y print(f\"x raised to y gives {z}\")   # self increment by 1 x = x + 1 # This is equivalent to x += 1 print(f\"x + 1 gives {x}\")  In\u00a0[\u00a0]: Copied! <pre># True and False are the key words that represent bool values in python\na = True\nb = False\n\nprint(f\"a is {a} and b is {b}\")\nprint(f\"Type of variable a and b is {type(a)}\")\n\n# None in python represents the absence of something; similar to null value\nc = None \nprint(f\"c is {c} and is of type {type(c)}\")\n\n# Any non-zero integer value is true and zero is false.\n# Also anything with a non-zero length is true and empty sequences are false.\n</pre> # True and False are the key words that represent bool values in python a = True b = False  print(f\"a is {a} and b is {b}\") print(f\"Type of variable a and b is {type(a)}\")  # None in python represents the absence of something; similar to null value c = None  print(f\"c is {c} and is of type {type(c)}\")  # Any non-zero integer value is true and zero is false. # Also anything with a non-zero length is true and empty sequences are false.  <p>You can use logical operators <code>and</code>, <code>or</code>, and <code>not</code> to combine boolean values.</p> In\u00a0[\u00a0]: Copied! <pre># logical operators \n\n# and, or and not operate on bool variables\n# OR operator: Gives True when either of the expressions evaluates to True\n# expr1 or expr2\nprint(f\"a or b is {a or b}\")\nprint(f\"a or a is {a or a}\")\nprint(f\"b or b is {b or b}\")\n\n# AND operator: Gives True when both the expressions evaluates to True\n# expr1 and expr2\nprint(f\"a and b is {a and b}\")\nprint(f\"a and a is {a and a}\")\nprint(f\"b and b is {b and b}\")\n\n# NOT operator: negates a bool\n# not expr1\nprint(f\"Not of a is {not a}\")\n</pre> # logical operators   # and, or and not operate on bool variables # OR operator: Gives True when either of the expressions evaluates to True # expr1 or expr2 print(f\"a or b is {a or b}\") print(f\"a or a is {a or a}\") print(f\"b or b is {b or b}\")  # AND operator: Gives True when both the expressions evaluates to True # expr1 and expr2 print(f\"a and b is {a and b}\") print(f\"a and a is {a and a}\") print(f\"b and b is {b and b}\")  # NOT operator: negates a bool # not expr1 print(f\"Not of a is {not a}\")  <p>Comparison operators result in boolean values. The comparison operators are <code>==</code> (equal), <code>!=</code> (not equal), <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, and <code>&lt;=</code>.</p> <p>You can also combine comparison operators to create more complex conditions, such as <code>x &gt; 5 and x &lt; 10</code>, which checks if <code>x</code> is between 5 and 10.</p> <p>Additionally, logical operators can be used in conjunction with comparison operators to create more complex conditions, for example: <code>if x &gt; 5 and x &lt; 10: print(\"x is between 5 and 10\")</code>.</p> In\u00a0[\u00a0]: Copied! <pre># comparison operators\n\nx = 10\ny = 3.0\nz = 5\n\n# greater that, less than, greater than equal to and lesser than equal to\nx &gt; y\nx &gt;= y\nx &lt; y\nx &lt;= y\n\n# equals and not equals\nx == y\nx != y\n\n# Chained Expressions \nx &gt; y &gt; z \n(x &gt; y) or (x &gt; z)\n</pre> # comparison operators  x = 10 y = 3.0 z = 5  # greater that, less than, greater than equal to and lesser than equal to x &gt; y x &gt;= y x &lt; y x &lt;= y  # equals and not equals x == y x != y  # Chained Expressions  x &gt; y &gt; z  (x &gt; y) or (x &gt; z)  In\u00a0[\u00a0]: Copied! <pre># strings are represented using single or double quotes\nfirst_name = \"Adam\" \nlast_name = 'Eve'\n\n# \\ is used to escape characters\nmiddle_name = 'zero\\'s'\n\n# string concatenation\nfull_name = first_name +' ' + middle_name + ' ' + last_name\n\nprint(f\"Full name is {full_name}\")\nprint(f\"Full name is of type {type(full_name)}\")\n\n# length of string\nlength = len(full_name)\nprint(f\"Length of full name is {length}\")\n</pre> # strings are represented using single or double quotes first_name = \"Adam\"  last_name = 'Eve'  # \\ is used to escape characters middle_name = 'zero\\'s'  # string concatenation full_name = first_name +' ' + middle_name + ' ' + last_name  print(f\"Full name is {full_name}\") print(f\"Full name is of type {type(full_name)}\")  # length of string length = len(full_name) print(f\"Length of full name is {length}\")  <p>Strings can be indexed and sliced similar to lists and tuples.</p> <p>List indexing is discussed in the list section</p> <p>You can 'multiply' a string by an integer to repeat it.</p> In\u00a0[\u00a0]: Copied! <pre>repeated_string = \"Hello \" * 10\n\nprint(f\"repeated_string is {repeated_string}\")\n</pre> repeated_string = \"Hello \" * 10  print(f\"repeated_string is {repeated_string}\")  <p>Some characters have special meanings in strings. For example, <code>\\n</code> represents a newline character, and <code>\\t</code> represents a tab character. You can use <code>r</code> before the string to treat it as a raw string, ignoring special characters.</p> In\u00a0[\u00a0]: Copied! <pre># characters with special meaning\n# \\n - newline\n# \\t - tab\n# \\\\ - backslash\n# examples\nprint(\"Hello\\nWorld\")\nprint(\"Hello\\tWorld\")\nprint(\"Hello\\\\World\")\n</pre> # characters with special meaning # \\n - newline # \\t - tab # \\\\ - backslash # examples print(\"Hello\\nWorld\") print(\"Hello\\tWorld\") print(\"Hello\\\\World\")  <p>Casting is the process of converting one data type to another. You can cast strings to integers or floats, and vice versa. For example, you can convert a string to an integer using <code>int(\"5\")</code>.</p> In\u00a0[\u00a0]: Copied! <pre># casting str to int  \ntotal = int('1') + int('2')\nprint(f\"The value of total is {total} and it is of type {type(total)}\")\n\n# casting int to str\ntotal = str(1) + str(2)\nprint(f\"The value of total is {total} and it is of type {type(total)}\")\n</pre> # casting str to int   total = int('1') + int('2') print(f\"The value of total is {total} and it is of type {type(total)}\")  # casting int to str total = str(1) + str(2) print(f\"The value of total is {total} and it is of type {type(total)}\")  In\u00a0[\u00a0]: Copied! <pre># empty string is considered as False\nempty_string = ''\nbool(empty_string)\n</pre> # empty string is considered as False empty_string = '' bool(empty_string)  <p>To replace a part of a string, you can use the <code>replace()</code> method. To split a string into a list of substrings, you can use the <code>split()</code> method.</p> In\u00a0[\u00a0]: Copied! <pre># replace method\ns = \"Hello, World!\"\ns = s.replace(\"World\", \"Universe\")\nprint(s)\n</pre> # replace method s = \"Hello, World!\" s = s.replace(\"World\", \"Universe\") print(s)  <p>Note that the result of the f-string composition is a string.</p> In\u00a0[\u00a0]: Copied! <pre># f-string composition is a string\nhelloworld = f\"Hello, World!\"\nfull_sentence = f\"This is a full sentence. {helloworld} and it is of type {type(helloworld)}\"\nprint(full_sentence)\n</pre> # f-string composition is a string helloworld = f\"Hello, World!\" full_sentence = f\"This is a full sentence. {helloworld} and it is of type {type(helloworld)}\" print(full_sentence)  <p>Q: Create a sentence about the weather using variables for the temperature and the weather condition.</p> <ul> <li>Print the sentence</li> <li>Example: \"The temperature is 25 degrees and it is sunny.\"</li> <li>Then use replace to change the weather condition to \"cloudy\" and print the new sentence.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  In\u00a0[\u00a0]: Copied! <pre># Arrays are implemented as lists in python\n\n# creating empty list\nnames = []\nnames = list()\n\n# list of strings\nnames = ['Zach', 'Jay']\nprint(names)\n\n# list of intergers\nnums = [1, 2, 3, 4, 5]\nprint(nums)\n\n# list of different data types\nl = ['Zach', 1, True, None]\nprint(l)\n\n# list of lists\nll = [[1, 3], [2, 3], [3, 4]]\n\n# finding the length of list\nlength = len(l)\nprint(length)\n</pre> # Arrays are implemented as lists in python  # creating empty list names = [] names = list()  # list of strings names = ['Zach', 'Jay'] print(names)  # list of intergers nums = [1, 2, 3, 4, 5] print(nums)  # list of different data types l = ['Zach', 1, True, None] print(l)  # list of lists ll = [[1, 3], [2, 3], [3, 4]]  # finding the length of list length = len(l) print(length)  <p>You can repeat a list or tuple by multiplying it by an integer.</p> In\u00a0[\u00a0]: Copied! <pre># You can repeat a list or tuple by multiplying it by an integer.\nl = [1, 2, 3] * 3\nprint(l)\n\nt = (1, 2, 3) * 3\nprint(t)\n</pre> # You can repeat a list or tuple by multiplying it by an integer. l = [1, 2, 3] * 3 print(l)  t = (1, 2, 3) * 3 print(t)  <p>You can change the content of a list by assigning new values to its elements. You can also add elements to a list using the <code>append()</code> method or extend a list with another list using the <code>extend()</code> method.</p> In\u00a0[\u00a0]: Copied! <pre># Lists are mutable\n\nnames = names + ['Ravi']\nnames.append('Richard')\nnames.extend(['Abi', 'Kevin'])\nprint(names)\n</pre> # Lists are mutable  names = names + ['Ravi'] names.append('Richard') names.extend(['Abi', 'Kevin']) print(names)  <p>An element or a subset of a list can be accessed using element's index or slice of indices.</p> <p>The same notation applies for strings but at the character level.</p> In\u00a0[\u00a0]: Copied! <pre># some_list[index]\n# some_list[start_index: end_index(not included)]\n\nnumbers = [0, 1, 2, 3, 4, 5, 6]\n\n# indices start from 0 in python\nprint(f'The first element in numbers is {numbers[0]}')\nprint(f'The third element in numbers is {numbers[2]}')\n\nprint(f'Elements from 1st to 5th index are {numbers[1:6]}')\nprint(f'Elements from start to 5th index are {numbers[:6]}')\nprint(f'Elements from 4th index to end are {numbers[4:]}')\n\nprint(f'Last Element is {numbers[-1]}')\nprint(f'Last four element are {numbers[-4:]}')\n\n# changing 1st element in the numbers list\nnumbers[0] = 100\nprint(numbers)\n\n# changing first 3 numbers\nnumbers[0: 3] = [100, 200, 300]\nprint(numbers)\n</pre> # some_list[index] # some_list[start_index: end_index(not included)]  numbers = [0, 1, 2, 3, 4, 5, 6]  # indices start from 0 in python print(f'The first element in numbers is {numbers[0]}') print(f'The third element in numbers is {numbers[2]}')  print(f'Elements from 1st to 5th index are {numbers[1:6]}') print(f'Elements from start to 5th index are {numbers[:6]}') print(f'Elements from 4th index to end are {numbers[4:]}')  print(f'Last Element is {numbers[-1]}') print(f'Last four element are {numbers[-4:]}')  # changing 1st element in the numbers list numbers[0] = 100 print(numbers)  # changing first 3 numbers numbers[0: 3] = [100, 200, 300] print(numbers)  <p>Q: Define a list of four items representing your favorite hobbies/activities.</p> <ul> <li>Print only the second and third items using list slicing.</li> </ul> In\u00a0[1]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  <p>On the other hand, tuples are immutable, meaning that you cannot change the content of a tuple once it is created.</p> In\u00a0[\u00a0]: Copied! <pre># Tuples are immutable lists. They are created using () instead of [].\n\nnames = tuple()\nnames = ('Zach', 'Jay') \nprint(names[0])\n\n# trying to alter the tuple gives an error\nnames[0] = 'Richard'\n\n# similar to tuples, strings are also immutable\n</pre> # Tuples are immutable lists. They are created using () instead of [].  names = tuple() names = ('Zach', 'Jay')  print(names[0])  # trying to alter the tuple gives an error names[0] = 'Richard'  # similar to tuples, strings are also immutable  In\u00a0[\u00a0]: Copied! <pre># empty lists and tuples are considered as False\nemptyList = []\nemptyTuple = ()\nprint(f\"emptyList is {bool(emptyList)} and emptyTuple is {bool(emptyTuple)}\")\n</pre> # empty lists and tuples are considered as False emptyList = [] emptyTuple = () print(f\"emptyList is {bool(emptyList)} and emptyTuple is {bool(emptyTuple)}\") <p>The <code>split()</code> method can be used to split a string into a list of substrings. By default, it splits the string by spaces, but you can specify a different separator.</p> In\u00a0[\u00a0]: Copied! <pre># split method\ns = \"Hello, World!\"\nwords = s.split()\nprint(words)\n\n# split with a different separator\ns = \"apple,banana,orange\"\nfruits = s.split(',')\nprint(fruits)\n</pre> # split method s = \"Hello, World!\" words = s.split() print(words)  # split with a different separator s = \"apple,banana,orange\" fruits = s.split(',') print(fruits)  <p>You can use the <code>join()</code> method to concatenate a list of strings into a single string.</p> In\u00a0[\u00a0]: Copied! <pre># join method\nwords = ['Hello', 'World']\ns = ' '.join(words)\nprint(s)\n</pre> # join method words = ['Hello', 'World'] s = ' '.join(words) print(s)  <p>Q: See what happens when you try to change the content of a tuple.</p> <ul> <li>Since you cannot change the content of a tuple, you need to create a new tuple with the new content.</li> <li>Create a new tuple with 3 elements based on the first three elements of the tuple you defined above.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  In\u00a0[\u00a0]: Copied! <pre># hash maps in python are called Dictionaries\n# dict{key: value}\n\n# Empty dictionary\nphonebook = dict()\n\n# contruction dict using sequences of key-value pairs\ndict([('sape', 4139), ('guido', 4127), ('jack', 4098)])\n\n# Dictionary with one item\nphonebook = {'jack': 4098}\n\n# Add another item\nphonebook['guido'] = 4127\n\nprint(phonebook)\nprint(phonebook['jack'])\nprint(phonebook.items())\nprint(phonebook.keys())\nprint(phonebook.values())\n\nprint('jack' in phonebook)\nprint('Kevin' in phonebook)\n\n# Delete an item\ndel phonebook['jack'] \nprint(phonebook)\n</pre> # hash maps in python are called Dictionaries # dict{key: value}  # Empty dictionary phonebook = dict()  # contruction dict using sequences of key-value pairs dict([('sape', 4139), ('guido', 4127), ('jack', 4098)])  # Dictionary with one item phonebook = {'jack': 4098}  # Add another item phonebook['guido'] = 4127  print(phonebook) print(phonebook['jack']) print(phonebook.items()) print(phonebook.keys()) print(phonebook.values())  print('jack' in phonebook) print('Kevin' in phonebook)  # Delete an item del phonebook['jack']  print(phonebook)  In\u00a0[\u00a0]: Copied! <pre># An empty dictionary is considered as False\nemptyDict = {}\nprint(f\"emptyDict is {bool(emptyDict)}\")\n</pre> # An empty dictionary is considered as False emptyDict = {} print(f\"emptyDict is {bool(emptyDict)}\")  <p>Q: Define a dictionary where the keys are the names of the planets and the values are their positions relative to the Sun (like 3 for Earth).</p> <ul> <li>Print the dictionary.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  In\u00a0[\u00a0]: Copied! <pre># Sets are unordered collection of unique elements\n# Sets are mutable\n\n# Empty set\ns = set()\n\n# set of integers\ns = {1, 2, 3, 4, 5}\nprint(f\"The set is {s}\")\n\n# set of mixed data types\ns = {1, 2.0, 'three'}\nprint(f\"The set is {s}\")\n\n# set of tuples\ns = {('a', 'b'), ('c', 'd')}\nprint(f\"The set is {s}\")\n\n# set operations\ns1 = {1, 2, 3, 4, 5}\ns2 = {4, 5, 6, 7, 8}\n\nprint(f\"s1 is {s1}\")\nprint(f\"s2 is {s2}\")\n\n# Union\ns3 = s1 | s2\nprint(f\"Union of s1 and s2 is {s3}\")\n\n# Intersection\ns3 = s1 &amp; s2\nprint(f\"Intersection of s1 and s2 is {s3}\")\n\n# Difference\ns3 = s1 - s2\nprint(f\"Difference of s1 and s2 is {s3}\")\n\n# Symmetric Difference\ns3 = s1 ^ s2\nprint(f\"Symmetric Difference of s1 and s2 is {s3}\")\n\n# Empty set is considered as False\nemptySet = set()\nprint(f\"emptySet is {bool(emptySet)}\")\n</pre> # Sets are unordered collection of unique elements # Sets are mutable  # Empty set s = set()  # set of integers s = {1, 2, 3, 4, 5} print(f\"The set is {s}\")  # set of mixed data types s = {1, 2.0, 'three'} print(f\"The set is {s}\")  # set of tuples s = {('a', 'b'), ('c', 'd')} print(f\"The set is {s}\")  # set operations s1 = {1, 2, 3, 4, 5} s2 = {4, 5, 6, 7, 8}  print(f\"s1 is {s1}\") print(f\"s2 is {s2}\")  # Union s3 = s1 | s2 print(f\"Union of s1 and s2 is {s3}\")  # Intersection s3 = s1 &amp; s2 print(f\"Intersection of s1 and s2 is {s3}\")  # Difference s3 = s1 - s2 print(f\"Difference of s1 and s2 is {s3}\")  # Symmetric Difference s3 = s1 ^ s2 print(f\"Symmetric Difference of s1 and s2 is {s3}\")  # Empty set is considered as False emptySet = set() print(f\"emptySet is {bool(emptySet)}\")  <p>Q: Define two sets of strings with some common elements and some unique elements.</p> <ul> <li><p>Perform the following operations on the sets:</p> <ul> <li>Union</li> <li>Intersection</li> <li>Difference</li> </ul> </li> <li><p>Print the results.</p> </li> </ul> In\u00a0[21]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here <p>Flow control statements are used to control the flow of a program. They include <code>if</code>, <code>elif</code>, and <code>else</code> statements, <code>for</code> and <code>while</code> loops, and <code>break</code> and <code>continue</code> statements.</p> <p>Differently from other programming languages, Python uses indentation to define code blocks. The code block following an <code>if</code>, <code>elif</code>, <code>else</code>, <code>for</code>, or <code>while</code> statement must be indented.</p> <p>IMPORTANT: The standard indentation is 4 spaces. Always use the same number of spaces for indentation in the same block of code. Avoid using tabs for indentation. Most of the code editors are configured to automatically convert tabs to spaces.</p> In\u00a0[\u00a0]: Copied! <pre># if expr1:\n#     code1\n# elif expr2:\n#     code2\n#   .\n#   .\n#   .\n#   .\n# else:\n#     code_n\n\n# code1 is executed if expr1 is evaluated to true. Else it moves to expr2 and checks for true \n# condition and moves to the next if not true. \n# Finally if all the excpression's are false, code_n is executed\n\nx = int(input(\"Please enter an integer: \"))\n\nif x &lt; 0:\n    x = 0\n    print('Negative changed to zero')\nelif x == 0:\n    print('Zero')\nelif x == 1:\n    print('Single')\nelse:\n    print('More')\n</pre> # if expr1: #     code1 # elif expr2: #     code2 #   . #   . #   . #   . # else: #     code_n  # code1 is executed if expr1 is evaluated to true. Else it moves to expr2 and checks for true  # condition and moves to the next if not true.  # Finally if all the excpression's are false, code_n is executed  x = int(input(\"Please enter an integer: \"))  if x &lt; 0:     x = 0     print('Negative changed to zero') elif x == 0:     print('Zero') elif x == 1:     print('Single') else:     print('More')  <p>Q: Prompt the user to input a number and use an if-elif-else statement to check if the number is negative, zero, or positive.</p> <ul> <li>Print an appropriate message in each case.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  <p>If statements can be used in conjunction with comparison operators to create more complex conditions.</p> In\u00a0[\u00a0]: Copied! <pre>x = int(input(\"Please enter an integer: \"))\n\nif x &gt; 0 and x &lt; 10:\n    print(f'x={x} is a positive single digit number')\nelse:\n    print(f'x={x} is not a positive single digit number')\n</pre> x = int(input(\"Please enter an integer: \"))  if x &gt; 0 and x &lt; 10:     print(f'x={x} is a positive single digit number') else:     print(f'x={x} is not a positive single digit number')  In\u00a0[\u00a0]: Copied! <pre># for loop is used to iter over any iterable object\n\n# iterating over list\nfor name in ['Steve', 'Jill', 'Venus']:\n    print(name)\n</pre> # for loop is used to iter over any iterable object  # iterating over list for name in ['Steve', 'Jill', 'Venus']:     print(name)  <p>You can iterate over every character in a string.</p> In\u00a0[\u00a0]: Copied! <pre># iterating over string\nfor char in \"Hellooooo\":\n    print(char)\n</pre> # iterating over string for char in \"Hellooooo\":     print(char)  <p>To iterate over dictionary keys and values, you can use the <code>items()</code> method. The for loop can deal with multiple variables.</p> In\u00a0[\u00a0]: Copied! <pre># iterating over dict keys\nphone_nos = {\"Henry\": 6091237458,\n             \"James\": 1234556789, \n             \"Larry\": 5698327549, \n             \"Rocky\": 8593876589}\n\nfor name, no in phone_nos.items(): # items() returns a list of tuples\n    print(f\"{name}: {no}\")\n</pre> # iterating over dict keys phone_nos = {\"Henry\": 6091237458,              \"James\": 1234556789,               \"Larry\": 5698327549,               \"Rocky\": 8593876589}  for name, no in phone_nos.items(): # items() returns a list of tuples     print(f\"{name}: {no}\")   <p>To iterate over a sequence of numbers we use <code>range()</code> function.</p> <p>The <code>range()</code> function generates a sequence of numbers. It can take one, two, or three parameters. For example, <code>range(5)</code> generates a sequence of numbers from 0 to 4.</p> In\u00a0[\u00a0]: Copied! <pre>for i in range(5):\n    print(i)\n</pre> for i in range(5):     print(i)  <p>However, if you want to iterate over a sequence of numbers with a different starting point, you can use <code>range(start, stop, step)</code>. For example, <code>range(2, 10, 2)</code> generates a sequence of numbers from 2 to 20 with a step of 2:</p> In\u00a0[\u00a0]: Copied! <pre>for i in range(2, 20, 2):\n    print(i)\n</pre>  for i in range(2, 20, 2):     print(i)  <p>You can use the len() function to get the length of a list or a string and use it in the range() function.</p> In\u00a0[\u00a0]: Copied! <pre># using len of list/tuple in range\nnames = ['Steve', 'Rock', 'Harry']\nfor i in range(len(names)):\n    print(names[i])\n</pre> # using len of list/tuple in range names = ['Steve', 'Rock', 'Harry'] for i in range(len(names)):     print(names[i])  <p>Alternatively, if you want to have both the element in a sequences as well as an index, you can use the <code>enumerate()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre># using len of list/tuple in range\nnames = ['Steve', 'Rock', 'Harry']\nfor i in range(len(names)):\n    print(f\"{i}: {names[i]}\")\n</pre> # using len of list/tuple in range names = ['Steve', 'Rock', 'Harry'] for i in range(len(names)):     print(f\"{i}: {names[i]}\")  <p>Q: Using a for loop, print the square of each number in range(1, 6).</p> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  In\u00a0[\u00a0]: Copied! <pre># While loop executes as long as the condition remains true.\n# while cond1:\n#     pass\n\ni = 0\nwhile i &lt; 10:\n    print(i)\n    i += 1\n</pre> # While loop executes as long as the condition remains true. # while cond1: #     pass  i = 0 while i &lt; 10:     print(i)     i += 1  <p>Forever loop: The below code runs for ever (don't run it)</p> In\u00a0[\u00a0]: Copied! <pre># while True:\n#     print('Forever...')\n</pre> # while True: #     print('Forever...')  In\u00a0[\u00a0]: Copied! <pre># break statement breaks out of the the loop\nwhile True:\n    print('We\u2019re stuck in a loop...')\n    break # Break out of the while loop\nprint(\"not!\")\n</pre> # break statement breaks out of the the loop while True:     print('We\u2019re stuck in a loop...')     break # Break out of the while loop print(\"not!\")  <p>You can skip the current iteration without exiting the loop by using the <code>continue</code> statement.</p> In\u00a0[\u00a0]: Copied! <pre># continue statement skips a loop\nfor i in range(5):\n    continue \n    print(i)\n</pre> # continue statement skips a loop for i in range(5):     continue      print(i)  <p>This is more usefult when you have a condition to skip the current iteration but you don't want to exit the loop. For instance, you can skip the iteration if the number is even.</p> In\u00a0[\u00a0]: Copied! <pre>for i in range(10):\n    if i % 2 == 0: # Calculate the remainder of i divided by 2 and check if it is 0\n        continue\n    print(i)\n</pre> for i in range(10):     if i % 2 == 0: # Calculate the remainder of i divided by 2 and check if it is 0         continue     print(i)  <p>The <code>pass</code> statement is used as a placeholder for future code. When the <code>pass</code> statement is executed, nothing happens, but you avoid getting an error. It allows you to write minimal code structure without causing issues during execution, which is helpful in situations where code needs to be developed incrementally.</p> In\u00a0[\u00a0]: Copied! <pre>for i in range(10):\n    pass\n</pre> for i in range(10):     pass  <p>Q: Ask the user for an input, print that input, and then print the input in reverse.</p> <ul> <li>Use a while loop to repeat this process until the user inputs 'quit'.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  In\u00a0[\u00a0]: Copied! <pre># importing modules using import statement\nimport math\n\n# Import under an alias (avoid this pattern except with the most \n# common modules like pandas, numpy, etc.)\nimport math as m\n\n# Access components with pkg.fn\nm.pow(2, 3) \n\n# Import specific submodules/functions (not usually recommended \n# because it can be confusing to know where a function comes from)\nfrom math import pow\npow(2, 3)\n</pre> # importing modules using import statement import math  # Import under an alias (avoid this pattern except with the most  # common modules like pandas, numpy, etc.) import math as m  # Access components with pkg.fn m.pow(2, 3)   # Import specific submodules/functions (not usually recommended  # because it can be confusing to know where a function comes from) from math import pow pow(2, 3)  <p>Q: After importing the math module, can you import the random module and generate five random floating-point numbers between 0 and 1?</p> <ul> <li>Tip: use random.random() in a for loop</li> </ul> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  In\u00a0[\u00a0]: Copied! <pre># Functions in python are defined using key word \"def\"\n\n# simple function to print greetings `greet_word` is an optional argument with default value 'Hello'\ndef greet(name, greet_word='Hello'):\n    print(f\"{greet_word} {name}. How are you doing?\")\n\n# here greet has 'Hello' as default argument for greet_word\nprint(greet('James'))\nprint(greet(\"Steven\", greet_word=\"Howdy\"))\n\n# Observe that the function by default returns None\n</pre> # Functions in python are defined using key word \"def\"  # simple function to print greetings `greet_word` is an optional argument with default value 'Hello' def greet(name, greet_word='Hello'):     print(f\"{greet_word} {name}. How are you doing?\")  # here greet has 'Hello' as default argument for greet_word print(greet('James')) print(greet(\"Steven\", greet_word=\"Howdy\"))  # Observe that the function by default returns None  <p>To return a value from a function, you can use the <code>return</code> statement.</p> In\u00a0[\u00a0]: Copied! <pre># To return a value from a function, you can use the `return` statement.\ndef multiply_and_subtract_one(x, y):\n    return x * y - 1\n\nprint(multiply_and_subtract_one(2, 3))\n</pre> # To return a value from a function, you can use the `return` statement. def multiply_and_subtract_one(x, y):     return x * y - 1  print(multiply_and_subtract_one(2, 3))  <p>Q: Define a function called \"divide_by_two\" that takes a single parameter and returns half its value. Then test it with both integer and float inputs.</p> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  <p>Python supports recursive functions. A recursive function is a function that calls itself.</p> In\u00a0[\u00a0]: Copied! <pre># Function to print nth fibonacci number\ndef fib(n):\n    if n &lt;= 1:\n        return 1\n    else:\n        return fib(n-1)+fib(n-2)\nn = 10\nprint(f\"{n}th fibonacci number is {fib(n)}\")\n</pre> # Function to print nth fibonacci number def fib(n):     if n &lt;= 1:         return 1     else:         return fib(n-1)+fib(n-2) n = 10 print(f\"{n}th fibonacci number is {fib(n)}\")  In\u00a0[\u00a0]: Copied! <pre># Writing to a text file\nwith open(\"example.txt\", \"wt\") as f:\n    f.write(\"This is an example.\\n\")\n    f.write(\"This is another example in another line.\\n\")\n</pre> # Writing to a text file with open(\"example.txt\", \"wt\") as f:     f.write(\"This is an example.\\n\")     f.write(\"This is another example in another line.\\n\")   <p>You can load the content of a file using the <code>read()</code> method. You can also iterate over the lines of a file using a <code>for</code> loop. This will progressively load the file line by line.</p> In\u00a0[\u00a0]: Copied! <pre># Reading from a text file\nwith open(\"example.txt\", \"rt\") as f:\n    content = f.read()\n    print(content)\n\n# Read line by line\nwith open(\"example.txt\", \"rt\") as f:\n    for line in f:\n        print(line)\n</pre> # Reading from a text file with open(\"example.txt\", \"rt\") as f:     content = f.read()     print(content)  # Read line by line with open(\"example.txt\", \"rt\") as f:     for line in f:         print(line)   <p>Note that the last print resulted into 2 line breaks because the <code>print()</code> function adds a newline character at the end of the line that already contains a newline character.</p> In\u00a0[\u00a0]: Copied! <pre># Note that the last print resulted into 2 line breaks because the `print()` function adds a newline character at the end of the line that already contains a newline character.\n\n# You can strip the newline character by using the `strip()` method of the string object.\nwith open(\"example.txt\", \"rt\") as f:\n    for line in f:\n        print(line.strip())\n</pre> # Note that the last print resulted into 2 line breaks because the `print()` function adds a newline character at the end of the line that already contains a newline character.  # You can strip the newline character by using the `strip()` method of the string object. with open(\"example.txt\", \"rt\") as f:     for line in f:         print(line.strip())   <p>Append a file: You can append to a file by opening it in append mode. When you open a file in append mode, new data is written to the end of the file.</p> In\u00a0[\u00a0]: Copied! <pre>with open(\"example.txt\", \"at\") as f:\n    f.write(\"This is an appended line.\\n\")\n\nwith open(\"example.txt\", \"rt\") as f:\n    content = f.read()\n    print(content)\n</pre> with open(\"example.txt\", \"at\") as f:     f.write(\"This is an appended line.\\n\")  with open(\"example.txt\", \"rt\") as f:     content = f.read()     print(content)  <p>Q: Append user input to a file named \"user_input.txt\" and then read it again to verify the content was appended.</p> <ul> <li>Use the <code>with open(..., 'at')</code> approach.</li> <li>Then open the file again in read mode and print its contents.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  In\u00a0[\u00a0]: Copied! <pre># creating a json file\nimport json\ndata = {\n    \"name\": \"John\",\n    \"age\": 30,\n    \"city\": \"New York\"\n}\nwith open(\"data.json\", \"wt\") as f:\n    json.dump(data, f)\n\n# reading from a json file\nwith open(\"data.json\", \"rt\") as f:\n    data = json.load(f)\n    print(data)\n</pre> # creating a json file import json data = {     \"name\": \"John\",     \"age\": 30,     \"city\": \"New York\" } with open(\"data.json\", \"wt\") as f:     json.dump(data, f)  # reading from a json file with open(\"data.json\", \"rt\") as f:     data = json.load(f)     print(data)  In\u00a0[\u00a0]: Copied! <pre># creating a csv file\nimport csv\nwith open(\"example.csv\", \"w\") as f:\n    writer = csv.writer(f)\n    writer.writerow([\"Name\", \"City\"])\n    writer.writerow([\"John\", \"New York\"])\n    writer.writerow([\"Peter\", \"Los Angeles\"])\n\n# reading from a csv file\nwith open(\"example.csv\", \"r\") as f:\n    reader = csv.reader(f)\n    for row in reader:\n        print(row)\n</pre> # creating a csv file import csv with open(\"example.csv\", \"w\") as f:     writer = csv.writer(f)     writer.writerow([\"Name\", \"City\"])     writer.writerow([\"John\", \"New York\"])     writer.writerow([\"Peter\", \"Los Angeles\"])  # reading from a csv file with open(\"example.csv\", \"r\") as f:     reader = csv.reader(f)     for row in reader:         print(row)    In\u00a0[\u00a0]: Copied! <pre># List comprehension is a concise way to create lists\n# [expr for var in iterable]\n\nresult = [x**2 for x in range(10)]\nprint(f\"x squared is {result}\")\n</pre> # List comprehension is a concise way to create lists # [expr for var in iterable]  result = [x**2 for x in range(10)] print(f\"x squared is {result}\")   <p>You can add a condition to the list comprehension to filter the elements. For example, you can create a list of even numbers from 0 to 9.</p> In\u00a0[\u00a0]: Copied! <pre># List comprehension with condition\nresult = [x**2 for x in range(10) if x % 2 == 0]\nprint(f\"x squared when x is even is {result}\")\n</pre>  # List comprehension with condition result = [x**2 for x in range(10) if x % 2 == 0] print(f\"x squared when x is even is {result}\")  <p>Q: Use a list comprehension to generate the cubes of numbers from 1 to 7.</p> <ul> <li>Then filter out cubes that are divisible by 3 and print the result.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  <p>You can have complex nested loops in list comprehensions. For example, you can create a list of tuples by combining elements from two lists.</p> In\u00a0[\u00a0]: Copied! <pre># List comprehension with nested loops\nresult = [(x, y) for x in range(3) for y in range(3)]\nprint(f\"Cartesian product of [0, 1, 2] is {result}\")\n</pre>  # List comprehension with nested loops result = [(x, y) for x in range(3) for y in range(3)] print(f\"Cartesian product of [0, 1, 2] is {result}\")   <p>You can combine nested loops with conditions.</p> In\u00a0[\u00a0]: Copied! <pre># List comprehension with nested loops and condition\nresult = [(x, y) for x in range(3) for y in range(3) if x != y]\nprint(f\"Cartesian product of [0, 1, 2] without diagonal is {result}\")\n</pre>  # List comprehension with nested loops and condition result = [(x, y) for x in range(3) for y in range(3) if x != y] print(f\"Cartesian product of [0, 1, 2] without diagonal is {result}\")  <p>Nested loops can be useful to flatten a list of lists. For example, you can flatten a list of lists into a single list.</p> In\u00a0[\u00a0]: Copied! <pre>list_of_lists = [[1, 2], [3, 4], [5, 6]]\n\nresult = [item for sublist in list_of_lists for item in sublist]\nprint(f\"Flattened list is {result}\")\n</pre> list_of_lists = [[1, 2], [3, 4], [5, 6]]  result = [item for sublist in list_of_lists for item in sublist] print(f\"Flattened list is {result}\")  In\u00a0[\u00a0]: Copied! <pre># Lambda functions are anonymous functions\n# lambda arguments: expression\nf = lambda x: x**2\nprint(f\"Square of 10 is {f(10)}\")\n</pre> # Lambda functions are anonymous functions # lambda arguments: expression f = lambda x: x**2 print(f\"Square of 10 is {f(10)}\")   <p>Lambda functions are used with map, filter and reduce functions.</p> <p>The <code>map()</code> function takes in a function and a list. The function is applied to every item in the list. It returns a list of the results.</p> In\u00a0[\u00a0]: Copied! <pre># map applies a function to all the items in an input list\n# map(function, iterable)\nresult = list(map(lambda x: x**2, range(10)))\nprint(f\"Square of 0 to 9 is {result}\")\n</pre>  # map applies a function to all the items in an input list # map(function, iterable) result = list(map(lambda x: x**2, range(10))) print(f\"Square of 0 to 9 is {result}\")   <p>The <code>filter()</code> function takes in a function and a list. The function is applied to every item in the list. It returns a list of items for which the function returns True.</p> In\u00a0[\u00a0]: Copied! <pre># filter creates a list of elements for which a function returns true\n# filter(function, iterable)\nresult = list(filter(lambda x: x % 2 == 0, range(10)))\nprint(f\"Even numbers in 0 to 9 are {result}\")\n</pre>  # filter creates a list of elements for which a function returns true # filter(function, iterable) result = list(filter(lambda x: x % 2 == 0, range(10))) print(f\"Even numbers in 0 to 9 are {result}\")   <p>The <code>reduce()</code> function is defined in the <code>functools</code> module. It applies a rolling computation to sequential pairs of values in a list. For example, you can use the <code>reduce()</code> function to calculate the sum of a list of numbers.</p> In\u00a0[\u00a0]: Copied! <pre># reduce applies a rolling computation to sequential pairs of values in a list\n# reduce(function, iterable)\nfrom functools import reduce\nresult = reduce(lambda x, y: x + y, range(10))\n\n# The above code is equivalent to sum(range(10))\nprint(f\"Sum of 0 to 9 is {result}\")\n</pre>  # reduce applies a rolling computation to sequential pairs of values in a list # reduce(function, iterable) from functools import reduce result = reduce(lambda x, y: x + y, range(10))  # The above code is equivalent to sum(range(10)) print(f\"Sum of 0 to 9 is {result}\")   <p>Lambda functions have other uses as well, such as in sorting and in defining functions that take functions as arguments.</p> In\u00a0[\u00a0]: Copied! <pre>pairs = [(1, 'one'), (2, 'two'), (3, 'three'), (4, 'four')]\npairs.sort(key=lambda pair: pair[1])\nprint(f\"Sorted pairs based on second element is {pairs}\")\n</pre> pairs = [(1, 'one'), (2, 'two'), (3, 'three'), (4, 'four')] pairs.sort(key=lambda pair: pair[1]) print(f\"Sorted pairs based on second element is {pairs}\")  <p>If you are on colab, you can install extra packages using</p> In\u00a0[\u00a0]: Copied! <pre>!pip install package_name\n</pre> !pip install package_name  <p>If you are on jupyter notebook, you can install extra packages using:</p> In\u00a0[\u00a0]: Copied! <pre>pip install package_name\n</pre> pip install package_name  In\u00a0[\u00a0]: Copied! <pre># TRY IT OUT\n</pre> # TRY IT OUT  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE"},{"location":"tutorials/python_basics/#tutorial-python","title":"Tutorial Python\u00b6","text":""},{"location":"tutorials/python_basics/#objectives","title":"Objectives\u00b6","text":"<ul> <li>Construct and Execute Python Scripts: Develop the ability to write and run basic Python scripts that adhere to standard syntax, establishing a firm foundation in programming.</li> <li>Apply Loops and Conditional Statements: Implement loops and conditional constructs to manipulate data effectively and solve intricate problems.</li> <li>Create Reusable Functions: Design and integrate Python functions that promote modularity and reusability, enhancing both code efficiency and maintainability.</li> </ul>"},{"location":"tutorials/python_basics/#the-topics-covered-in-this-module","title":"The topics covered in this module:\u00b6","text":"<ul> <li>Variables and built in datatypes</li> <li>Printing and user input</li> <li>Operators</li> <li>Lists and Tuples</li> <li>Dictionaries</li> <li>Sets</li> <li>Flow control</li> <li>Importing Modules</li> <li>Defining Functions</li> <li>File I/O</li> <li>Loading Data (JSON, CSV)</li> <li>List Comprehensions</li> <li>Lambda Functions</li> <li>Installing Packages</li> </ul>"},{"location":"tutorials/python_basics/#1-variables-and-built-in-data-types","title":"1. Variables and built-in data types\u00b6","text":""},{"location":"tutorials/python_basics/#11-basic-and-variables","title":"1.1 Basic and variables\u00b6","text":""},{"location":"tutorials/python_basics/#12-printing-composed-variables-and-text","title":"1.2 Printing composed variables and text\u00b6","text":"<p>Use the <code>print()</code> function to print variables and text. You can use the <code>+</code> operator to concatenate strings. You can also concatenate variables and strings by separating them with a comma.</p>"},{"location":"tutorials/python_basics/#13-int-and-float","title":"1.3 Int and Float\u00b6","text":"<p>Numbers can be stored in variables. Integers are whole numbers, while floats are numbers with decimal points. Example: <code>x = 5</code> or <code>y = 3.14</code>.</p>"},{"location":"tutorials/python_basics/#14-booleans-and-none","title":"1.4 Booleans and None\u00b6","text":"<p>Bolean values are <code>True</code> and <code>False</code> and are used to represent binary values. <code>None</code> is a special value that represents the absence of a value, when tested, normally it results to <code>False</code>.</p>"},{"location":"tutorials/python_basics/#15-strings","title":"1.5 Strings\u00b6","text":"<p>Strings are sequences of characters, e.g., pieces of text. You can create them by enclosing characters in quotes. Python treats single quotes the same as double quotes.</p> <p>Strings can be concatenated using the <code>+</code> operator. You can also multiply strings by integers to repeat them.</p> <p>You may need to escape special characters in strings using a backslash <code>\\</code>. For example, to include a quote in a string, you can use <code>\\'</code>.</p> <p>You can use the <code>len()</code> function to get the length of a string.</p>"},{"location":"tutorials/python_basics/#14-lists-and-tuples","title":"1.4 Lists and Tuples\u00b6","text":"<p>Lists are collections of items that are ordered and changeable. Tuples are collections of items that are ordered and unchangeable.</p> <p>They can contain any type of object, even other lists or tuples. They can be used for instance to store a sequence of numbers, strings, or a mix of both.</p>"},{"location":"tutorials/python_basics/#15-dictionary","title":"1.5 Dictionary\u00b6","text":"<p>Dictionary is a collection of key-value pairs. It is unordered, changeable and indexed. Dictionaries are written with curly brackets, and have keys and values.</p> <p>They can be used for example to associate a name with a phone number.</p>"},{"location":"tutorials/python_basics/#16-sets","title":"1.6 Sets\u00b6","text":"<p>Sets are unordered collections of unique elements. They are used to store multiple items in a single variable. Sets are written with curly brackets.</p>"},{"location":"tutorials/python_basics/#2-flow-control-statements","title":"2. Flow control statements\u00b6","text":""},{"location":"tutorials/python_basics/#21-if-elif-else","title":"2.1 if... elif... else...\u00b6","text":"<p>The <code>if</code> statement is used to test a condition. If the condition is true, the code block following the <code>if</code> statement is executed. The <code>elif</code> statement is used to test multiple conditions. If the condition is true, the code block following the <code>elif</code> statement is executed. The <code>else</code> statement is used to execute a block of code if the condition is false.</p> <p>For instance, this is needed if you want to check if a number is positive, negative, or zero and then decide what to do based on the result.</p>"},{"location":"tutorials/python_basics/#22-for-loops","title":"2.2 For loops\u00b6","text":"<p>A <code>for</code> loop is used for iterating over a sequence (that is either a list, a tuple, a dictionary, a set, or a string).</p> <p>In general, loops are used to repeat a block of code multiple times. The <code>for</code> loop iterates over a sequence of elements, such as a list or a tuple. The <code>for</code> loop can also iterate over a string, which is a sequence of characters.</p>"},{"location":"tutorials/python_basics/#23-while-loop","title":"2.3 While Loop\u00b6","text":"<p>With the <code>while</code> loop we can execute a set of statements as long as a condition is true.</p>"},{"location":"tutorials/python_basics/#24-break-continue-and-pass-statements","title":"2.4 Break, Continue and Pass statements\u00b6","text":"<p><code>break</code> statement is used to exit a loop when a condition is met. <code>continue</code> statement is used to skip the current block and return to the <code>for</code> or <code>while</code> statement. <code>pass</code> statement is used as a placeholder for future code.</p>"},{"location":"tutorials/python_basics/#3-importing-modules","title":"3. Importing Modules\u00b6","text":"<p>Python has a lot of built-in modules that you can use. You can import these modules using the <code>import</code> statement.</p> <p>The same command can be used to import installed packages.</p>"},{"location":"tutorials/python_basics/#4-defining-functions","title":"4. Defining Functions\u00b6","text":"<p>A function is a block of code that only runs when it is called. You can pass data, known as parameters, into a function. A function can return data as a result.</p>"},{"location":"tutorials/python_basics/#5file-io","title":"5.File I/O\u00b6","text":"<p>Python has functions for file handling and manipulation. The key function to work with files in Python is the <code>open()</code> function. The open() function takes two parameters; filename, and mode. The mode parameter specifies whether you want to read, write, or append to the file. Common modes are 'r' for reading, 'w' for writing (which overwrites the file), and 'a' for appending.</p> <p>You can also define the format of the file by adding 't' for text or 'b' for binary.</p>"},{"location":"tutorials/python_basics/#6-loading-data","title":"6 Loading Data\u00b6","text":""},{"location":"tutorials/python_basics/#61-dealing-with-json-files-in-python","title":"6.1 Dealing with json files in Python\u00b6","text":"<p>JSON (JavaScript Object Notation) is a lightweight data-interchange format. It is easy for humans to read and write. It is easy for machines to parse and generate.</p> <p>An example of a JSON file:</p> <pre>{\n  \"name\": \"John\",\n  \"age\": 30,\n  \"city\": \"New York\"\n  \"children\": [\n    {\n      \"name\": \"Anna\",\n      \"age\": 5\n    },\n    {\n      \"name\": \"Betty\",\n      \"age\": 7\n    }\n  ]\n}\n</pre>"},{"location":"tutorials/python_basics/#62-dealing-with-csv-files-in-python","title":"6.2 Dealing with csv files in Python\u00b6","text":"<p>CSV (Comma Separated Values) is a simple file format used to store tabular data, such as a spreadsheet or database. A CSV file stores tabular data (numbers and text) in plain text.</p> <p>NOTE THAT PANDAS IS A BETTER OPTION FOR DEALING WITH CSV FILES. WE WILL DISCUSS PANDAS IN THE NEXT SECTION.</p>"},{"location":"tutorials/python_basics/#7-list-comprehensions-advanced","title":"7. List Comprehensions (Advanced)\u00b6","text":"<p>List comprehensions provide a concise way to create lists. Common applications are to make new lists where each element is the result of some operation applied to each member of another sequence or iterable, or to create a subsequence of those elements that satisfy a certain condition.</p>"},{"location":"tutorials/python_basics/#8-lambda-functions-advanced","title":"8. Lambda Functions (Advanced)\u00b6","text":"<p>Lambda functions are small anonymous functions. They can have any number of arguments but only one expression. They are defined using the <code>lambda</code> keyword.</p>"},{"location":"tutorials/python_basics/#9-installing-packages","title":"9. Installing Packages\u00b6","text":"<p>There are many packages available for Python. Most of them are available on the Python Package Index (PyPI). You can install packages using the <code>pip</code> command. For example, to install the <code>numpy</code> package, you can use the command <code>!pip install numpy</code> (on collab).</p> <p>Check out the PyPI website for more information on available packages.</p> <p>Some advanced packages can only be installed via conda. For example, to install the <code>numpy</code> package, you can use the command <code>!conda install &lt;package_name&gt;</code>. Refer to the conda documentation for more information.</p>"},{"location":"tutorials/python_basics/#10-references-to-other-advanced-topics","title":"10. References to other advanced topics\u00b6","text":"<p>Python has a lot of advanced topics that you can learn. For instance it is possible to create classes and objects, use decorators, context managers, and more. These topics are not covered in this course but you can find more information in the Python documentation or in other tutorials.</p> <ul> <li><p>Python Documentation</p> </li> <li><p>Python Tutorial</p> </li> <li><p>Python Library Reference</p> </li> <li><p>YYiki Python</p> </li> <li><p>Real Python</p> </li> </ul>"},{"location":"tutorials/python_basics/#extra-questions","title":"Extra questions\u00b6","text":"<p>For each of the following question, first think about the result without running the code. Then test it by running the code. Reach out if you don't understand why!</p>"},{"location":"tutorials/python_basics/#whats-the-output","title":"What's the output?\u00b6","text":"<pre>def func(a):\n    a = a + 2\n    a = a * 2\n    return a\n\nprint(func(2))\n</pre>"},{"location":"tutorials/python_basics/#true-false-why","title":"True? False? Why?\u00b6","text":"<pre>0.1 + 0.2 == 0.3\n</pre>"},{"location":"tutorials/python_basics/#3-what-is-list_1-and-list_2-and-why","title":"3. What is <code>list_1</code> and <code>list_2</code> and why?\u00b6","text":"<pre>list_1 = [1,2,3]\nlist_2 = list_1\nlist_1.append(4)\nlist_2 += [5]\nlist_2 = list_2 + [10]\n</pre>"},{"location":"tutorials/python_basics/#whats-the-output","title":"What's the output?\u00b6","text":"<pre>l = [i**2 for i in range(10)]\nl[-4:2:-3]\n</pre>"},{"location":"tutorials/python_basics/#what-does-the-code-do-if-the-ordering-doesnt-matter-how-can-it-be-simplified","title":"What does the code do? If the ordering doesn't matter, how can it be simplified?\u00b6","text":"<pre>def func1(lst):\n    a = []\n    for i in lst:\n        if i not in a:\n            a.append(i)\n    return a\n</pre>"},{"location":"tutorials/python_basics/#what-would-be-the-output","title":"What would be the output?\u00b6","text":"<pre>val = [0, 10, 15, 20]\ndata = 15\ntry:\n    data = data/val[0]\nexcept ZeroDivisionError:\n    print(\"zero division error - 1\")\nexcept:\n    print(\"zero division error - 2\")\nfinally:\n    print(\"zero division error - 3\")\n\nval = [0, 10, 15, 20]\ndata = 15\n\ntry:\n    data = data/val[4]\nexcept ZeroDivisionError:\n    print(\"zero division error - 1\")\nexcept:\n    print(\"zero division error - 2\")\nfinally:\n    print(\"zero division error - 3\")\n</pre>"},{"location":"tutorials/python_basics/#what-does-the-code-do","title":"What does the code do?\u00b6","text":"<pre>def func(s):\n    d = {}\n    for c in s:\n        if c in d:\n            d[n] += 1\n        else:\n            d[n] = 1\n    return d\n</pre> <p>(Btw, the same operation can be done by simply running <code>Counter(s)</code> by using <code>Counter</code> data structure in the <code>collections</code> module.)</p>"},{"location":"tutorials/python_basics/#whats-the-output","title":"What's the output?\u00b6","text":"<pre>def func(l):\n    l.append(10)\n    return l\n\na = [1,2,3]\nb = func(a)\na == b\n</pre>"},{"location":"tutorials/python_basics/#whats-happening-to-a-in-each-step-why","title":"What's happening to <code>a</code> in each step? Why?\u00b6","text":"<pre># step 1\na = [ [ ] ] * 5\n# step 2\na[0].append(10)\n# step 3\na[1].append(20)\n# step 4\na.append(30)\n</pre>"},{"location":"tutorials/python_basics/#whats-the-output","title":"What's the output?\u00b6","text":"<pre>L = list('abcdefghijk')\nL[1] = L[4] = 'x'\nL[3] = L[-3]\nprint(L)\n</pre>"},{"location":"tutorials/python_basics/#whats-the-output","title":"What's the output?\u00b6","text":"<pre>y = 8\nz = lambda x : x * y\nprint(z(6))\n</pre>"},{"location":"tutorials/python_basics/#whats-the-output","title":"What's the output?\u00b6","text":"<pre>count = 1\n\ndef func(count):\n    for i in (1, 2, 3):\n        count += 1\nfunc(count = 10)\ncount += 5\nprint(count)\n</pre>"},{"location":"tutorials/terminal_basics/","title":"Terminal Basics","text":"<p>Below is a concise, beginner-friendly guide to using the command line (often called the \u201cterminal\u201d in macOS and Linux). The guide covers the basics for Windows, macOS, and Linux side-by-side. Mastering the terminal is vital for our Data Viz course as it empowers you to efficiently manage files, automate workflows, and interact with various AI tools and scripts.</p>"},{"location":"tutorials/terminal_basics/#1-opening-the-command-line","title":"1. Opening the Command Line","text":""},{"location":"tutorials/terminal_basics/#windows","title":"Windows","text":"<ul> <li>Command Prompt: Press <code>Win + R</code>, type <code>cmd</code>, and hit Enter.  </li> <li>PowerShell: Press <code>Win + X</code> and select \u201cWindows PowerShell\u201d (or just search for PowerShell in the Start menu).</li> </ul>"},{"location":"tutorials/terminal_basics/#macos","title":"macOS","text":"<ul> <li>Terminal: Open \u201cApplications\u201d \u2192 \u201cUtilities\u201d \u2192 \u201cTerminal\u201d, or use Spotlight search (<code>Cmd + Space</code>) and type \u201cTerminal\u201d.</li> </ul>"},{"location":"tutorials/terminal_basics/#linux","title":"Linux","text":"<ul> <li>Terminal: Typically found in the application menu under \u201cAccessories\u201d or \u201cSystem Tools\u201d. You can also use a keyboard shortcut like <code>Ctrl + Alt + T</code> on many distributions.</li> </ul>"},{"location":"tutorials/terminal_basics/#2-basic-navigation-commands","title":"2. Basic Navigation Commands","text":"Action Windows macOS / Linux List files/folders <code>dir</code> <code>ls</code> Change directory <code>cd folder_name</code> <code>cd folder_name</code> Go up one directory <code>cd ..</code> <code>cd ..</code> Clear screen <code>cls</code> <code>clear</code>"},{"location":"tutorials/terminal_basics/#examples","title":"Examples","text":"<ul> <li>List Files:  <ul> <li>Windows: <code>dir</code> </li> <li>macOS/Linux: <code>ls</code> </li> </ul> </li> <li>Change Directory (to <code>Documents</code> folder): <pre><code>cd Documents\n</code></pre></li> <li>Go Up One Level: <pre><code>cd ..\n</code></pre></li> </ul>"},{"location":"tutorials/terminal_basics/#3-creating-and-managing-foldersfiles","title":"3. Creating and Managing Folders/Files","text":"Action Windows (Command Prompt/PowerShell) macOS / Linux Create a folder <code>mkdir folder_name</code> <code>mkdir folder_name</code> Remove an empty folder <code>rmdir folder_name</code> <code>rmdir folder_name</code> Remove a folder + contents <code>rmdir /s folder_name</code> (CMD)  <code>Remove-Item folder_name -Recurse -Force</code> (PowerShell) <code>rm -r folder_name</code>"},{"location":"tutorials/terminal_basics/#examples_1","title":"Examples","text":"<ul> <li>Create Folder: <pre><code>mkdir MyProject\n</code></pre></li> <li>Remove Folder:  <ul> <li>Windows Command Prompt:   <pre><code>rmdir /s MyProject\n</code></pre></li> <li>PowerShell:   <pre><code>Remove-Item MyProject -Recurse -Force\n</code></pre></li> <li>macOS/Linux:   <pre><code>rm -r MyProject\n</code></pre></li> </ul> </li> </ul>"},{"location":"tutorials/terminal_basics/#4-viewing-and-editing-files","title":"4. Viewing and Editing Files","text":"Action Windows (Command Prompt/PowerShell) macOS / Linux Show file contents <code>type filename.txt</code> <code>cat filename.txt</code> Open file in text editor Use Notepad or similar:  <code>notepad filename.txt</code> Use Nano/Vim/gedit etc.:  <code>nano filename.txt</code>"},{"location":"tutorials/terminal_basics/#examples_2","title":"Examples","text":"<ul> <li>Display Contents:<ul> <li>Windows: <code>type README.txt</code></li> <li>macOS/Linux: <code>cat README.txt</code></li> </ul> </li> <li>Edit File:<ul> <li>Windows: <code>notepad README.txt</code></li> <li>macOS/Linux: <code>nano README.txt</code> (or <code>vim</code>, <code>gedit</code>, etc.)</li> </ul> </li> </ul>"},{"location":"tutorials/terminal_basics/#5-copying-moving-and-deleting-files","title":"5. Copying, Moving, and Deleting Files","text":"Action Windows macOS / Linux Copy a file <code>copy old.txt new.txt</code> <code>cp old.txt new.txt</code> Move/rename a file <code>move old.txt new.txt</code> <code>mv old.txt new.txt</code> Delete a file <code>del filename.txt</code> <code>rm filename.txt</code>"},{"location":"tutorials/terminal_basics/#examples_3","title":"Examples","text":"<ul> <li>Copy a File:  <ul> <li>Windows: <code>copy report.txt backup_report.txt</code></li> <li>macOS/Linux: <code>cp report.txt backup_report.txt</code></li> </ul> </li> <li>Move/Rename a File:  <ul> <li>Windows: <code>move data.csv archived_data.csv</code></li> <li>macOS/Linux: <code>mv data.csv archived_data.csv</code></li> </ul> </li> <li>Delete a File:  <ul> <li>Windows: <code>del old_data.csv</code></li> <li>macOS/Linux: <code>rm old_data.csv</code></li> </ul> </li> </ul>"},{"location":"tutorials/terminal_basics/#6-piping-and-redirection","title":"6. Piping and Redirection","text":"<p>Often, you\u2019ll want to chain commands together or redirect output to a file.</p> Action Windows (Command Prompt/PowerShell) macOS / Linux Redirect output to a file <code>command &gt; output.txt</code> <code>command &gt; output.txt</code> Append output to a file <code>command &gt;&gt; output.txt</code> <code>command &gt;&gt; output.txt</code> Pipe output (send to another command) <code>command1 | command2</code> <code>command1 \\| command2</code>"},{"location":"tutorials/terminal_basics/#examples_4","title":"Examples","text":"<ul> <li> <p>Redirect output: <pre><code>dir &gt; files_list.txt    # Windows\nls &gt; files_list.txt     # macOS/Linux\n</code></pre>     This saves the output to <code>files_list.txt</code> instead of displaying it on the screen.</p> </li> <li> <p>Pipe output:  </p> <ul> <li>Windows: <code>dir | findstr \".py\"</code> </li> <li>macOS/Linux: <code>ls | grep \".py\"</code> This finds all Python files in the current directory.</li> </ul> </li> </ul>"},{"location":"tutorials/terminal_basics/#7-additional-tips","title":"7. Additional Tips","text":"<ul> <li>Up/Down Arrow Keys: Cycle through your command history to reuse or edit previous commands.  </li> <li>Tab Completion: Start typing a folder or file name and press <code>Tab</code> to auto-complete it (works in most shells).  </li> <li>Help: Use <code>--help</code> or <code>-h</code> after commands to see help information. Windows commands often use <code>/?</code> instead.</li> <li>Case Sensitivity: Windows commands can be case-insensitive, while macOS/Linux commands are case-sensitive.</li> <li>Historical Commands: Use <code>history</code> (Linux or macOS) or <code>doskey /history</code> (Windows) to view past commands.</li> <li>Permissions: Sometimes you may find permissions issues when running commands. In such cases, you may need to run the command with elevated permissions (e.g., <code>sudo</code> on Linux/macOS or as an administrator on Windows). This should be done with caution, especially when deleting files or changing system settings. You may also need to change the permissions of a file or folder; refer to the documentation of your operating system for more information.<ul> <li>Additional Command: Use <code>chmod</code> (Linux/macOS) to change file permissions as needed.</li> <li>File Ownership Command: Use <code>chown</code> (Linux/macOS) to change file ownership if necessary.</li> </ul> </li> </ul>"},{"location":"tutorials/terminal_basics/#8-more-resources","title":"8. More Resources","text":"<ul> <li>Windows Command Prompt Documentation</li> <li>macOS Terminal Documentation</li> <li>Linux Command Line Basics</li> <li>Introduction to permissions in Linux</li> </ul>"},{"location":"tutorials/testing_python/","title":"Testing your ability with Python (Optional Homework)","text":"In\u00a0[\u00a0]: Copied! <pre># TODO - Enter Library Name\nimport _\n</pre> # TODO - Enter Library Name import _ <p>Next, we'll need to put the text files in a known location. The cell below prints out the current path, place the text files in this directory to ensure they are found by your script.</p> In\u00a0[\u00a0]: Copied! <pre># Getting the current path and displaying it\ncurrent_path = os.getcwd()\nprint(current_path)\n</pre> # Getting the current path and displaying it current_path = os.getcwd() print(current_path) <p>You can compose paths using the <code>os.path.join()</code> function. This function takes two arguments, the first being the directory path and the second being the file name. This function will return a string with the path to the file.</p> <p>For example, suppose you have a directory named <code>data</code> and a file named <code>story-1.txt</code>. You can compose the path to the file using the following code:</p> In\u00a0[\u00a0]: Copied! <pre># Composing the path to the file\na_path = os.path.join('data', 'story-1.txt')\nprint(a_path)\n</pre> # Composing the path to the file a_path = os.path.join('data', 'story-1.txt') print(a_path) <p>Now let's create the directory path to the <code>Datasets</code> folder. You can use <code>\"..\"</code> to refer to the parent directory. For example to compose a path from the parent of the current directory</p> <pre>path_to_datasets = os.path.join(\"..\", \"Datasets\")\n</pre> <p>You can use multiple path components to compose a new path. For example to compose a path from the parent of the current directory to the <code>Datasets</code> folder you can use:</p> <pre>path_to_datasets = os.path.join(a_path, \"..\", \"..\",\"a_file.txt\")\n</pre> <p>In the cell below, complete the code to create the path to the <code>Datasets</code> folder. Remember you already have the current path from previous cells.</p> In\u00a0[\u00a0]: Copied! <pre># Defining the datasets directory path\n# TODO - Enter the path to the datasets\ndataset_path = os.path.join(_)\nprint(dataset_path)\n</pre> # Defining the datasets directory path # TODO - Enter the path to the datasets dataset_path = os.path.join(_) print(dataset_path) In\u00a0[\u00a0]: Copied! <pre># TODO - Fill in the stories filename\nfile_path = os.path.join(dataset_path,_)\n# Opening the file\n# TODO - Fill the way to open the file\nwith open(file_path, _) as fp:\n    # TODO - Fill in the variable that represents the file we are working with\n    content = _.read()\n    # now you should have the content of the file in the variable content\nprint(\"Content of the file:\",content)\n</pre> # TODO - Fill in the stories filename file_path = os.path.join(dataset_path,_) # Opening the file # TODO - Fill the way to open the file with open(file_path, _) as fp:     # TODO - Fill in the variable that represents the file we are working with     content = _.read()     # now you should have the content of the file in the variable content print(\"Content of the file:\",content) <p>You may also find some code with try and catch. In our <code>except</code> block, we check if any issues occur, and if they do, we print them to the screen. This is used as a way to catch any errors that may occur during the reading of the file.</p> <p>For instance:</p> <pre>try:\n    with open(\"a_file.txt\", \"rt\") as fp:\n        text = fp.read()\n        print(text) \nexcept Exception as e:\n    print(\"Error reading file:\", e)\n</pre> <p>Add a <code>try</code> and <code>except</code> block to the code that you complete in the previous cell to catch any errors that may occur during the reading of the file.</p> In\u00a0[\u00a0]: Copied! <pre># TODO - Add a try except block to catch the exception\ntry:\n    # TODO - Fill in the file name\n    # Add your previous cell code here\n    ...\nexcept Exception as e:\n    print(\"Error reading file:\", e)\n</pre> # TODO - Add a try except block to catch the exception try:     # TODO - Fill in the file name     # Add your previous cell code here     ... except Exception as e:     print(\"Error reading file:\", e) In\u00a0[\u00a0]: Copied! <pre># TODO - Convert content to lowercase and split it into words\nlower_content = content.lower()\nwords = ... \n\n# The response should be a python list of words like: \"This is a test\" -&gt; ['this', 'is', 'a', 'test']\n</pre> # TODO - Convert content to lowercase and split it into words lower_content = content.lower() words = ...   # The response should be a python list of words like: \"This is a test\" -&gt; ['this', 'is', 'a', 'test'] <p>Now create a dictionary and start counting the words by using a for loop. If the word is not in the dictionary, add it with a count of 1. If the word is already in the dictionary, increment the count by 1.</p> In\u00a0[\u00a0]: Copied! <pre># Now create a dictionary and start counting the words by using a for loop. If the word is not in the dictionary, add it with a count of 1. If the word is already in the dictionary, increment the count by 1.\nword_count = {}\nfor word in words:\n    if word in word_count:\n        # TODO - Increment the count\n        ...\n    else:\n        # TODO - Add the word to the dictionary with a count of 1\n        ...\n\nprint(word_count)\n</pre> # Now create a dictionary and start counting the words by using a for loop. If the word is not in the dictionary, add it with a count of 1. If the word is already in the dictionary, increment the count by 1. word_count = {} for word in words:     if word in word_count:         # TODO - Increment the count         ...     else:         # TODO - Add the word to the dictionary with a count of 1         ...  print(word_count)   <p>Create a function <code>count_words_in_file(file_path)</code> that reads a file and returns a dictionary with the word count. The function should take the file path as an argument.</p> <p>Use your previous codes as a base to create this function. You can copy and paste the code you wrote before into the function.</p> In\u00a0[\u00a0]: Copied! <pre>def count_words_in_file(file_path):\n    # TODO - Copy the code from the previous cells here, make sure that file_path \n    ...\n    return word_count\n</pre> def count_words_in_file(file_path):     # TODO - Copy the code from the previous cells here, make sure that file_path      ...     return word_count <p>Let's test the function with another story file, like <code>story-2.txt</code>.</p> In\u00a0[\u00a0]: Copied! <pre># You should be able to test your function by running the following code:\nfile_path = os.path.join(dataset_path, \"story-2.txt\")\ncounts_dictionary = count_words_in_file(file_path)\nprint(counts_dictionary)\n</pre> # You should be able to test your function by running the following code: file_path = os.path.join(dataset_path, \"story-2.txt\") counts_dictionary = count_words_in_file(file_path) print(counts_dictionary) <p>We can now create a list of the files names to individually loop through.  Notice stories is a <code>list</code>, as it is assigned to values enclosed by brackets (<code>[]</code>).</p> In\u00a0[\u00a0]: Copied! <pre># List of files names to read in the same directory\n# TODO - Enter the file names inside the list\nstories = [...]\n</pre> # List of files names to read in the same directory # TODO - Enter the file names inside the list stories = [...] <p>Let's now loop through the list of files and call the function <code>count_words_in_file</code> for each file. We will store the result in a list of dictionaries called <code>word_counts</code>.</p> In\u00a0[\u00a0]: Copied! <pre>word_counts = []\n\nfor story in _:\n    # TODO - Fill in the file path\n    file_path = os.path.join(dataset_path, story)\n    # TODO - Call the function count_words_in_file with the file_path\n    word_count = ...\n    # TODO - Append the result to the word_counts list\n    ...\n\nprint(word_counts)\n</pre> word_counts = []  for story in _:     # TODO - Fill in the file path     file_path = os.path.join(dataset_path, story)     # TODO - Call the function count_words_in_file with the file_path     word_count = ...     # TODO - Append the result to the word_counts list     ...  print(word_counts) <p>And there you have it! Submit the completed version of this assignment for points. Save the notebook as a PDF or HTML file and submit it via Canvas. Keep the output of the code cells visible in the exported file.</p>"},{"location":"tutorials/testing_python/#testing-your-ability-with-python-optional-homework","title":"Testing your ability with Python (Optional Homework)\u00b6","text":"<p>In this optional homework, we will be getting more experienced with python through file processing. With this guide, you will be able to:</p> <ul> <li>read from a text (.txt) file</li> <li>process the words of the file</li> <li>output the result of this process</li> </ul>"},{"location":"tutorials/testing_python/#instructions","title":"Instructions\u00b6","text":"<ol> <li>Follow the instructions on how to setup your Python and Jupyter (or VSCode) environment and cloning or downloading our repository. Instructions can be found in this jupyter notebook: https://filipinascimento.github.io/dataviz/tutorials/testing_python</li> <li>Ensure that you have Python and Jupyter Notebook working. (You can also try using Google Colab. This is not the preferred method for this homework, but it is an option)</li> <li>Load the text files: <code>story-1.txt</code>, <code>story-2.txt</code>, <code>story-3.txt</code>, and <code>story-4.txt</code>, located in the <code>Datasets</code> directory. (If you are using Google Colab, you will need to upload the files to the colab environment)</li> <li>Answer the questions below by writing or completing the code in the provided cells.</li> </ol>"},{"location":"tutorials/testing_python/#dataset-overview","title":"Dataset Overview\u00b6","text":"<p>The dataset consists of four text files, each containing a story. The stories are:</p> <ul> <li><code>story-1.txt</code>: The Monkey and the Crocodile</li> <li><code>story-2.txt</code>: The Musical Donkey</li> <li><code>story-3.txt</code>: A Tale of Three Fish</li> <li><code>story-4.txt</code>: The Foolish Lion and the Clever Rabbit</li> </ul>"},{"location":"tutorials/testing_python/#submission-guidelines","title":"Submission Guidelines\u00b6","text":"<ul> <li>Submit your completed notebook as a HTML export, or a PDF file.</li> </ul> <p>To export to HTML, if you are on Jupyter, select <code>File</code> &gt; <code>Export Notebook As</code> &gt; <code>HTML</code>.</p> <p>If you are on VSCode, you can use the <code>Jupyter: Export to HTML</code> command.</p> <ul> <li>Open the command palette (Ctrl+Shift+P or Cmd+Shift+P on Mac).<ul> <li>Search for <code>Jupyter: Export to HTML</code>.</li> <li>Save the HTML file to your computer and submit it via Canvas.</li> </ul> </li> </ul> <p>Using Generative AI Responsibly</p> <p>You're welcome to use Generative AI to assist your learning, but focus on understanding the concepts rather than just solving the assignment. For example, instead of copying and pasting the question into the model, ask it to explain the concept in the question. Try asking: <code>How can I open a file in Python? Can you give me examples?</code> or <code>What functions and methods can I use to extract the words of a text file? Can you explain how they work with some examples?</code></p> <p>This way, you will learn how the solution works while building your skills. Remember to give context to the generative AI, so it can better assist you. Talk to the instructor and AIs if you have any questions or need insights.</p> <p>To begin, we will first need to make sure the functions (set of instructions) we need to use are included in this file. We'll do this through an import statement, followed by the libraries we need. In this case, we will use the <code>os</code> library. In the cell below, type the library name where the <code>_</code> is. In the remainder of this guide, you will be filling in some value where ever a <code>_</code> or <code>...</code> is.</p>"},{"location":"tutorials/testing_python/#opening-and-reading-one-of-the-text-files","title":"Opening and Reading one of the text files\u00b6","text":"<p>Let's open the first file (<code>stories-1.txt</code>) and read it's content.   We will need to call <code>read()</code> on <code>fp</code> to read in values from the file. </p>"},{"location":"tutorials/testing_python/#counting-words","title":"Counting Words\u00b6","text":"<p>Now we can read the file, we want to count the words that we read into the <code>words</code> variable. Take a look at the lesons and/or python documentation to find out how to split the text into words. I also suggest you use the <code>lower()</code> method to convert all words to lowercase. This way, we can count the words without worrying about the case of the words.</p>"},{"location":"tutorials/testing_python/#repeating-the-process-for-all-files","title":"Repeating the process for all files\u00b6","text":"<p>Now that you have the code to read and count the words in a file, you can repeat this process for all the files. You can create a function that reads a file and returns the word count. Then you can call this function for each file.</p>"},{"location":"tutorials/vscode_basics/","title":"VS Code Start Guide","text":""},{"location":"tutorials/vscode_basics/#introduction","title":"Introduction","text":"<p>Visual Studio Code is a practical choice for our Usable AI course. It supports running Python notebooks and provides essential tools for project management through its integrated GitHub features. This approach makes managing code and projects straightforward without unnecessary complexity. You can also customize the editor to suit your preferences and workflow and use it across different operating systems, even remotely.</p>"},{"location":"tutorials/vscode_basics/#1-downloading-vs-code","title":"1. Downloading VS Code","text":"<p>To get started, download the latest version of VS Code from the official website: - Visit https://code.visualstudio.com and select the installer for your operating system. - Follow the installation instructions provided on the site. - To use python, you need to have set up Python on your machine. You can follow the instructions in the Setup Lecture.</p>"},{"location":"tutorials/vscode_basics/#2-signing-in-for-enhanced-features","title":"2. Signing In for Enhanced Features","text":"<p>After installation: - Open VS Code and use the built-in account manager to sign in with your GitHub account. - Signing in unlocks access to repository management directly within the editor, integrates GitHub Copilot for AI-driven code suggestions, and synchronizes your settings across devices. - For further guidance, check the GitHub Authentication Guide.</p>"},{"location":"tutorials/vscode_basics/#3-installing-essential-extensions","title":"3. Installing Essential Extensions","text":"<p>Enhance VS Code functionality by installing key extensions: - Python Extension: Provides powerful language support, debugging, and linting. Learn more at Python in VS Code. - Jupyter Extension: Enables interactive coding, allowing you to create, open, and execute Jupyter Notebooks. For further instructions, visit Using Jupyter in VS Code. - You may additionally explore other extensions related to your projects through the marketplace.</p>"},{"location":"tutorials/vscode_basics/#4-key-features","title":"4. Key Features","text":"<p>VS Code comes with a range of essential features: - Jupyter Notebooks Integration: Work seamlessly with <code>.ipynb</code> files, allowing interactive data exploration and visualization. - Integrated Terminal: Launch a terminal within VS Code to execute commands, run scripts, or manage version control without leaving the editor. - Remote Development: Use extensions like Remote - SSH to connect to remote servers, containers, or WSL, enabling you to develop on powerful machines from a lightweight local editor. - Customizable User Interface: Tailor your workspace with themes, layouts, and keyboard shortcuts to create a personalized development environment.</p>"},{"location":"tutorials/vscode_basics/#5-additional-resources-guides","title":"5. Additional Resources &amp; Guides","text":"<p>Further reading and resources to help you master VS Code: - VS Code Official Documentation - Getting Started with VS Code - Remote Development in VS Code - VS Code Tips and Tricks</p>"},{"location":"tutorials/web_basics/","title":"Web setup","text":"<p>This guide will help you set up a simple web development environment using Vite (a modern build tool) and HTML Canvas (for drawing graphics). We\u2019ll cover the basics of creating a project, writing HTML/CSS/JavaScript, and drawing on the canvas.</p>"},{"location":"tutorials/web_basics/#important-links","title":"Important Links:","text":"<ul> <li>Vite Documentation</li> <li>HTML Canvas API</li> </ul> <p>If you don't know what HTML is, check out these resources: - MDN Web Docs: HTML Basics - W3Schools: HTML Tutorial</p> <p>If you don't know what CSS is, check out these resources: - MDN Web Docs: CSS Basics - W3Schools: CSS Tutorial</p> <p>If you don't know what JavaScript is, check out our review or these resources: - MDN Web Docs: JavaScript Basics - W3Schools: JavaScript Tutorial</p>"},{"location":"tutorials/web_basics/#1-install-nodejs-and-npm","title":"1. Install Node.js and npm","text":"<ol> <li>Go to https://nodejs.org and download the LTS (Long-Term Support) version for your operating system.</li> <li>Run the installer. This will also install npm (Node Package Manager), which comes bundled with Node.js.</li> <li>Verify the installation by opening a terminal (or command prompt) and typing:    <pre><code>node -v\nnpm -v\n</code></pre>    You should see version numbers for both.</li> </ol>"},{"location":"tutorials/web_basics/#2-create-a-simple-vite-project","title":"2. Create a Simple Vite Project","text":""},{"location":"tutorials/web_basics/#step-a-initialize-the-project","title":"Step A: Initialize the Project","text":"<ol> <li>Open a terminal in the folder where you want your project to live.</li> <li>Run:    <pre><code>npm create vite@latest\n</code></pre></li> <li>Enter a name for your project folder (e.g., <code>my-dataviz-project</code>).</li> <li>Choose <code>vanilla</code> when prompted.</li> </ol> <p>(Alternatively, you can specify everything in one go: <code>npm create vite@latest my-dataviz-project -- --template vanilla</code>.)</p> <ol> <li>Navigate into your new project folder:    <pre><code>cd my-dataviz-project\n</code></pre></li> </ol>"},{"location":"tutorials/web_basics/#step-b-install-dependencies","title":"Step B: Install Dependencies","text":"<p>Inside the project folder, run: <pre><code>npm install\n</code></pre> This will download everything needed for your Vite setup.</p>"},{"location":"tutorials/web_basics/#step-c-run-the-development-server","title":"Step C: Run the Development Server","text":"<p><pre><code>npm run dev\n</code></pre> You should see something like: <pre><code>  VITE vX.0.0  ready in X ms\n\n  \u279c  Local:   http://localhost:5173/\n  \u279c  Network: use --host to expose\n</code></pre> Open the provided local URL in your web browser to see a basic starter page.</p>"},{"location":"tutorials/web_basics/#3-project-structure-overview","title":"3. Project Structure Overview","text":"<p>A typical Vite vanilla project looks like this: <pre><code>my-dataviz-project/\n  \u251c\u2500 index.html\n  \u251c\u2500 main.js     # Your main JavaScript entry point\n  \u251c\u2500 style.css   # (optional) separate CSS file\n  \u251c\u2500 package.json\n  \u251c\u2500 vite.config.js\n  \u2514\u2500 ...\n</code></pre></p> <ul> <li>index.html: The main HTML file Vite will serve.  </li> <li>main.js: Your core JavaScript logic (you can rename it if you like).  </li> <li>style.css: A CSS file for styling (or inline CSS in <code>index.html</code>).  </li> <li>package.json: Keeps track of dependencies and scripts.  </li> <li>vite.config.js: Vite\u2019s configuration file (often not needed for simple projects).</li> </ul>"},{"location":"tutorials/web_basics/#4-basic-html-css-setup","title":"4. Basic HTML + CSS Setup","text":"<p>Let\u2019s create or update <code>index.html</code> so it has a container that centers the canvas. Here\u2019s a minimal example:</p> <p><code>index.html</code> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"UTF-8\" /&gt;\n    &lt;title&gt;Basic Vite + JS Project&lt;/title&gt;\n    &lt;!-- Link to our CSS (optional, you can inline) --&gt;\n    &lt;link rel=\"stylesheet\" href=\"/style.css\" /&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div class=\"canvas-container\"&gt;\n      &lt;canvas id=\"myCanvas\" width=\"400\" height=\"300\"&gt;&lt;/canvas&gt;\n    &lt;/div&gt;\n\n    &lt;!-- Main JS entry point --&gt;\n    &lt;script type=\"module\" src=\"/main.js\"&gt;&lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p><code>style.css</code> (in the same directory or a dedicated CSS folder): <pre><code>/* Make the body fill the screen and remove default margins */\nhtml,\nbody {\n  margin: 0;\n  padding: 0;\n  height: 100%;\n  background-color: #f0f0f0;\n  font-family: sans-serif;\n}\n\n/* A container that uses flexbox to center its contents */\n.canvas-container {\n  display: flex;\n  justify-content: center;\n  align-items: center;\n  height: 100vh; /* Full viewport height */\n}\n\n/* Optional: add a simple border around the canvas */\n#myCanvas {\n  border: 2px solid #333;\n}\n</code></pre></p> <p>Now, our canvas will be centered on the screen and have a simple border.</p>"},{"location":"tutorials/web_basics/#5-basic-javascript-fundamentals","title":"5. Basic JavaScript Fundamentals","text":"<p>Below are some essential concepts you\u2019ll use in most JavaScript projects:</p> <ol> <li>Variables: <pre><code>let userName = \"Alice\";\nconst MAX_SCORE = 100;\n</code></pre></li> <li>Functions &amp; Arrow Functions: <pre><code>// Traditional function:\nfunction greet(name) {\n  console.log(`Hello, ${name}!`);\n}\n\n// Arrow function:\nconst greetArrow = (name) =&gt; {\n  console.log(`Hello from arrow func, ${name}!`);\n};\n\ngreet(\"Alice\");\ngreetArrow(\"Bob\");\n</code></pre></li> <li>Event Listeners: <pre><code>document.addEventListener(\"click\", function () {\n  console.log(\"Page clicked!\");\n});\n// Or as an arrow function:\ndocument.addEventListener(\"click\", () =&gt; {\n  console.log(\"Page clicked (arrow)!\");\n});\n</code></pre></li> <li>DOM Manipulation: <pre><code>const myCanvas = document.getElementById(\"myCanvas\");\nconsole.log(myCanvas.width, myCanvas.height);\n</code></pre></li> <li>Conditionals and Loops: <pre><code>if (userName === \"Alice\") {\n  // do something\n}\n\nfor (let i = 0; i &lt; 5; i++) {\n  console.log(i);\n}\n</code></pre></li> </ol>"},{"location":"tutorials/web_basics/#6-drawing-on-the-canvas","title":"6. Drawing on the Canvas","text":"<p>We\u2019ll show a simple example that draws a rectangle and some text on the canvas using <code>main.js</code>.</p> <p><code>main.js</code> <pre><code>// 1. Get the canvas element\nconst canvas = document.getElementById(\"myCanvas\");\n\n// 2. Get the 2D context\nconst ctx = canvas.getContext(\"2d\");\n\n// 3. Fill the background\nctx.fillStyle = \"#fff\";\nctx.fillRect(0, 0, canvas.width, canvas.height);\n\n// 4. Draw a rectangle\nctx.fillStyle = \"#ff0000\"; // red\nctx.fillRect(50, 50, 100, 60);\n\n// 5. Draw some text\nctx.fillStyle = \"#000\";\nctx.font = \"20px Arial\";\nctx.fillText(\"Hello Canvas!\", 50, 40);\n\n// Example: arrow function to draw a circle\nconst drawCircle = (x, y, radius, color) =&gt; {\n  ctx.fillStyle = color;\n  ctx.beginPath();\n  ctx.arc(x, y, radius, 0, Math.PI * 2);\n  ctx.fill();\n};\n\n// Use our arrow function\ndrawCircle(200, 150, 30, \"#00ff00\");\n</code></pre></p>"},{"location":"tutorials/web_basics/#explanation","title":"Explanation","text":"<ul> <li><code>ctx.fillStyle</code> sets the fill color or pattern.  </li> <li><code>ctx.fillRect(x, y, width, height)</code> draws a filled rectangle at <code>(x,y)</code> with the given dimensions.  </li> <li><code>ctx.font</code> and <code>ctx.fillText(text, x, y)</code> let you draw text on the canvas.  </li> <li><code>ctx.beginPath()</code> starts a new path for shapes like circles, and <code>ctx.arc()</code> draws a circular arc.  </li> <li>We wrapped the circle logic in an arrow function (<code>drawCircle</code>) to demonstrate how you might package drawing logic in reusable functions.</li> </ul>"},{"location":"tutorials/web_basics/#7-running-the-application","title":"7. Running the Application","text":"<ol> <li>Ensure you\u2019ve saved all files: <code>index.html</code>, <code>style.css</code>, and <code>main.js</code>.  </li> <li>In your project folder, run:    <pre><code>npm run dev\n</code></pre></li> <li>Open your browser at the URL shown (e.g., <code>http://localhost:5173</code>).</li> <li>You should see a centered canvas with a red rectangle, the text \u201cHello Canvas!\u201d, and a green circle.</li> </ol>"},{"location":"tutorials/web_basics/#8-summary-of-the-steps","title":"8. Summary of the Steps","text":"<ol> <li>Install Node.js (which includes npm).  </li> <li>Initialize a new Vite project via <code>npm create vite@latest</code>.  </li> <li>Install dependencies with <code>npm install</code>.  </li> <li>Run the development server using <code>npm run dev</code>.  </li> <li>Set up a minimal HTML and CSS to display a centered canvas.  </li> <li>Use JavaScript in <code>main.js</code> to draw on the canvas.</li> </ol> <p>Enjoy coding with JavaScript and creating interactive visuals using the HTML5 Canvas!</p>"},{"location":"w02-recap_setup/assignment_w2_sol/","title":"Assignment 2: Exploring Solar System Bodies","text":"In\u00a0[\u00a0]: Copied! <pre># if you are running this notebook in your local machine,\n# make sure you have all the dependencies installed\n# uncomment the following lines to install the dependencies\n# This may be needed if you are running this notebook in online\n# environments such as Google Colab\n#\n# !pip install numpy pandas\n#\n# also copy the data file to the same directory as this notebook\n# and update the paths accordingly\n</pre> # if you are running this notebook in your local machine, # make sure you have all the dependencies installed # uncomment the following lines to install the dependencies # This may be needed if you are running this notebook in online # environments such as Google Colab # # !pip install numpy pandas # # also copy the data file to the same directory as this notebook # and update the paths accordingly In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\n\n# Load the dataset\ndata = pd.read_json('../../Datasets/sol_data.json')\n# The ../../ are needed to go back two levels in the directory structure.\n# Note that the path is relative to the location of the notebook file.\ndata.head()\n</pre> import pandas as pd import numpy as np  # Load the dataset data = pd.read_json('../../Datasets/sol_data.json') # The ../../ are needed to go back two levels in the directory structure. # Note that the path is relative to the location of the notebook file. data.head() In\u00a0[\u00a0]: Copied! <pre># Total number of objects\n# Fill in code to calculate total number of objects\n\n# Number of planets\n# Fill in code to calculate number of planets\n\n# Number of moons\n# Fill in code to calculate number of moons\n</pre> # Total number of objects # Fill in code to calculate total number of objects  # Number of planets # Fill in code to calculate number of planets  # Number of moons # Fill in code to calculate number of moons <p>Hint: By moon we mean a natural satellite of a planet or another object in the solar system. Take a look at the columns and see if you can identify the criteria for classifying an object as a moon. Ask the instructor or AIs for help if needed.</p> In\u00a0[\u00a0]: Copied! <pre># Mean density of all planets\n# Fill in code\n\n# Planet with the highest surface gravity\n# Fill in code\n\n# Planets by descending mass\n# Fill in code\n</pre>  # Mean density of all planets # Fill in code  # Planet with the highest surface gravity # Fill in code  # Planets by descending mass # Fill in code  In\u00a0[\u00a0]: Copied! <pre># Number of moons orbiting each planet\n# Fill in code\n\n# Average radius of all moons\n# Fill in code\n\n# Compare average surface gravity of moons vs. planets\n# Fill in code\n</pre> # Number of moons orbiting each planet # Fill in code  # Average radius of all moons # Fill in code  # Compare average surface gravity of moons vs. planets # Fill in code  In\u00a0[\u00a0]: Copied! <pre># Highest orbital eccentricity\n# Fill in code\n\n# Average semi-major axis of planets vs. moons\n# Fill in code\n\n# Moon with the shortest orbital period\n# Fill in code\n</pre> # Highest orbital eccentricity # Fill in code  # Average semi-major axis of planets vs. moons # Fill in code  # Moon with the shortest orbital period # Fill in code In\u00a0[\u00a0]: Copied! <pre># Objects with discovery dates\n# Fill in code\n\n# Oldest discovered moon\n# Fill in code\n</pre> # Objects with discovery dates # Fill in code  # Oldest discovered moon # Fill in code In\u00a0[\u00a0]: Copied! <pre># Average density of moons orbiting planets with mass &gt; Earth\n# Fill in code\n\n# Average orbital eccentricity by orbit_type\n# Fill in code\n\n# Top 3 moons with highest escape velocity\n# Fill in code\n</pre> # Average density of moons orbiting planets with mass &gt; Earth # Fill in code  # Average orbital eccentricity by orbit_type # Fill in code  # Top 3 moons with highest escape velocity # Fill in code In\u00a0[\u00a0]: Copied! <pre># Moons with a mass less than Earth's moon and percentage\n# Fill in code\n\n# Ratio of moons to planets and planet with highest moon to mass ratio\n# Fill in code\n\n# Average density of moons per planet\n# Fill in code\n</pre> # Moons with a mass less than Earth's moon and percentage # Fill in code  # Ratio of moons to planets and planet with highest moon to mass ratio # Fill in code  # Average density of moons per planet # Fill in code"},{"location":"w02-recap_setup/assignment_w2_sol/#assignment-2-exploring-solar-system-bodies","title":"Assignment 2: Exploring Solar System Bodies\u00b6","text":"<p>Welcome to Assignment 2!</p> <p>In this assignment, we will analyze data about celestial bodies in the solar system using Python, NumPy, and Pandas. The goals of this assignment are to:</p> <ul> <li>Handle JSON datasets effectively.</li> <li>Apply statistical analysis to real-world data.</li> <li>Refine Python programming skills through hands-on practice.</li> <li>Ensure you can run Python and Python notebook environments (e.g., Jupyter Notebook, JupyterLab, Collab, VSCode) and troubleshoot any setup issues.</li> </ul> <p>A key part of this assignment is verifying that you can successfully run Python notebooks. If you encounter any difficulties, seek help from the instructor or AIs. Additionally, use the <code>#q_and_a</code> Slack channel to ask questions or share insights. If you see a classmate struggling, helping them out will be great for a collaborative learning environment (and may count extra points in engagement \ud83d\ude00).</p>"},{"location":"w02-recap_setup/assignment_w2_sol/#instructions","title":"Instructions\u00b6","text":"<ol> <li>Follow the instructions on how to setup your Python and Jupyter (or VSCode) environment and cloning or downloading our repository. Instructions can be found in the class notes: https://filipinascimento.github.io/dataviz/w02-recap_setup/class/</li> <li>Ensure that you have Python, Jupyter Notebook, and the necessary libraries installed (<code>NumPy</code> and <code>Pandas</code>).</li> <li>Load the dataset <code>Datasets/sol_data.json</code> into a Pandas DataFrame.</li> <li>Answer the questions below by writing Python code.</li> <li>No plots or visualizations are required\u2014your insights should come from code-based analysis and outputs.</li> </ol>"},{"location":"w02-recap_setup/assignment_w2_sol/#dataset-overview","title":"Dataset Overview\u00b6","text":"<p>The dataset contains information about celestial objects, including:</p> <ul> <li>isPlanet: Indicates whether the object is a planet (<code>True</code> or <code>False</code>).</li> <li>orbit_type: Classifies the object as \"Primary\" (planets) or \"Secondary\" (moons).</li> <li>Physical and orbital properties, such as mass, density, meanRadius, gravity, sideralOrbit, and more.</li> </ul>"},{"location":"w02-recap_setup/assignment_w2_sol/#submission-guidelines","title":"Submission Guidelines\u00b6","text":"<ul> <li>Submit your completed notebook as a HTML export, or a PDF file.</li> </ul> <p>To export to HTML, if you are on Jupyter, select <code>File</code> &gt; <code>Export Notebook As</code> &gt; <code>HTML</code>.</p> <p>If you are on VSCode, you can use the <code>Jupyter: Export to HTML</code> command.</p> <ul> <li>Open the command palette (Ctrl+Shift+P or Cmd+Shift+P on Mac).<ul> <li>Search for <code>Jupyter: Export to HTML</code>.</li> <li>Save the HTML file to your computer and submit it via Canvas.</li> </ul> </li> </ul> <p>Hint: If you are learning pandas, check out our tutorials or the official documentation:</p> <ul> <li>Pandas Getting started</li> <li>Pandas DataFrame API Documentation</li> <li>Our tutorials on Pandas</li> </ul> <p>Using Generative AI Responsibly</p> <p>You're welcome to use Generative AI to assist your learning, but focus on understanding the concepts rather than just solving the assignment. For example:</p> <ul> <li>Instead of asking: <code>What's the code to count moons orbiting each planet?</code></li> <li>Try asking: <code>How can I use Pandas to group and count values? Can you provide examples? Can you explain the steps?</code></li> </ul> <p>This way, you will learn how the solution works while building your skills. Remember to give context to the generative AI, so it can better assist you. Talk to the instructor and AIs if you have any questions or need insights.</p>"},{"location":"w02-recap_setup/assignment_w2_sol/#1-general-information","title":"1. General Information\u00b6","text":"<ul> <li>How many objects are in the dataset?</li> <li>How many are planets? How many are moons?</li> </ul>"},{"location":"w02-recap_setup/assignment_w2_sol/#2-planets","title":"2. Planets\u00b6","text":"<ul> <li>What is the mean density of all planets?</li> <li>Which planet has the highest surface gravity, and what is its gravity value?</li> <li>List all planets in descending order of their mass.</li> </ul>"},{"location":"w02-recap_setup/assignment_w2_sol/#3-moons","title":"3. Moons\u00b6","text":"<ul> <li>How many moons orbit each planet? Present this as a table or dictionary.</li> <li>What is the average radius (meanRadius) of all moons?</li> <li>Compare the average surface gravity of moons to that of planets.</li> </ul>"},{"location":"w02-recap_setup/assignment_w2_sol/#4-orbital-properties","title":"4. Orbital Properties\u00b6","text":"<ul> <li>Which object has the highest orbital eccentricity, and what is its value?</li> <li>Calculate the average semi-major axis (semimajorAxis) for planets and compare it to that of moons.</li> <li>Identify the moon with the shortest orbital period (sideralOrbit) and the planet it orbits.</li> </ul>"},{"location":"w02-recap_setup/assignment_w2_sol/#5-discovery-dates","title":"5. Discovery Dates\u00b6","text":"<ul> <li>How many objects have recorded discovery dates?</li> <li>Which is the oldest discovered moon, and when was it discovered?</li> </ul>"},{"location":"w02-recap_setup/assignment_w2_sol/#6-advanced-analysis","title":"6. Advanced Analysis\u00b6","text":"<ul> <li>Calculate the average density of moons that orbit planets with a mass greater than Earth's mass (<code>5.97e24 kg</code>).</li> <li>Group all objects by their <code>orbit_type</code> and compute the average orbital eccentricity for each group.</li> <li>Identify the top 3 moons with the highest escape velocity (escape).</li> </ul>"},{"location":"w02-recap_setup/assignment_w2_sol/#7-extra-questions","title":"7. Extra questions\u00b6","text":"<ol> <li>How many moons have a mass less than 10% of Earth's moon? What percentage of all moons does this represent?</li> <li>Calculate the ratio of moons to planets in the dataset. Which planet has the highest number of moons relative to its mass?</li> <li>Group moons by their host planet and calculate the average density for each group. Which planet hosts moons with the highest average density?</li> </ol>"},{"location":"w02-recap_setup/class/","title":"Lecture 2: Recap and Setup","text":"<p>In this lecture, we will recap the key technologies needed for the Data Visualization course and set up the development environment.</p> <p>We will cover the setup of two essential tools: Python and javascript (Web Development). We will also discuss the basic usage of GitHub.</p> <p>If you are new to these tools, follow the instructions below to set up your environment.</p> <p>If you are unfamiliar with Python, the terminal, web development, or Git, or if you would like to refresh your knowledge, we recommend visiting the tutorials section of this course.</p> <p>For this course we will also be using numpy and pandas extensively, so make sure you are familiar with these libraries too.</p>"},{"location":"w02-recap_setup/class/#tutorials","title":"Tutorials:","text":"<ul> <li>Terminal Basics</li> <li>Git Basics</li> <li>Python Basics</li> <li>NumPy Basics</li> <li>Pandas Basics</li> <li>Web Development Basics</li> </ul>"},{"location":"w02-recap_setup/class/#cloning-the-repository","title":"Cloning the repository","text":"<p>Github is a platform where you can store your code and collaborate with others. Git is a version control system that allows you to track changes in your code. If something goes wrong, you can always revert to a previous version.</p> <p>To get started, you need to clone the repository to your local machine. This will create a copy of the repository on your computer.</p> <ol> <li>Installing GIT:</li> <li>Windows: Download Git for Windows and install. You can use Git Bash or Command Prompt after installation.</li> <li>macOS: Git often comes pre-installed. If not, install Xcode Command Line Tools (<code>xcode-select --install</code>) or get the official installer.</li> <li> <p>Linux: Install via your package manager, e.g. <code>sudo apt-get install git</code> (Debian/Ubuntu) or <code>sudo dnf install git</code> (Fedora).</p> </li> <li> <p>Configure Git (All Platforms): After installation, open a terminal and set your name and email for commits: <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n</code></pre> You can check your configuration with: <pre><code>git config --list\n</code></pre></p> </li> <li> <p>Navigate to the Repository:</p> </li> <li> <p>Go to the github.com/filipinascimento/dataviz repository on GitHub.</p> </li> <li> <p>Clone the Repository:</p> </li> <li>Click on the \"Code\" button at the top right corner of the page.</li> <li>Copy the URL under \"Clone with HTTPS\".</li> <li>Open a terminal on your computer and run <code>git clone [URL]</code> (replace <code>[URL]</code> with the copied URL).</li> </ol> <p>Note: You can also use the download ZIP option if you don't want to use Git. Note: If you are using SSH, you can use the SSH URL instead of HTTPS.</p> <ol> <li>Navigate to the Repository:</li> <li>Go to the cloned repository on your local machine.</li> </ol> <p>The files in the repository are now available on your local machine. You now have access to the course materials in your local environment.</p>"},{"location":"w02-recap_setup/class/#updating-the-repository","title":"Updating the Repository","text":"<p>In case there are updates to the repository, you can pull the changes to your local machine. This will ensure that you have the latest version of the course materials.</p> <ol> <li>Navigate to the Repository:</li> <li> <p>Open a terminal and navigate to the cloned repository on your local machine.</p> </li> <li> <p>Pull the Changes:</p> </li> <li>Run <code>git pull origin main</code> to pull the changes from the main branch of the repository.</li> </ol> <p>The changes from the main branch will be merged into your local repository. You now have the latest version of the course materials on your local machine.</p>"},{"location":"w02-recap_setup/class/#setting-up-the-python-environment","title":"Setting up the Python Environment","text":"<p>We suggest using Miniforge or Miniconda to install python packages and setup your environment. You can also use Anaconda, but it is a larger package and may take longer to install. Alternatively, you can also setup your own python environment using pip and virtualenv (this approach will not be covered in this document).</p>"},{"location":"w02-recap_setup/class/#step-1-install-miniforge","title":"Step 1: Install Miniforge","text":"<p>Miniforge is a minimal installer for Conda, a package manager and an environment manager. Here's how to install it:</p> <ol> <li>Download MiniForge:</li> <li>Visit the Miniconda download page.</li> <li>Choose the version suitable for your operating system (Windows, macOS, or Linux).</li> <li> <p>Download the appropriate installer.</p> </li> <li> <p>Install Miniconda:</p> </li> <li>Windows: Run the downloaded <code>.exe</code> file and follow the on-screen instructions.</li> <li> <p>macOS/Linux: Open a terminal, navigate to the folder containing the downloaded file, and run <code>bash Miniforge3-MacOSX-arm64.sh</code> (adjust the filename as needed).</p> </li> <li> <p>Verify the Installation:</p> </li> <li>Open a new terminal window.<ul> <li>On Windows, you can look for the Miniforge Prompt.</li> </ul> </li> <li>Type <code>conda list</code>. If Miniconda is installed correctly, you'll see a list of installed packages.</li> </ol>"},{"location":"w02-recap_setup/class/#step-2-create-a-conda-environment","title":"Step 2: Create a Conda Environment","text":"<p>Creating a separate environment for your Data Science projects is good practice:</p> <ol> <li> <p>Create a New Environment for this course:</p> <ul> <li>Run the command: <code>conda env create -f environment.yml</code>. This will create a new environment called datascience with all the necessary packages installed.</li> </ul> </li> <li> <p>Activate the Environment:</p> </li> <li> <p>Run: <code>conda activate dataviz</code>.</p> </li> <li> <p>Launch Jupyter Lab:</p> </li> <li>Run: <code>jupyter lab</code>.</li> <li>This will open Jupyter Lab in your default web browser.</li> </ol>"},{"location":"w02-recap_setup/class/#step-4-verify-installation","title":"Step 4: Verify Installation","text":"<p>Make sure everything is installed correctly:</p> <ol> <li>Open a New Notebook in Jupyter Lab:</li> <li> <p>In Jupyter Lab, create a new notebook.</p> </li> <li> <p>Test the Packages:</p> </li> <li>Try importing the packages: <code>import numpy as np</code>, <code>import pandas as pd</code>, <code>import matplotlib.pyplot as plt</code>.</li> <li>If there are no errors, the packages are installed correctly.</li> </ol>"},{"location":"w02-recap_setup/class/#additional-tips","title":"Additional Tips","text":"<ul> <li>Updating Conda: Keep Conda and your packages updated with <code>conda update conda</code> and <code>conda update --all</code>.</li> <li>Managing Environments: View your environments with <code>conda env list</code> and switch between them using <code>conda activate &lt;env_name&gt;</code>.</li> <li>Finding Packages: To find available packages, use <code>conda search &lt;package_name&gt;</code>.</li> <li>Conda Cheat Sheet: For more commands, see the Conda Cheat Sheet.</li> </ul>"},{"location":"w02-recap_setup/class/#setup-web-development-environment","title":"Setup Web Development Environment","text":"<p>For this course, we will be using JavaScript and web development tools to create interactive visualizations. For now we will onlt try a few experiments with Canvas so you can get a feel of how it works and revisit 2D graphics.</p>"},{"location":"w02-recap_setup/class/#1-install-nodejs-and-npm","title":"1. Install Node.js and npm","text":"<ol> <li>Go to https://nodejs.org and download the LTS (Long-Term Support) version for your operating system.</li> <li>Run the installer. This will also install npm (Node Package Manager), which comes bundled with Node.js.</li> <li>Verify the installation by opening a terminal (or command prompt) and typing:    <pre><code>node -v\nnpm -v\n</code></pre>    You should see version numbers for both.</li> </ol>"},{"location":"w02-recap_setup/class/#2-go-to-the-web-folder","title":"2. Go to the Web folder","text":"<ol> <li> <p>Navigate to the <code>web</code> folder in the repository.</p> </li> <li> <p>Open a terminal in the <code>web/w2_canvas_example</code> folder and run:    <pre><code>npm install\n</code></pre>    This will install the necessary packages for the web development environment.</p> </li> <li> <p>Start the development server by running:    <pre><code>npm dev run\n</code></pre>    This will start a development server and open a browser window with the web page.</p> </li> <li> <p>You can now experiment with the code in <code>src/main.js</code>. The changes you make will be reflected in the browser window.</p> </li> </ol>"},{"location":"w02-recap_setup/class/#3-copy-the-file-datasetssol_datajson-to-the-webw2_canvas_examplepublic-folder","title":"3. Copy the file <code>Datasets/sol_data.json</code> to the <code>web/w2_canvas_example/public</code> folder","text":"<p>This file contains the data we will use in the canvas example. All acessible files should be in the <code>public</code> folder.</p>"},{"location":"w02-recap_setup/class/#4-experiment-with-the-code","title":"4. Experiment with the code","text":"<p>Open the <code>src/main.js</code> file and experiment with the code. You can change the draw functions, shapes, and other properties to see how they affect the canvas.</p>"},{"location":"w02-recap_setup/class/#5-additional-resources","title":"5. Additional Resources","text":"<ul> <li>MDN Web Docs</li> <li>W3Schools</li> <li>Canvas API</li> </ul>"},{"location":"w03-principles/assignment_w3_perception/","title":"Week 3 Assignment: Perception","text":"<p>Let's start by importing the necessary libraries.</p> In\u00a0[23]: Copied! <pre>import matplotlib.pyplot as plt\nimport random\nimport math\nimport numpy as np\n%matplotlib inline\n</pre> import matplotlib.pyplot as plt import random import math import numpy as np %matplotlib inline <p>Let's do an experiment! The procedure is as follows:</p> <ol> <li>Generate a random number between [1, 10];</li> <li>Use a horizontal bar to represent the number, i.e., the length of the bar is equal to the number;</li> <li>Guess the length of the bar by comparing it to two other bars with length 1 and 10 respectively;</li> <li>Store your guess (perceived length) and actual length to two separate lists;</li> <li>Repeat the above steps many times;</li> <li>Check whether Steven's power-law holds.</li> </ol> <p>First, let's define the length of a short and a long bar. We also create two empty lists to store perceived and actual length.</p> In\u00a0[5]: Copied! <pre>l_short_bar = 1\nl_long_bar = 10\n\nperceived_length_list = []\nactual_length_list = []\n</pre> l_short_bar = 1 l_long_bar = 10  perceived_length_list = [] actual_length_list = [] In\u00a0[\u00a0]: Copied! <pre>mystery_length = random.uniform(1, 10)  # generate a number between 1 and 10. this is the *actual* length.\n\nplt.barh(np.arange(3), [l_short_bar, mystery_length, l_long_bar], align='center')\nplt.yticks(np.arange(3), ('1', '?', '10'))\nplt.xticks([]) # no hint!\n</pre> mystery_length = random.uniform(1, 10)  # generate a number between 1 and 10. this is the *actual* length.  plt.barh(np.arange(3), [l_short_bar, mystery_length, l_long_bar], align='center') plt.yticks(np.arange(3), ('1', '?', '10')) plt.xticks([]) # no hint! <p>Btw, <code>np.arange</code> is used to create a simple integer list <code>[0, 1, 2]</code>.</p> In\u00a0[\u00a0]: Copied! <pre>np.arange(3)\n</pre> np.arange(3) <p>Now let's define a function to perform the experiment once. When you run this function, it picks a random number between 1.0 and 10.0 and show the bar chart. Then it asks you to input your estimate of the length of the middle bar. It then saves that number to the <code>perceived_length_list</code> and the actual answer to the <code>actual_length_list</code>.</p> <p>Note, if the input box does not appear for you try (1) switching to firefox, OR (2) removing the input line from the code and manually record the numbers in a cell, OR (3) adding a prompt to it like input(\"enter estimation\")</p> In\u00a0[13]: Copied! <pre>def run_exp_once():\n    mystery_length = random.uniform(1, 10)  # generate a number between 1 and 10. \n\n    plt.barh(np.arange(3), [l_short_bar, mystery_length, l_long_bar], height=0.5, align='center')\n    plt.yticks(np.arange(3), ('1', '?', '10'))\n    plt.xticks([]) # no hint!\n    plt.show()\n    \n    perceived_length_list.append( float(input()) )\n    actual_length_list.append(mystery_length)\n</pre> def run_exp_once():     mystery_length = random.uniform(1, 10)  # generate a number between 1 and 10.       plt.barh(np.arange(3), [l_short_bar, mystery_length, l_long_bar], height=0.5, align='center')     plt.yticks(np.arange(3), ('1', '?', '10'))     plt.xticks([]) # no hint!     plt.show()          perceived_length_list.append( float(input()) )     actual_length_list.append(mystery_length) <p>Now run the experiment:</p> In\u00a0[\u00a0]: Copied! <pre>run_exp_once()\n</pre> run_exp_once() <p>Run the experiment many times to gather your data. Check the two lists to make sure that you have the proper dataset. The length of the two lists should be the same.</p> <p>Note: You can run the experiment as many times as you want. Just rerun the cell or use a for.</p> In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE In\u00a0[\u00a0]: Copied! <pre>plt.scatter(x=[1,5,10], y=[1,10, 5])\n</pre> plt.scatter(x=[1,5,10], y=[1,10, 5]) <p>Q3: Now plot your result using the <code>scatter()</code> function. You should also use <code>plt.title()</code>, <code>plt.xlabel()</code>, and <code>plt.ylabel()</code> to label your axes and the plot itself.</p> In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE <p>After plotting, let's fit the relation between actual and perceived lengths using a polynomial function. We can easily do it using <code>curve_fit(f, x, y)</code> in Scipy, which is to fit $x$ and $y$ using the function <code>f</code>. In our case, $f = a*x^b +c$. For instance, we can check whether this works by creating a fake dataset that follows the exact form:</p> In\u00a0[\u00a0]: Copied! <pre>from scipy.optimize import curve_fit\n\ndef func(x, a, b, c):\n    return a * np.power(x, b) + c\n\nx = np.arange(20)  # [0,1,2,3, ..., 19]\ny = np.power(x, 2) # [0,1,4,9, ... ]\n\npopt, pcov = curve_fit(func, x, y)\nprint('{:.2f} x^{:.2f} + {:.2f}'.format(*popt))\n</pre> from scipy.optimize import curve_fit  def func(x, a, b, c):     return a * np.power(x, b) + c  x = np.arange(20)  # [0,1,2,3, ..., 19] y = np.power(x, 2) # [0,1,4,9, ... ]  popt, pcov = curve_fit(func, x, y) print('{:.2f} x^{:.2f} + {:.2f}'.format(*popt)) <p>Q4: Now fit your data! Do you see roughly linear relationship between the actual and the perceived lengths? It's ok if you don't!</p> In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE In\u00a0[\u00a0]: Copied! <pre>n1 = 0.005\nn2 = 0.05\n\nradius1 = np.sqrt(n1/np.pi) # area = pi * r * r\nradius2 = np.sqrt(n2/np.pi)\nrandom_radius = np.sqrt(n1*random.uniform(1,10)/np.pi)\n\nplt.axis('equal')\nplt.axis('off')\ncirc1 = plt.Circle( (0,0),         radius1, clip_on=False )\ncirc2 = plt.Circle( (4*radius2,0), radius2, clip_on=False )\nrand_circ = plt.Circle((2*radius2,0), random_radius, clip_on=False )\n\nplt.gca().add_artist(circ1)\nplt.gca().add_artist(circ2)\nplt.gca().add_artist(rand_circ)\n</pre> n1 = 0.005 n2 = 0.05  radius1 = np.sqrt(n1/np.pi) # area = pi * r * r radius2 = np.sqrt(n2/np.pi) random_radius = np.sqrt(n1*random.uniform(1,10)/np.pi)  plt.axis('equal') plt.axis('off') circ1 = plt.Circle( (0,0),         radius1, clip_on=False ) circ2 = plt.Circle( (4*radius2,0), radius2, clip_on=False ) rand_circ = plt.Circle((2*radius2,0), random_radius, clip_on=False )  plt.gca().add_artist(circ1) plt.gca().add_artist(circ2) plt.gca().add_artist(rand_circ) <p>Let's have two lists for this experiment.</p> In\u00a0[21]: Copied! <pre>perceived_area_list = []\nactual_area_list = []\n</pre> perceived_area_list = [] actual_area_list = [] <p>And define a function for the experiment.</p> <p>Note, if the input box does not appear for you try (1) switching to firefox, OR (2) removing the input line from the code and manually record the numbers in a cell, OR (3) adding a prompt to it like input(\"enter estimation\")</p> In\u00a0[24]: Copied! <pre>def run_area_exp_once(n1=0.005, n2=0.05):    \n    radius1 = np.sqrt(n1/np.pi) # area = pi * r * r\n    radius2 = np.sqrt(n2/np.pi)\n    \n    mystery_number = random.uniform(1,10)\n    random_radius = np.sqrt(n1*mystery_number/math.pi)\n\n    plt.axis('equal')\n    plt.axis('off')\n    circ1 = plt.Circle( (0,0),         radius1, clip_on=False )\n    circ2 = plt.Circle( (4*radius2,0), radius2, clip_on=False )\n    rand_circ = plt.Circle((2*radius2,0), random_radius, clip_on=False )\n    plt.gca().add_artist(circ1)\n    plt.gca().add_artist(circ2)\n    plt.gca().add_artist(rand_circ)    \n    plt.show()\n    \n    perceived_area_list.append( float(input()) )\n    actual_area_list.append(mystery_number)\n</pre> def run_area_exp_once(n1=0.005, n2=0.05):         radius1 = np.sqrt(n1/np.pi) # area = pi * r * r     radius2 = np.sqrt(n2/np.pi)          mystery_number = random.uniform(1,10)     random_radius = np.sqrt(n1*mystery_number/math.pi)      plt.axis('equal')     plt.axis('off')     circ1 = plt.Circle( (0,0),         radius1, clip_on=False )     circ2 = plt.Circle( (4*radius2,0), radius2, clip_on=False )     rand_circ = plt.Circle((2*radius2,0), random_radius, clip_on=False )     plt.gca().add_artist(circ1)     plt.gca().add_artist(circ2)     plt.gca().add_artist(rand_circ)         plt.show()          perceived_area_list.append( float(input()) )     actual_area_list.append(mystery_number) <p>Q5: Now you can run the experiment many times, plot the result, and fit a power-law curve to test the Stevens' power-law!</p> In\u00a0[\u00a0]: Copied! <pre># TODO: put your code here. You can use multiple cells. \n\n# YOUR SOLUTION HERE\n</pre> # TODO: put your code here. You can use multiple cells.   # YOUR SOLUTION HERE <p>What is your result? How are the exponents different from each other? Have you observed a result consistent with the Stevens' power-law?</p>"},{"location":"w03-principles/assignment_w3_perception/#week-3-assignment-perception","title":"Week 3 Assignment: Perception\u00b6","text":"<p>In this assignment, we will perform a small experiment to test the Stevens' power law.</p> <p>You should follow the instructions of this notebook and answer the questions with your code and results. After that, you should convert this notebook to HTML or PDF and submit it via Canvas. For full instructions on how to submit assignments, please check the previous assignment.</p> <p>Check out <code>why_visualization.ipynb</code> for bonus points.</p>"},{"location":"w03-principles/assignment_w3_perception/#perception-of-length","title":"Perception of length\u00b6","text":"<p>Let's run the experiment.</p> <p>The <code>random</code> module in Python provides various random number generators, and the <code>random.uniform(a,b)</code> function returns a floating point number in [a,b].</p> <p>We can plot horizontal bars using the <code>pyplot.barh()</code> function. Using this function, we can produce a bar graph that looks like this:</p>"},{"location":"w03-principles/assignment_w3_perception/#plotting-the-result","title":"Plotting the result\u00b6","text":"<p>Now we can draw the scatter plot of perceived and actual length. The <code>matplotlib</code>'s <code>scatter()</code> function will do this. This is the backend of the pandas' scatterplot. Here is an example of how to use <code>scatter</code>:</p>"},{"location":"w03-principles/assignment_w3_perception/#perception-of-area","title":"Perception of area\u00b6","text":"<p>Similar to the above experiment, we now represent a random number as a circle, and the area of the circle is equal to the number.</p> <p>First, calculate the radius of a circle from its area and then plot using the <code>Circle()</code> function. <code>plt.Circle((0,0), r)</code> will plot a circle centered at (0,0) with radius <code>r</code>.</p>"},{"location":"w03-principles/why_visualization/","title":"Why Visualization?","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\n\n# Load the dataset\ndata = pd.read_csv('../../Datasets/Anscombe_quartet_data.csv')\n# The ../../ are needed to go back two levels in the directory structure.\n# Note that the path is relative to the location of the notebook file.\n</pre> import pandas as pd import numpy as np  # Load the dataset data = pd.read_csv('../../Datasets/Anscombe_quartet_data.csv') # The ../../ are needed to go back two levels in the directory structure. # Note that the path is relative to the location of the notebook file. <p>Explore the dataset and calculate the mean and standard deviation for each dataset.</p> In\u00a0[\u00a0]: Copied! <pre># Explore here\n</pre> # Explore here <p>Also calculate the correlation between the x123 and y1,y2,y3 variables. Also do the same for the x4 and y4 variables.</p> In\u00a0[\u00a0]: Copied! <pre># Calculate correlations here\n</pre> # Calculate correlations here <p>Now, showcase the advantages of visualization by plotting the data. You can use any visualization library you want.</p> <p>For instance in matplotlib you can use the following code to plot data:</p> <pre>import matplotlib.pyplot as plt # Import the library\n\nfig = plt.figure(figsize=(12, 6)) # Create a figure object\nx1 = [2,4,6,8,10] # Example x values\ny1 = [4,4,4,8,8] # Example y values\nplt.plot(x1, y1, 'o') # Plot the data\nplt.show() # Show the plot\n</pre> <p>Check out the documentation on how to create a matrix of 2x2 plots with the data from the Anscombe's Quartet dataset. Check out subplots in matplotlib documentation</p> In\u00a0[\u00a0]: Copied! <pre># Plot the data here\n</pre> # Plot the data here In\u00a0[\u00a0]: Copied! <pre>data = pd.read_csv('../../Datasets/data9b.csv')\ndata.head()\n</pre> data = pd.read_csv('../../Datasets/data9b.csv') data.head() In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here  <p>SHORT REPORT HERE</p>"},{"location":"w03-principles/why_visualization/#why-visualization","title":"Why Visualization?\u00b6","text":"<p>In the class, we explored the Anscombe's Quartet. This dataset is composed of four different datasets that have the same statistical properties. However, when we visualize the data, we can see that they are quite different. Illustrate this concept by yourself using the Anscombe's Quartet dataset.</p> <p>In addition to the Anscombe's dataset and without giving too much away, let's also explore a very particular dataset for a simple statistics exercise.</p> <p>This notebook can be submitted together with assignment 3 for bonus points.</p> <p>Save it as a HTML or PDF file and submit it on the assignment page on Canvas.</p>"},{"location":"w03-principles/why_visualization/#anscombes-quartet-dataset","title":"Anscombe's Quartet dataset\u00b6","text":""},{"location":"w03-principles/why_visualization/#simple-statistics-exercise","title":"Simple statistics exercise\u00b6","text":"<p>Load the data, each row contains for one person the number of steps that this person took on a particular day (steps), the body mass index (bmi) and gender (male or female). Assume that both traits are normally distributed for males and for females. Consider the following (alternative, not null) hypotheses:</p> <p>a) There is a difference in the mean number of steps between women and men.</p> <p>b) The correlation coefficient between steps and bmi is negative for women.</p> <p>c) The correlation coefficient between steps and bmi is positive for men.</p> <p>Think about which test to use and calculate the corresponding P-value.</p> <p>Hint: You can use the <code>scipy.stats</code> library to calculate correlation and the p-value.</p> <p>Tip: Check out the <code>pearsonr</code> function.</p> <p>Which other conclusions can you draw from the data?</p> <p>If you are not familiar to statistics or want to try something else, you can instead (or also) explore the dataset and try to find other interesting insights. Feel free to explore it!</p> <p>Write a very short report (50-150 words) with your findings and submit it together within the notebook.</p> <p>Important: If possible, try not to share your findings with your colleagues. This is an individual assignment. All students that submit this part of the assignment will receive bonus points.</p>"},{"location":"w05-data/assignment_m05_billboard/","title":"Homework 5 - Tidy and Process the Billboard Dataset","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\n# 1. Load the Billboard dataset\ndf_bill = pd.read_csv(\"../../Datasets/billboard.csv\")  # Adjust path if needed\n\n# Let's quickly check the first few rows to confirm the structure.\ndf_bill.head()\n</pre> import pandas as pd  # 1. Load the Billboard dataset df_bill = pd.read_csv(\"../../Datasets/billboard.csv\")  # Adjust path if needed  # Let's quickly check the first few rows to confirm the structure. df_bill.head()  <p>The dataset has columns like:</p> <ul> <li><p>year, artist.inverted, track, time, genre, etc. (Song info)</p> </li> <li><p>date.entered, date.peaked (Chart-related dates)</p> </li> <li><p>x1st.week through x76th.week (Chart positions over 76 weeks)</p> </li> </ul> <p>Our objective is to melt these weekly columns into a single <code>week</code> and <code>rank</code> column.</p> In\u00a0[\u00a0]: Copied! <pre># 2. Melt/unpivot the data into a \"tidy\" format.\n# HINT: you can use pd.melt(). \n# We'll create a new DataFrame, for example df_tidy.\n\n# Your code here\n</pre> # 2. Melt/unpivot the data into a \"tidy\" format. # HINT: you can use pd.melt().  # We'll create a new DataFrame, for example df_tidy.  # Your code here  <p>After melting, each row should represent one song in one week. However, the <code>week</code> column will have strings like <code>\"x1st.week\"</code>, <code>\"x2nd.week\"</code>, etc. Let's clean those up and create a numeric week column.</p> In\u00a0[\u00a0]: Copied! <pre># 3. Convert the week column to numeric (strip the \"x\", remove \".week\", etc.)\n\n# Your code here\n</pre> # 3. Convert the week column to numeric (strip the \"x\", remove \".week\", etc.)  # Your code here  <p>Now, <code>week</code> should be 1, 2, 3, ... 76. Next, we want to calculate the exact date on the chart for each row by adding <code>(week - 1) * 7 days</code> to <code>date.entered</code>. Create a column named <code>\"date\"</code> to hold the result. Make sure <code>date.entered</code> is converted to datetime first, if it isn't already.</p> In\u00a0[\u00a0]: Copied! <pre># 4. Create the 'date' column: date = date.entered + (week - 1)*7 days\n\n# Your code here\n</pre> # 4. Create the 'date' column: date = date.entered + (week - 1)*7 days  # Your code here  In\u00a0[\u00a0]: Copied! <pre># 5.1 Create a songs table (unique rows). Assign a song_id. \n# Then merge that ID back into the main tidy DataFrame.\n</pre> # 5.1 Create a songs table (unique rows). Assign a song_id.  # Then merge that ID back into the main tidy DataFrame.  In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here  In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here  <p>By the end of this process you should have:</p> <ul> <li><p>A songs table with unique song info and a <code>song_id</code> for each song (keep song names, artists, etc.)</p> </li> <li><p>A positions table with <code>(song_id, week, rank, date)</code> for each song-week.</p> </li> </ul> <p>For example:</p> <p>songs table:</p> song_id artist track time year 1 The Beatles Hey Jude 3:35 1968 2 Elton John Rocket 3:10 1972 3 Whitney Houston I Will Always 4:30 1992 ... ... ... ... ... <p>positions table:</p> song_id week rank date 1 1 1 1968-09-14 1 2 1 1968-09-21 1 3 1 1968-09-28 ... ... ... ... In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here  In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here  In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here  In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here  In\u00a0[\u00a0]: Copied! <pre># 1. Example: Histogram of the number of weeks each song stayed in the top 10.\n# (Assuming you've created a 'weeks_in_top_10' table or series.)\n\n# Your code here\n</pre> # 1. Example: Histogram of the number of weeks each song stayed in the top 10. # (Assuming you've created a 'weeks_in_top_10' table or series.)  # Your code here  <ol> <li><p>Line Chart of a Song's Rank Over Time</p> <ul> <li><p>Pick a well-known song from the dataset (e.g., one of the most weeks at #1).</p> </li> <li><p>Plot its <code>rank</code> vs. <code>date</code> (or <code>week</code>).</p> </li> <li><p>The line should show how the rank changes over consecutive weeks.</p> </li> </ul> </li> </ol> In\u00a0[\u00a0]: Copied! <pre># Use the original `positions` DataFrame to plot a song's rank over time.\n# Your code here\n</pre> # Use the original `positions` DataFrame to plot a song's rank over time. # Your code here  <ol> <li><p>Distribution of Peak Ranks</p> <ul> <li><p>For each song, find its best (lowest number) rank.</p> </li> <li><p>Plot a histogram of these peak positions. Where do most songs peak? (#1, #5, #20, etc.)</p> </li> </ul> </li> </ol> In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here   In\u00a0[\u00a0]: Copied! <pre># Your extra analyses here\n</pre> # Your extra analyses here"},{"location":"w05-data/assignment_m05_billboard/#homework-5-tidy-and-process-the-billboard-dataset","title":"Homework 5 - Tidy and Process the Billboard Dataset\u00b6","text":"<p>The goal of this homework is to tidy the Billboard dataset and perform some basic analysis and visualization.</p> <p>You will demonstrate your understanding of data tidying principles and your ability to manipulate and visualize data using Python.</p> <p>The Billboard dataset comes with 76 columns corresponding to the chart position of each song from <code>x1st.week</code> through <code>x76th.week</code>. This is a classic example of wide data that needs to be melted (unpivoted) into a long (tidy) format.</p>"},{"location":"w05-data/assignment_m05_billboard/#instructions","title":"Instructions\u00b6","text":"<ol> <li><p>Project Setup: Follow the instructions on how to set up your Python and Jupyter (or VSCode) environment, as well as how to clone or download our repository. These can be found in the class notes.</p> </li> <li><p>Dataset Path: Verify the path to the CSV file (e.g. <code>\"../../Datasets/billboard.csv\"</code>). Adjust if necessary.</p> </li> <li><p>Load the Billboard CSV into a DataFrame with <code>pd.read_csv()</code>.</p> </li> <li><p>Melt/Unpivot the 76 weekly columns (<code>x1st.week</code> through <code>x76th.week</code>) into two new columns:</p> <ul> <li><p><code>week</code> (indicating which week of the chart it is, e.g., 1, 2, 3...)</p> </li> <li><p><code>rank</code> (the song's chart position for that week).</p> </li> <li><p>This is typically done using <code>pd.melt()</code> or <code>pd.wide_to_long()</code>.</p> </li> </ul> </li> <li><p>Clean the <code>week</code> values so that they are numeric (1 to 76) instead of strings like <code>\"x1st.week\"</code>.</p> </li> <li><p>Create a <code>date</code> column that indicates the exact date on the chart for each row, using:</p> <pre><code>date = date.entered + (week - 1) * 7 days\n</code></pre> <ul> <li>You may want to transform <code>date.entered</code> in a <code>datetime</code> format (you may need to convert it with <code>pd.to_datetime()</code> check documentation).</li> </ul> </li> <li><p>Split the data into two tables:</p> <ol> <li><p>songs table, which should contain unique song information (e.g., <code>song_id</code>, <code>artist</code>, <code>track</code>, <code>time</code>, etc.).</p> <ul> <li><p>Remember to drop duplicates so you only have one row per song.</p> </li> <li><p>Generate a new <code>song_id</code> column that can be used to join back to the weekly info.</p> </li> </ul> </li> <li><p>positions table, which should contain weekly data: <code>(song_id, week, rank, date)</code>.</p> </li> </ol> </li> <li><p>Save your tidy DataFrames to Feather format:</p> <ul> <li><p>Save the full \u201ctidy\u201d DataFrame (one row per song-week) with a filename suffix <code>_tidy</code>.</p> </li> <li><p>Also save the separate <code>songs</code> and <code>positions</code> tables as separate Feather files (e.g., <code>songs.feather</code>, <code>positions.feather</code>).</p> </li> </ul> </li> <li><p>Submission:</p> <ul> <li><p>Export your completed notebook to HTML or PDF.</p> </li> <li><p>If on Jupyter, select <code>File</code> &gt; <code>Export Notebook As</code> &gt; <code>HTML</code>.</p> </li> <li><p>If on VSCode, use <code>Jupyter: Export to HTML</code> from the command palette.</p> </li> <li><p>Submit your exported file on Canvas.</p> </li> </ul> </li> </ol>"},{"location":"w05-data/assignment_m05_billboard/#dataset-overview","title":"Dataset Overview\u00b6","text":"<p>The dataset consists of songs and their weekly chart positions on the Billboard Hot 100. It has the following main columns:</p> <ul> <li><code>year</code>: The year the song entered the chart.</li> <li><code>artist</code>: The artist of the song.</li> <li><code>track</code>: The title of the song.</li> <li><code>time</code>: The duration of the song.</li> <li><code>date.entered</code>: The date the song first entered the chart.</li> <li><code>x1st.week</code> to <code>x76th.week</code>: The chart position of the song for each of the 76 weeks (1 = highest rank, 100 = lower rank).</li> </ul>"},{"location":"w05-data/assignment_m05_billboard/#goals-detailed","title":"Goals (Detailed)\u00b6","text":"<ol> <li><p>Load the Billboard dataset from CSV into a pandas DataFrame.</p> </li> <li><p>Tidy the data by converting the weekly columns into a single <code>week</code> column (numeric) and a <code>rank</code> column.</p> </li> <li><p>Compute an exact <code>date</code> for each row by adding <code>(week - 1) * 7 days</code> to the <code>date.entered</code>.</p> </li> <li><p>Split the data into:</p> <ul> <li><p>songs: (One row per song) containing static info and a new <code>song_id</code>.</p> </li> <li><p>positions: (One row per song-week) containing columns: <code>[song_id, week, rank, date]</code>.</p> </li> </ul> </li> <li><p>Save everything to Feather files (or CSV if you prefer, but Feather is recommended).</p> </li> </ol>"},{"location":"w05-data/assignment_m05_billboard/#5-split-into-two-tables","title":"5. Split into Two Tables\u00b6","text":"<p>Why split? We often separate the static song info (e.g., artist, track, time, genre) from the weekly chart performance (week, rank, date).</p> <ul> <li><p>Songs Table: Contains unique identifiers for each song plus basic metadata.</p> </li> <li><p>Positions Table: Contains <code>(song_id, week, rank, date)</code>, referencing the song_id from the songs table.</p> </li> </ul> <p>Generate a new <code>song_id</code> for each unique song, then merge/join this <code>song_id</code> back into the melted DataFrame.</p>"},{"location":"w05-data/assignment_m05_billboard/#6-create-the-positions-table","title":"6. Create the Positions Table\u00b6","text":"<p>Once the <code>song_id</code> is part of the melted DataFrame, we can select the columns <code>(song_id, week, rank, date)</code> into a separate DataFrame called positions.</p>"},{"location":"w05-data/assignment_m05_billboard/#7-save-tidy-data-to-feather","title":"7. Save Tidy Data to Feather\u00b6","text":"<p>We want to save:</p> <ul> <li><p>The complete tidy DataFrame (one row per song-week) with a filename suffix <code>_tidy</code>.</p> </li> <li><p>The songs and positions tables as separate Feather files.</p> </li> </ul>"},{"location":"w05-data/assignment_m05_billboard/#part-1-basic-analysis","title":"Part 1 - Basic Analysis\u00b6","text":"<p>Below are some guided analyses to help you get started. Feel free to explore more on your own!</p>"},{"location":"w05-data/assignment_m05_billboard/#a-only-songs-that-reached-top-10","title":"A. Only songs that reached top 10\u00b6","text":"<ol> <li><p>Use <code>query()</code> to filter rows in <code>positions</code> for <code>rank &lt;= 10</code>.</p> </li> <li><p>Merge or join with the <code>songs</code> table to get the song details.</p> </li> </ol> <p>Hint: Remember you can drop duplicates if you only want the list of unique songs that reached top 10.</p>"},{"location":"w05-data/assignment_m05_billboard/#b-how-long-did-each-song-stay-in-the-top-10","title":"B. How long did each song stay in the top 10?\u00b6","text":"<ol> <li><p>Filter <code>positions</code> for <code>rank &lt;= 10</code>.</p> </li> <li><p>Group by <code>song_id</code> and count the number of rows. This gives <code>weeks_in_top_10</code>.</p> </li> <li><p>Merge back with the <code>songs</code> table to display the track/artist.</p> </li> </ol>"},{"location":"w05-data/assignment_m05_billboard/#c-in-which-week-did-each-song-first-reach-the-top-10","title":"C. In which week did each song first reach the top 10?\u00b6","text":"<ol> <li><p>Again filter for <code>rank &lt;= 10</code>.</p> </li> <li><p>Group by <code>song_id</code> and find the minimum <code>week</code> value. Call this <code>week_reached_top_10</code>.</p> </li> </ol>"},{"location":"w05-data/assignment_m05_billboard/#part-2-visualization","title":"Part 2 - Visualization\u00b6","text":"<p>For this part, you may use Altair (Preferable!), Matplotlib, Seaborn, or any other Python visualization library.</p> <p>Below are some suggested questions and plots:</p> <ol> <li><p>Histogram of Weeks in the Top 10</p> <ul> <li><p>How many songs stayed for 1 week vs. 2 weeks vs. 10 weeks in the top 10?</p> </li> <li><p>Plot this distribution.</p> </li> </ul> </li> </ol>"},{"location":"w05-data/assignment_m05_billboard/#bonus-additional-analysis-optional-but-will-give-you-extra-credit","title":"Bonus: Additional Analysis (Optional, but will give you extra credit!)\u00b6","text":"<p>(Add more charts and visualizations as you see fit!)</p> <p>Be creative and explore the dataset! showcase at least a new plot or analysis.</p>"},{"location":"w05-data/assignment_m05_billboard/#final-instructions","title":"Final Instructions\u00b6","text":"<ol> <li><p>Make sure your notebook runs from top to bottom without errors.</p> </li> <li><p>Export it to HTML or PDF.</p> </li> <li><p>Submit via Canvas.</p> </li> </ol> <p>Good luck and have fun exploring the Billboard dataset!</p>"},{"location":"w05-data/class/","title":"Lecture 9: Tidy Data","text":"In\u00a0[144]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[147]: Copied! <pre># population over time\ndf = pd.DataFrame({\n    \"country\": [\"USA\", \"Canada\", \"Brazil\"],\n    \"1990\": [253, 28, 149],\n    \"2000\": [282, 31, 170],\n    \"2010\": [309, 34, 192],\n    \"2020\": [339, 38, 209],\n    \"continent\": [\"North America\", \"North America\", \"South America\"],\n})\n\n# Wide format\ndisplay(df)\n</pre> # population over time df = pd.DataFrame({     \"country\": [\"USA\", \"Canada\", \"Brazil\"],     \"1990\": [253, 28, 149],     \"2000\": [282, 31, 170],     \"2010\": [309, 34, 192],     \"2020\": [339, 38, 209],     \"continent\": [\"North America\", \"North America\", \"South America\"], })  # Wide format display(df)  country 1990 2000 2010 2020 continent 0 USA 253 282 309 339 North America 1 Canada 28 31 34 38 North America 2 Brazil 149 170 192 209 South America In\u00a0[150]: Copied! <pre>df_tidy = df.melt(id_vars=[\"country\", \"continent\"], var_name=\"year\", value_name=\"population\")\ndisplay(df_tidy)\n</pre> df_tidy = df.melt(id_vars=[\"country\", \"continent\"], var_name=\"year\", value_name=\"population\") display(df_tidy)  country continent year population 0 USA North America 1990 253 1 Canada North America 1990 28 2 Brazil South America 1990 149 3 USA North America 2000 282 4 Canada North America 2000 31 5 Brazil South America 2000 170 6 USA North America 2010 309 7 Canada North America 2010 34 8 Brazil South America 2010 192 9 USA North America 2020 339 10 Canada North America 2020 38 11 Brazil South America 2020 209 <p>Notice how each row now represents one country in one year, and each column is a single variable.</p> In\u00a0[151]: Copied! <pre>df_wide = df_tidy.pivot(index=\"country\", columns=\"year\", values=\"population\")\ndisplay(df_wide)\n</pre> df_wide = df_tidy.pivot(index=\"country\", columns=\"year\", values=\"population\") display(df_wide)  year 1990 2000 2010 2020 country Brazil 149 170 192 209 Canada 28 31 34 38 USA 253 282 309 339 <p>Here, each row is a country, and each column is a year\u2014back to wide format.</p> In\u00a0[164]: Copied! <pre>for key,data in df_tidy.groupby(\"year\"):\n    display(key)\n    display(data)\n</pre> for key,data in df_tidy.groupby(\"year\"):     display(key)     display(data) <pre>'1990'</pre> country continent year population 0 USA North America 1990 253 1 Canada North America 1990 28 2 Brazil South America 1990 149 <pre>'2000'</pre> country continent year population 3 USA North America 2000 282 4 Canada North America 2000 31 5 Brazil South America 2000 170 <pre>'2010'</pre> country continent year population 6 USA North America 2010 309 7 Canada North America 2010 34 8 Brazil South America 2010 192 <pre>'2020'</pre> country continent year population 9 USA North America 2020 339 10 Canada North America 2020 38 11 Brazil South America 2020 209 In\u00a0[161]: Copied! <pre>df_year_mean = df_tidy.groupby(\"year\")[\"population\"].std()\ndisplay(df_year_mean)\n# \n</pre> df_year_mean = df_tidy.groupby(\"year\")[\"population\"].std() display(df_year_mean) #  <pre>year\n1990    112.606986\n2000    125.741799\n2010    138.008454\n2020    150.964676\nName: population, dtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>df_year_country_sum = df_tidy.groupby([\"year\", \"continent\"])[\"population\"].sum()\ndisplay(df_year_country_sum)\n</pre> df_year_country_sum = df_tidy.groupby([\"year\", \"continent\"])[\"population\"].sum() display(df_year_country_sum)  <pre>year  continent    \n1990  North America    281\n      South America    149\n2000  North America    313\n      South America    170\n2010  North America    343\n      South America    192\n2020  North America    377\n      South America    209\nName: population, dtype: int64</pre> <p>This returns a multi-index Series, showing the population by year and by country.</p> In\u00a0[167]: Copied! <pre>df_agg = df_tidy.groupby(\"year\").agg({\"population\": [\"mean\", \"max\",\"sum\"]})\ndisplay(df_agg)\n</pre> df_agg = df_tidy.groupby(\"year\").agg({\"population\": [\"mean\", \"max\",\"sum\"]}) display(df_agg)  population mean max sum year 1990 143.333333 253 430 2000 161.000000 282 483 2010 178.333333 309 535 2020 195.333333 339 586 <p>This shows the average (<code>mean</code>) population and the maximum (<code>max</code>) population in each year.</p> In\u00a0[169]: Copied! <pre># Create a copy with artificially introduced NaNs\ndf_missing = df_tidy.copy()\ndf_missing.loc[(df_missing[\"country\"] == \"Brazil\") &amp; (df_missing[\"year\"] == \"2020\"), \"population\"] = None\n\ndisplay(df_missing)\n</pre> # Create a copy with artificially introduced NaNs df_missing = df_tidy.copy() df_missing.loc[(df_missing[\"country\"] == \"Brazil\") &amp; (df_missing[\"year\"] == \"2020\"), \"population\"] = None  display(df_missing)  country continent year population 0 USA North America 1990 253.0 1 Canada North America 1990 28.0 2 Brazil South America 1990 149.0 3 USA North America 2000 282.0 4 Canada North America 2000 31.0 5 Brazil South America 2000 170.0 6 USA North America 2010 309.0 7 Canada North America 2010 34.0 8 Brazil South America 2010 192.0 9 USA North America 2020 339.0 10 Canada North America 2020 38.0 11 Brazil South America 2020 NaN In\u00a0[173]: Copied! <pre>df_dropped = df_missing.dropna(subset=[\"population\"])\ndisplay(df_dropped)\n</pre> df_dropped = df_missing.dropna(subset=[\"population\"]) display(df_dropped)  country continent year population 0 USA North America 1990 253.0 1 Canada North America 1990 28.0 2 Brazil South America 1990 149.0 3 USA North America 2000 282.0 4 Canada North America 2000 31.0 5 Brazil South America 2000 170.0 6 USA North America 2010 309.0 7 Canada North America 2010 34.0 8 Brazil South America 2010 192.0 9 USA North America 2020 339.0 10 Canada North America 2020 38.0 <p>Brazil's 2020 row is completely removed because of the missing population.</p> In\u00a0[174]: Copied! <pre>df_filled = df_missing.fillna(0)\ndisplay(df_filled)\n</pre> df_filled = df_missing.fillna(0) display(df_filled)  country continent year population 0 USA North America 1990 253.0 1 Canada North America 1990 28.0 2 Brazil South America 1990 149.0 3 USA North America 2000 282.0 4 Canada North America 2000 31.0 5 Brazil South America 2000 170.0 6 USA North America 2010 309.0 7 Canada North America 2010 34.0 8 Brazil South America 2010 192.0 9 USA North America 2020 339.0 10 Canada North America 2020 38.0 11 Brazil South America 2020 0.0 <p>Now, the missing value is replaced with <code>0</code>.</p> In\u00a0[175]: Copied! <pre>gdp_data = pd.DataFrame({\n    \"country\": [\"USA\", \"Canada\", \"Brazil\"],\n    \"year\": [\"2020\", \"2020\", \"2020\"],\n    \"gdp\": [21439, 1736, 1445],  # GDP in billions (fictitious or approximate)\n})\n\n# Merging on both country and year\ndf_merged = df_tidy.merge(gdp_data, on=[\"country\", \"year\"], how=\"left\")\ndisplay(df_merged)\n</pre> gdp_data = pd.DataFrame({     \"country\": [\"USA\", \"Canada\", \"Brazil\"],     \"year\": [\"2020\", \"2020\", \"2020\"],     \"gdp\": [21439, 1736, 1445],  # GDP in billions (fictitious or approximate) })  # Merging on both country and year df_merged = df_tidy.merge(gdp_data, on=[\"country\", \"year\"], how=\"left\") display(df_merged)  country continent year population gdp 0 USA North America 1990 253 NaN 1 Canada North America 1990 28 NaN 2 Brazil South America 1990 149 NaN 3 USA North America 2000 282 NaN 4 Canada North America 2000 31 NaN 5 Brazil South America 2000 170 NaN 6 USA North America 2010 309 NaN 7 Canada North America 2010 34 NaN 8 Brazil South America 2010 192 NaN 9 USA North America 2020 339 21439.0 10 Canada North America 2020 38 1736.0 11 Brazil South America 2020 209 1445.0 <p>We used <code>how=\"left\"</code> so that all rows from <code>df_tidy</code> are preserved, even if some may not match in <code>gdp_data</code>.</p> <ul> <li><p><code>how=\"inner\"</code> would only keep matching rows.</p> </li> <li><p><code>how=\"outer\"</code> keeps all rows from both DataFrames.</p> </li> </ul> In\u00a0[176]: Copied! <pre># Sort by population descending\ndf_sorted = df_tidy.sort_values(\"population\", ascending=False)\ndisplay(df_sorted)\n</pre> # Sort by population descending df_sorted = df_tidy.sort_values(\"population\", ascending=False) display(df_sorted)  country continent year population 9 USA North America 2020 339 6 USA North America 2010 309 3 USA North America 2000 282 0 USA North America 1990 253 11 Brazil South America 2020 209 8 Brazil South America 2010 192 5 Brazil South America 2000 170 2 Brazil South America 1990 149 10 Canada North America 2020 38 7 Canada North America 2010 34 4 Canada North America 2000 31 1 Canada North America 1990 28 In\u00a0[177]: Copied! <pre>df_filtered = df_tidy.query(\"population &gt; 200 and country == 'USA'\")\ndisplay(df_filtered)\n</pre> df_filtered = df_tidy.query(\"population &gt; 200 and country == 'USA'\") display(df_filtered)  country continent year population 0 USA North America 1990 253 3 USA North America 2000 282 6 USA North America 2010 309 9 USA North America 2020 339 In\u00a0[178]: Copied! <pre>import pandas as pd\n\n# 1. Load the Billboard dataset\ndf_bill = pd.read_csv(\"../../Datasets/billboard.csv\")\n\n# Let's check a few columns to see the structure.\ndf_bill.head()\n</pre> import pandas as pd  # 1. Load the Billboard dataset df_bill = pd.read_csv(\"../../Datasets/billboard.csv\")  # Let's check a few columns to see the structure. df_bill.head() Out[178]: year artist.inverted track time genre date.entered date.peaked x1st.week x2nd.week x3rd.week ... x67th.week x68th.week x69th.week x70th.week x71st.week x72nd.week x73rd.week x74th.week x75th.week x76th.week 0 2000 Destiny's Child Independent Women Part I 3:38 Rock 2000-09-23 2000-11-18 78 63.0 49.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 2000 Santana Maria, Maria 4:18 Rock 2000-02-12 2000-04-08 15 8.0 6.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 2000 Savage Garden I Knew I Loved You 4:07 Rock 1999-10-23 2000-01-29 71 48.0 43.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 2000 Madonna Music 3:45 Rock 2000-08-12 2000-09-16 41 23.0 18.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 2000 Aguilera, Christina Come On Over Baby (All I Want Is You) 3:38 Rock 2000-08-05 2000-10-14 57 47.0 45.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN <p>5 rows \u00d7 83 columns</p> <p>The dataset has columns like:</p> <ul> <li><p>year, artist.inverted, track, time, genre \u2026 (song info)</p> </li> <li><p>date.entered, date.peaked \u2026 (chart-related dates)</p> </li> <li><p>x1st.week through x76th.week \u2026 (chart positions over 76 weeks)</p> </li> </ul> <p>We want to melt these weekly columns into a single <code>week</code> and <code>rank</code> column.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Notice how each row is now one song in one week. However, the <code>week</code> column currently contains strings like <code>\"x1st.week\"</code>, <code>\"x2nd.week\"</code>, etc. Let's clean those up and create a numeric week column.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Now, <code>week = 1, 2, 3, ... 76</code>. Next, we want to calculate the exact date on the chart for each row by adding <code>week * 7</code> days to <code>date.entered</code>.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Next, we merge this <code>song_id</code> back into our <code>df_tidy</code> so we can create the positions table.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Now we need to remove duplicates to get a list of unique songs that reached the top 10.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"w05-data/class/#lecture-9-tidy-data","title":"Lecture 9: Tidy Data\u00b6","text":"<p>What is Tidy Data?</p> <p>Tidy data is a structured format where:</p> <ul> <li><p>Each row represents one observation (e.g., a country in a given year).</p> </li> <li><p>Each column is a variable (e.g., GDP, life expectancy).</p> </li> <li><p>Each table represents a dataset (e.g., economic statistics).</p> </li> </ul> <p>\ud83d\udca1 Why use tidy data?</p> <ul> <li><p>Easier to analyze: Works well with <code>groupby()</code>, <code>agg()</code>, and visualization libraries like Seaborn.</p> </li> <li><p>More readable: No redundant columns.</p> </li> <li><p>Plays nicely with Pandas and Seaborn.</p> </li> </ul>"},{"location":"w05-data/class/#wide-format-to-tidy-long-format","title":"Wide Format to Tidy (Long) Format\u00b6","text":"<p>In the dataset below, each year's population is in a separate column, which makes it a wide format.</p> <p>We can convert it to tidy format using <code>pd.melt()</code>.</p>"},{"location":"w05-data/class/#pdmelt","title":"<code>pd.melt()</code>\u00b6","text":"<ul> <li><p><code>id_vars</code>: The columns that stay the same (identifiers).</p> </li> <li><p><code>var_name</code>: Name of the new column that will hold the old column headers (years).</p> </li> <li><p><code>value_name</code>: Name of the new column that will store the values (population in this case).</p> </li> </ul>"},{"location":"w05-data/class/#converting-tidy-long-format-back-to-wide-format","title":"Converting Tidy (Long) Format Back to Wide Format\u00b6","text":"<ul> <li>If you ever need to go back to wide format, you can use <code>pivot()</code> or <code>pivot_table()</code>.</li> </ul>"},{"location":"w05-data/class/#summarizing-tidy-data-with-groupby","title":"Summarizing Tidy Data with <code>groupby()</code>\u00b6","text":"<p>Tidy data makes it straightforward to group and summarize.</p>"},{"location":"w05-data/class/#groupbyyearpopulationmean","title":"<code>groupby(\"year\")[\"population\"].mean()</code>\u00b6","text":"<p>This computes the mean population for each year across all countries.</p>"},{"location":"w05-data/class/#grouping-by-multiple-columns","title":"Grouping by Multiple Columns\u00b6","text":"<p>We can also group by both <code>year</code> and <code>country</code>.</p>"},{"location":"w05-data/class/#agg-for-multiple-summaries","title":"<code>agg()</code> for Multiple Summaries\u00b6","text":"<p>The <code>agg()</code> function lets us apply multiple aggregations at once.</p> <p>For instance, we can find the mean and the max population per year.</p>"},{"location":"w05-data/class/#handling-missing-data","title":"Handling Missing Data\u00b6","text":"<p>Let's introduce some missing values to demonstrate <code>dropna()</code> and <code>fillna()</code>.</p>"},{"location":"w05-data/class/#dropna","title":"<code>dropna()</code>\u00b6","text":"<ul> <li>Removes rows with missing values.</li> </ul>"},{"location":"w05-data/class/#fillna","title":"<code>fillna()</code>\u00b6","text":"<ul> <li>Fills missing values with a specified value or method.</li> </ul>"},{"location":"w05-data/class/#combining-data-with-merge","title":"Combining Data with <code>merge()</code>\u00b6","text":"<p>Often, you'll have multiple DataFrames that need to be joined.</p> <p>Below is an example for merging a GDP dataset with our population dataset.</p>"},{"location":"w05-data/class/#example-sort_values-and-query","title":"Example: <code>sort_values()</code> and <code>query()</code>\u00b6","text":"<p>Tidy data also makes it easy to sort and filter.</p>"},{"location":"w05-data/class/#query","title":"<code>query()</code>\u00b6","text":"<p>An alternative way to filter rows:</p> <pre>df.query(\"population &gt; 200 and country == 'USA'\")\n</pre> <p>is equivalent to</p> <pre>df[(df[\"population\"] &gt; 200) &amp; (df[\"country\"] == \"USA\")]\n</pre>"},{"location":"w05-data/class/#tidy-and-process-the-billboard-dataset","title":"Tidy and Process the Billboard Dataset\u00b6","text":"<p>The Billboard dataset comes with 76 columns corresponding to the chart position of each song from <code>x1st.week</code> through <code>x76th.week</code>. This is a classic example of wide data that needs to be melted (unpivoted) into a long (tidy) format.</p>"},{"location":"w05-data/class/#goals","title":"Goals\u00b6","text":"<ol> <li><p>Load the Billboard dataset from CSV.</p> </li> <li><p>Tidy the data so each row represents one song in one week.</p> </li> <li><p>Calculate the actual date for each week using <code>date.entered + week * 7 days</code>.</p> </li> <li><p>Split the data into two tables:</p> <ul> <li><p>A songs table with static song information.</p> </li> <li><p>A positions table with <code>(song_id, week, rank, date)</code>.</p> </li> </ul> </li> <li><p>Save the tidy data to Feather format in the same directory with <code>_tidy</code> suffix.</p> </li> </ol>"},{"location":"w05-data/class/#split-into-two-tables","title":"Split into Two Tables\u00b6","text":"<p>Why split? We often separate the static song info (e.g., artist, track, time, genre) from the weekly chart performance (week, rank, date).</p> <ul> <li><p>Songs Table: Contains unique identifiers for each song plus basic metadata.</p> </li> <li><p>Positions Table: Contains <code>(song_id, week, rank, date)</code>, referencing the song_id from the songs table.</p> </li> </ul>"},{"location":"w05-data/class/#create-the-positions-table","title":"Create the Positions Table\u00b6","text":"<p>We only keep the relevant columns for weekly positions: <code>song_id</code>, <code>week</code>, <code>rank</code>, and <code>date</code>.</p>"},{"location":"w05-data/class/#8playing-with-the-data","title":"8.Playing with the data\u00b6","text":"<p>Now that we have our data in a tidy format, let's do some analysis.</p>"},{"location":"w05-data/class/#only-songs-that-reached-top-10","title":"Only songs that reached top 10\u00b6","text":"<p>We can use <code>query()</code> to filter the data for songs that reached the top 10 at least once. We will merge this back to the songs table to get the song details.</p>"},{"location":"w05-data/class/#how-long-did-each-song-stay-in-the-top-10","title":"How long did each song stay in the top 10?\u00b6","text":""},{"location":"w05-data/class/#in-which-week-did-each-song-reach-the-top-10","title":"In which week did each song reach the top 10?\u00b6","text":""},{"location":"w05-data/class/#9-save-tidy-data-to-feather","title":"9. Save Tidy Data to Feather\u00b6","text":"<p>We want to save:</p> <ul> <li><p>The tidy DataFrame (<code>df_tidy</code>) to a single file with the suffix <code>_tidy</code>.</p> </li> <li><p>(Optionally) Also save songs and positions as separate Feather files if needed.</p> </li> </ul>"},{"location":"w05-data/more_tidy_data/","title":"Lecture 9.5: More on Tidy Data","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport altair as alt\n</pre> import pandas as pd import altair as alt In\u00a0[\u00a0]: Copied! <pre>wide_df = pd.DataFrame({\n    \"Year\":  [2020, 2021, 2022, 2023],\n    \"North\": [100, 150, 200, 250],\n    \"South\": [ 90, 130, 170, 220],\n    \"East\":  [ 80, 120, 160, 210],\n    \"West\":  [ 70, 110, 150, 200]\n})\n\ndisplay(wide_df)\n</pre> wide_df = pd.DataFrame({     \"Year\":  [2020, 2021, 2022, 2023],     \"North\": [100, 150, 200, 250],     \"South\": [ 90, 130, 170, 220],     \"East\":  [ 80, 120, 160, 210],     \"West\":  [ 70, 110, 150, 200] })  display(wide_df)  In\u00a0[\u00a0]: Copied! <pre>tidy_df = wide_df.melt(\n    id_vars=[\"Year\"], \n    var_name=\"Region\", \n    value_name=\"Sales\"\n)\n\ndisplay(tidy_df)\n</pre> tidy_df = wide_df.melt(     id_vars=[\"Year\"],      var_name=\"Region\",      value_name=\"Sales\" )  display(tidy_df)  In\u00a0[\u00a0]: Copied! <pre>east_after_2021_wide = wide_df.loc[wide_df[\"Year\"] &gt; 2021, [\"Year\", \"East\"]]\n# east_after_2021_wide = east_after_2021_wide.rename(columns={\"East\": \"Sales\"})\ndisplay(east_after_2021_wide)\n</pre> east_after_2021_wide = wide_df.loc[wide_df[\"Year\"] &gt; 2021, [\"Year\", \"East\"]] # east_after_2021_wide = east_after_2021_wide.rename(columns={\"East\": \"Sales\"}) display(east_after_2021_wide)  In\u00a0[\u00a0]: Copied! <pre>east_after_2021_tidy = tidy_df.query(\"Region == 'East' and Year &gt; 2021\")\n\ndisplay(east_after_2021_tidy)\n</pre> east_after_2021_tidy = tidy_df.query(\"Region == 'East' and Year &gt; 2021\")  display(east_after_2021_tidy)  In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\nmean_north = wide_df[\"North\"].mean()\nmean_south = wide_df[\"South\"].mean()\nmean_east  = wide_df[\"East\"].mean()\nmean_west  = wide_df[\"West\"].mean()\n\nscaled_wide = wide_df.copy()\nscaled_wide[\"North\"] = scaled_wide[\"North\"] / mean_north\nscaled_wide[\"South\"] = scaled_wide[\"South\"] / mean_south\nscaled_wide[\"East\"]  = scaled_wide[\"East\"]  / mean_east\nscaled_wide[\"West\"]  = scaled_wide[\"West\"]  / mean_west\n\n# a for could be used to avoid repetition, but it's still cumbersome\n\ndisplay(scaled_wide)\n</pre> import numpy as np  mean_north = wide_df[\"North\"].mean() mean_south = wide_df[\"South\"].mean() mean_east  = wide_df[\"East\"].mean() mean_west  = wide_df[\"West\"].mean()  scaled_wide = wide_df.copy() scaled_wide[\"North\"] = scaled_wide[\"North\"] / mean_north scaled_wide[\"South\"] = scaled_wide[\"South\"] / mean_south scaled_wide[\"East\"]  = scaled_wide[\"East\"]  / mean_east scaled_wide[\"West\"]  = scaled_wide[\"West\"]  / mean_west  # a for could be used to avoid repetition, but it's still cumbersome  display(scaled_wide)  In\u00a0[\u00a0]: Copied! <pre>scaled_tidy = tidy_df.copy()\nscaled_tidy[\"Sales_Scaled\"] = scaled_tidy.groupby(\"Region\")[\"Sales\"] \\\n                                         .transform(lambda x: x / x.mean())\n\ndisplay(scaled_tidy)\n</pre> scaled_tidy = tidy_df.copy() scaled_tidy[\"Sales_Scaled\"] = scaled_tidy.groupby(\"Region\")[\"Sales\"] \\                                          .transform(lambda x: x / x.mean())  display(scaled_tidy)  In\u00a0[\u00a0]: Copied! <pre>chart_north = alt.Chart(wide_df).mark_line(stroke='blue').encode(\n    x=\"Year:O\",\n    y=\"North:Q\"\n).properties(title=\"North\")\n\nchart_south = alt.Chart(wide_df).mark_line(stroke='red').encode(\n    x=\"Year:O\",\n    y=\"South:Q\"\n).properties(title=\"South\")\n\nchart_east = alt.Chart(wide_df).mark_line(stroke='green').encode(\n    x=\"Year:O\",\n    y=\"East:Q\"\n).properties(title=\"East\")\n\nchart_west = alt.Chart(wide_df).mark_line(stroke='orange').encode(\n    x=\"Year:O\",\n    y=\"West:Q\"\n).properties(title=\"West\")\n\n# compose the charts\nchart_wide_layered = alt.layer(chart_north, chart_south, chart_east, chart_west).properties(\n    width=400,\n    height=300\n)\n\n# Adding the legends would be a bit more work\n\nchart_wide_layered\n</pre> chart_north = alt.Chart(wide_df).mark_line(stroke='blue').encode(     x=\"Year:O\",     y=\"North:Q\" ).properties(title=\"North\")  chart_south = alt.Chart(wide_df).mark_line(stroke='red').encode(     x=\"Year:O\",     y=\"South:Q\" ).properties(title=\"South\")  chart_east = alt.Chart(wide_df).mark_line(stroke='green').encode(     x=\"Year:O\",     y=\"East:Q\" ).properties(title=\"East\")  chart_west = alt.Chart(wide_df).mark_line(stroke='orange').encode(     x=\"Year:O\",     y=\"West:Q\" ).properties(title=\"West\")  # compose the charts chart_wide_layered = alt.layer(chart_north, chart_south, chart_east, chart_west).properties(     width=400,     height=300 )  # Adding the legends would be a bit more work  chart_wide_layered  <ul> <li><p>We had to manually define a chart for each column (region).</p> </li> <li><p>Adding new regions or removing one requires extra lines of code.</p> </li> <li><p>Legends and other customizations would be more complex.</p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>chart_wide_pivot = (\n    alt.Chart(wide_df)\n    .transform_fold( # this is like pd.melt()!!!!\n        fold=[\"North\",\"South\",\"East\",\"West\"],  # must list every region\n        as_=[\"Region\",\"Sales\"]\n    )\n    .mark_line()\n    .encode(\n        x=\"Year:O\",\n        y=\"Sales:Q\",\n        color=\"Region:N\"\n    )\n    .properties(\n        width=400,\n        height=300\n    )\n)\n\nchart_wide_pivot\n</pre> chart_wide_pivot = (     alt.Chart(wide_df)     .transform_fold( # this is like pd.melt()!!!!         fold=[\"North\",\"South\",\"East\",\"West\"],  # must list every region         as_=[\"Region\",\"Sales\"]     )     .mark_line()     .encode(         x=\"Year:O\",         y=\"Sales:Q\",         color=\"Region:N\"     )     .properties(         width=400,         height=300     ) )  chart_wide_pivot  In\u00a0[\u00a0]: Copied! <pre>chart_tidy = alt.Chart(tidy_df).mark_bar().encode(\n    x=\"Year:O\",\n    y=\"Sales:Q\",\n    color=\"Region:N\"\n).properties(\n    width=400,\n    height=300\n).interactive()\n\nchart_tidy\n</pre> chart_tidy = alt.Chart(tidy_df).mark_bar().encode(     x=\"Year:O\",     y=\"Sales:Q\",     color=\"Region:N\" ).properties(     width=400,     height=300 ).interactive()  chart_tidy"},{"location":"w05-data/more_tidy_data/#lecture-95-more-on-tidy-data","title":"Lecture 9.5: More on Tidy Data\u00b6","text":"<p>With this I plan to convince you that tidy data is the way to go.</p> <p>This notebook demonstrates:</p> <ol> <li><p>How to convert wide to tidy format.</p> </li> <li><p>How filtering, grouping, and scaling is more complicated in wide format compared to tidy.</p> </li> <li><p>How visualization in Altair is also simpler with tidy data.</p> </li> </ol>"},{"location":"w05-data/more_tidy_data/#1-imports-and-setup","title":"1. Imports and Setup\u00b6","text":""},{"location":"w05-data/more_tidy_data/#2-creating-the-example-dataset-wide-format","title":"2. Creating the Example Dataset (Wide Format)\u00b6","text":"<p>We start with a wide DataFrame: each region (<code>North</code>, <code>South</code>, <code>East</code>, <code>West</code>) is in its own column.</p>"},{"location":"w05-data/more_tidy_data/#3-converting-wide-to-tidy","title":"3. Converting Wide to Tidy\u00b6","text":"<p>We use <code>pd.melt()</code> to unpivot the data so that each row represents a <code>(Year, Region, Sales)</code> combination.</p>"},{"location":"w05-data/more_tidy_data/#4-filtering-grouping","title":"4. Filtering &amp; Grouping\u00b6","text":"<p>We'll do two demonstrations:</p> <ol> <li><p>Filtering data for a specific region and year range.</p> </li> <li><p>Scaling each region's values by its mean sales.</p> </li> </ol>"},{"location":"w05-data/more_tidy_data/#41-filtering-for-the-east-region-where-year-2021","title":"4.1 Filtering for the East region where <code>Year &gt; 2021</code>\u00b6","text":""},{"location":"w05-data/more_tidy_data/#411-wide-format-complicated","title":"4.1.1 Wide Format (Complicated)\u00b6","text":"<p>Since there's no direct \"Region\" column, we must manually pick the column (<code>East</code>) and rename it:</p>"},{"location":"w05-data/more_tidy_data/#412-tidy-format-easy","title":"4.1.2 Tidy Format (Easy)\u00b6","text":"<p>Just use <code>query(\"Region == 'East' and Year &gt; 2021\")</code>:</p>"},{"location":"w05-data/more_tidy_data/#42-grouping-scaling-by-the-mean","title":"4.2 Grouping &amp; Scaling by the Mean\u00b6","text":"<p>Task: Divide each region's sales by that region's mean sales.</p>"},{"location":"w05-data/more_tidy_data/#421-wide-format","title":"4.2.1 Wide Format\u00b6","text":"<p>Compute the mean of each column and then manually scale:</p>"},{"location":"w05-data/more_tidy_data/#422-tidy-format","title":"4.2.2 Tidy Format\u00b6","text":"<p>A single <code>.groupby(\"Region\")</code> and <code>.transform()</code> handles all regions:</p>"},{"location":"w05-data/more_tidy_data/#5-visualization-in-altair","title":"5. Visualization in Altair\u00b6","text":"<p>We'll create a simple line plot of Sales over Year for each region.</p>"},{"location":"w05-data/more_tidy_data/#51-wide-format","title":"5.1 Wide Format\u00b6","text":""},{"location":"w05-data/more_tidy_data/#option-a-layer-each-regions-line-separately","title":"Option A: Layer each region's line separately\u00b6","text":""},{"location":"w05-data/more_tidy_data/#option-b-use-transform_fold-to-pivot-columns-inside-altair","title":"Option B: Use <code>transform_fold</code> to pivot columns inside Altair\u00b6","text":"<p>This is effectively using Altair to do what <code>melt()</code> does, but you must list all columns:</p>"},{"location":"w05-data/more_tidy_data/#52-tidy-format-so-much-simpler","title":"5.2 Tidy Format (So Much Simpler)\u00b6","text":"<p>A single command, no need to list the regions. Altair automatically creates multiple lines, coloring by <code>Region</code>.</p>"},{"location":"w05-data/more_tidy_data/#6-final-comparison","title":"6. Final Comparison\u00b6","text":"Action Wide Format Tidy Format Filtering Must pick &amp; rename columns, no direct 'Region' column Simple <code>query(\"Region=='X'\")</code> Grouping or Scaling Repeat operations for each column (or do complex loops) Single <code>.groupby(\"Region\")</code> call Adding New Categories Must add new columns and update code Rows just grow, existing code continues to work Visualization Either layer each column manually or use <code>transform_fold</code> One-liner with <code>color=\"Region:N\"</code> Code Complexity High (lots of repeated steps, listing columns) Low (concise, flexible) <p>Takeaway:</p> <ul> <li><p>Tidy format is recommended for most data science tasks because it avoids repetitive code, makes grouping/filtering straightforward, and integrates smoothly with visualization libraries like Altair, seaborn, etc.</p> </li> <li><p>Wide format can be okay for quick tasks or certain machine learning APIs, but it often becomes cumbersome when you need to filter, group, scale, or plot multiple categories.</p> </li> </ul> <p>Whenever possible, convert to tidy to save time and headaches!</p>"},{"location":"w06-encoding/assigment_m06A_visual_encoding/","title":"Homework 6A - 1D Visualization","text":"In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline  In\u00a0[7]: Copied! <pre>cars = pd.read_json('../../Datasets/cars.json')\ncars.head()\n# adjust the path to the location of your dataset\n</pre> cars = pd.read_json('../../Datasets/cars.json') cars.head() # adjust the path to the location of your dataset Out[7]: Name Miles_per_Gallon Cylinders Displacement Horsepower Weight_in_lbs Acceleration Year Origin 0 chevrolet chevelle malibu 18.0 8 307.0 130.0 3504 12.0 1970-01-01 USA 1 buick skylark 320 15.0 8 350.0 165.0 3693 11.5 1970-01-01 USA 2 plymouth satellite 18.0 8 318.0 150.0 3436 11.0 1970-01-01 USA 3 amc rebel sst 16.0 8 304.0 150.0 3433 12.0 1970-01-01 USA 4 ford torino 17.0 8 302.0 140.0 3449 10.5 1970-01-01 USA <p>Let's consider the <code>Acceleration</code> column as our 1D data. If we ask pandas to plot this series, it'll produce a line graph where the index becomes the horizontal axis.</p> In\u00a0[8]: Copied! <pre>cars.Acceleration.plot()\n</pre> cars.Acceleration.plot() Out[8]: <pre>&lt;Axes: &gt;</pre> <p>Because the index is not really meaningful, drawing line between subsequent values is misleading! This is definitely not the plot we want!</p> <p>It's actually not trivial to use pandas to create an 1-D scatter plot. Instead, we can use <code>matploblib</code>'s <code>scatter</code> function. We can first create an array with zeros that we can use as the vertical coordinates of the points that we will plot.  <code>np.zeros_like</code> returns an array with zeros that matches the shape of the input array.</p> In\u00a0[9]: Copied! <pre>np.zeros_like([1,2,3])\n</pre> np.zeros_like([1,2,3]) Out[9]: <pre>array([0, 0, 0])</pre> <p>Q: now can you create an 1D scatter plot wit <code>matplotlib</code>'s scatter function? Make the figure wide (e.g. set <code>figsize=(10,2)</code>) and then remove the y tics.</p> In\u00a0[5]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE Out[5]: <pre>&lt;matplotlib.collections.PathCollection at 0x129e03fe0&gt;</pre> <p>As you can see, there are lots of occlusions. So this plot cannot show the distribution properly and we would like to fix it. How about adding some jitters? You can use <code>numpy</code>'s <code>random.rand()</code> function to generate random numbers, instead of using an array with zeros.</p> <p>Q: create a jittered 1D scatter plot.</p> In\u00a0[6]: Copied! <pre># jittered_y = ...\n\n# YOUR SOLUTION HERE\n</pre> # jittered_y = ...  # YOUR SOLUTION HERE <p>We can further improve this by adding transparency to the symbols. The transparency option for <code>scatter</code> function is called <code>alpha</code>. Set it to be 0.2.</p> <p>Q: create a jittered 1D scatter plot with transparency (alpha=0.2)</p> In\u00a0[7]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE <p>Another strategy is using empty symbols. The option is <code>facecolors</code>. You can also change the stroke color (<code>edgecolors</code>).</p> <p>Q: create a jittered 1D scatter plot with empty symbols.</p> In\u00a0[8]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE In\u00a0[9]: Copied! <pre># TODO: play with N and see what happens. \nN = 100000\nx = np.random.rand(N)\njittered_y = np.random.rand(N)\n\n# YOUR SOLUTION HERE\n</pre> # TODO: play with N and see what happens.  N = 100000 x = np.random.rand(N) jittered_y = np.random.rand(N)  # YOUR SOLUTION HERE In\u00a0[10]: Copied! <pre>cars.Acceleration.hist()\n</pre> cars.Acceleration.hist() Out[10]: <pre>&lt;Axes: &gt;</pre> <p>You can adjust the bin size, which is the main parameter of the histogram.</p> In\u00a0[11]: Copied! <pre>cars.Acceleration.hist(bins=15)\n</pre> cars.Acceleration.hist(bins=15) Out[11]: <pre>&lt;Axes: &gt;</pre> <p>You can even specify the actual bins.</p> In\u00a0[12]: Copied! <pre>bins = [7.5, 8.5, 10, 15, 30]\ncars.Acceleration.hist(bins=bins)\n</pre> bins = [7.5, 8.5, 10, 15, 30] cars.Acceleration.hist(bins=bins) Out[12]: <pre>&lt;Axes: &gt;</pre> <p>Do you see anything funky going on with this histogram? What's wrong? Can you fix it?</p> <p>Q: Explain what's wrong with this histogram and fix it.</p> <p>(hints: do you remember what we discussed regarding histogram? Also pandas documentation does not show the option that you should use. You should take a look at the <code>matplotlib</code>'s documentation.</p> In\u00a0[13]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE Out[13]: <pre>&lt;Axes: &gt;</pre> In\u00a0[14]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE Out[14]: <pre>&lt;Axes: &gt;</pre> In\u00a0[11]: Copied! <pre>import seaborn as sns\nimport altair as alt\n</pre> import seaborn as sns import altair as alt  In\u00a0[12]: Copied! <pre>cars.head()\n</pre> cars.head() Out[12]: Name Miles_per_Gallon Cylinders Displacement Horsepower Weight_in_lbs Acceleration Year Origin 0 chevrolet chevelle malibu 18.0 8 307.0 130.0 3504 12.0 1970-01-01 USA 1 buick skylark 320 15.0 8 350.0 165.0 3693 11.5 1970-01-01 USA 2 plymouth satellite 18.0 8 318.0 150.0 3436 11.0 1970-01-01 USA 3 amc rebel sst 16.0 8 304.0 150.0 3433 12.0 1970-01-01 USA 4 ford torino 17.0 8 302.0 140.0 3449 10.5 1970-01-01 USA In\u00a0[13]: Copied! <pre>sns.stripplot(x='Origin', y='Acceleration', data=cars)\n</pre> sns.stripplot(x='Origin', y='Acceleration', data=cars) Out[13]: <pre>&lt;Axes: xlabel='Origin', ylabel='Acceleration'&gt;</pre> <p>And you can easily add jitters or even create a beeswarm plot.</p> In\u00a0[14]: Copied! <pre>sns.stripplot(x='Origin', y='Acceleration', data=cars, jitter=True)\n</pre> sns.stripplot(x='Origin', y='Acceleration', data=cars, jitter=True) Out[14]: <pre>&lt;Axes: xlabel='Origin', ylabel='Acceleration'&gt;</pre> <p>Seems like European cars tend to have good acceleration. \ud83d\ude0e Let's look at the beeswarm plot, which is a pretty nice option for fairly small datasets.</p> In\u00a0[15]: Copied! <pre>sns.swarmplot(x='Origin', y='Acceleration', data=cars)\n</pre> sns.swarmplot(x='Origin', y='Acceleration', data=cars) Out[15]: <pre>&lt;Axes: xlabel='Origin', ylabel='Acceleration'&gt;</pre> <p>Seaborn also allows you to use colors for another categorical variable. The option is <code>hue</code>.</p> <p>Q: can you create a beeswarm plot where the swarms are grouped by <code>Cylinders</code>, y-values are <code>Acceleration</code>, and colors represent the <code>Origin</code>?</p> In\u00a0[20]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE Out[20]: <pre>&lt;Axes: xlabel='Cylinders', ylabel='Acceleration'&gt;</pre> <p>And of course you can create box plots too.</p> <p>Q: Create boxplots to show the relationships between <code>Cylinders</code> and <code>Acceleration</code>.</p> In\u00a0[21]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE Out[21]: <pre>&lt;Axes: xlabel='Cylinders', ylabel='Acceleration'&gt;</pre> <p>With <code>altair</code>, you're thinking in terms of a whole dataframe, rather than vectors for x or vectors for y. Passing the dataset to <code>Chart</code> creates an empty plot. If you try to run <code>alt.Chart(cars)</code>, it will complain. You need to say what's the visual encoding of the data.</p> In\u00a0[16]: Copied! <pre>alt.Chart(cars)\n</pre> alt.Chart(cars) <pre>\n---------------------------------------------------------------------------\nSchemaValidationError                     Traceback (most recent call last)\nFile ~/miniforge3/envs/dataviz/lib/python3.11/site-packages/altair/vegalite/v5/api.py:4033, in Chart.to_dict(self, validate, format, ignore, context)\n   4031     copy.data = core.InlineData(values=[{}])\n   4032     return super(Chart, copy).to_dict(**kwds)\n-&gt; 4033 return super().to_dict(**kwds)\n\nFile ~/miniforge3/envs/dataviz/lib/python3.11/site-packages/altair/vegalite/v5/api.py:2004, in TopLevelMixin.to_dict(self, validate, format, ignore, context)\n   2001 # remaining to_dict calls are not at top level\n   2002 context[\"top_level\"] = False\n-&gt; 2004 vegalite_spec: Any = _top_schema_base(super(TopLevelMixin, copy)).to_dict(\n   2005     validate=validate, ignore=ignore, context=dict(context, pre_transform=False)\n   2006 )\n   2008 # TODO: following entries are added after validation. Should they be validated?\n   2009 if is_top_level:\n   2010     # since this is top-level we add $schema if it's missing\n\nFile ~/miniforge3/envs/dataviz/lib/python3.11/site-packages/altair/utils/schemapi.py:1169, in SchemaBase.to_dict(self, validate, ignore, context)\n   1167         self.validate(result)\n   1168     except jsonschema.ValidationError as err:\n-&gt; 1169         raise SchemaValidationError(self, err) from None\n   1170 return result\n\nSchemaValidationError: '{'data': {'name': 'data-cab73f27e67d5ff6b36fe04232c5ff83'}}' is an invalid value.\n\n'mark' is a required property</pre> Out[16]: <pre>alt.Chart(...)</pre> In\u00a0[17]: Copied! <pre>alt.Chart(cars).mark_point()\n</pre>  alt.Chart(cars).mark_point() Out[17]: <p>So you just see one point. But actually this is not a single point. This is every row of the dataset represented as a point at the same location. Because there is no specification about where to put the points, it simply draws everything on top of each other. Let's specify how to spread them across the horizontal axis.</p> In\u00a0[18]: Copied! <pre>alt.Chart(cars).mark_point().encode(\n    x='Acceleration',\n)\n</pre> alt.Chart(cars).mark_point().encode(     x='Acceleration', ) Out[18]: <p>There is another nice mark called <code>tick</code>:</p> In\u00a0[19]: Copied! <pre>alt.Chart(cars).mark_tick().encode(\n    x='Acceleration',\n)\n</pre> alt.Chart(cars).mark_tick().encode(     x='Acceleration', ) Out[19]: <p>In <code>altair</code>, histogram is not a special type of visualization, but simply a plot with bars where a variable is binned and a counting aggregation function is used.</p> In\u00a0[20]: Copied! <pre>alt.Chart(cars).mark_bar().encode(\n    x=alt.X('Acceleration', bin=True),\n    y='count()'\n) \n</pre> alt.Chart(cars).mark_bar().encode(     x=alt.X('Acceleration', bin=True),     y='count()' )  Out[20]: <p>Q: can you create a 2D scatterplot with <code>Acceleration</code> and <code>Horsepower</code>? Use <code>Origin</code> for the colors.</p> <p>Note that you need to encode both x and y axes and use mark <code>points</code>.</p> In\u00a0[27]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE Out[27]: <p>Because altair/vega-lite/vega are essentially drawing the chart using javascript (and D3.js), it is very easy to export it on the web. Probably the simplest way is just exporting it into an HTML file: https://altair-viz.github.io/getting_started/starting.html#publishing-your-visualization (you can use the <code>chart.save</code> method).</p> <p>Save the chart to assignment_m06A_plot.html and upload it too.</p> In\u00a0[28]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE"},{"location":"w06-encoding/assigment_m06A_visual_encoding/#homework-6a-1d-visualization","title":"Homework 6A - 1D Visualization\u00b6","text":"<p>In this homework, we will be using different libraries to visualize 1D data. We will use <code>matplotlib</code>, <code>seaborn</code>, and <code>altair</code> to create different types of plots. The goal is to understand how to visualize 1D data using different libraries and how to customize the plots. We will use the cars dataset from the Datasets folder: <code>Datasets/cars.json</code>.</p>"},{"location":"w06-encoding/assigment_m06A_visual_encoding/#instructions","title":"Instructions\u00b6","text":"<ol> <li><p>Project Setup:</p> <ul> <li>Set up your Python and Jupyter (or VSCode) environment.</li> <li>Clone or download the repository provided in class (refer to the class notes).</li> </ul> </li> <li><p>Fill the cells:</p> <ul> <li>Fill in the cells with the code provided in the instructions.</li> <li>You can use the provided code as a starting point and modify it as needed.</li> <li>Make sure to run the code in each cell to see the output.</li> </ul> </li> <li><p>Documentation:</p> <ul> <li>Comment your code and add markdown explanations for each part of your analysis.</li> </ul> </li> <li><p>Submission:</p> <ul> <li>Save your notebook and export as either PDF or HTML. If the visualization is not shown in the html, submit a separated version with altair html. Refer to: https://altair-viz.github.io/getting_started/starting.html#publishing-your-visualization (you can use the <code>chart.save('chart_file.html')</code> method).</li> <li>Submit to Canvas.</li> </ul> </li> </ol>"},{"location":"w06-encoding/assigment_m06A_visual_encoding/#1d-scatter-plot","title":"1D scatter plot\u00b6","text":""},{"location":"w06-encoding/assigment_m06A_visual_encoding/#what-happens-if-you-have-lots-and-lots-of-points","title":"What happens if you have lots and lots of points?\u00b6","text":"<p>Whatever strategy that you use, it's almost useless if you have too many data points. Let's play with different number of data points and see how it looks.</p> <p>It not only becomes completely useless, it also take a while to draw the plot itself.</p>"},{"location":"w06-encoding/assigment_m06A_visual_encoding/#histogram-and-boxplot","title":"Histogram and boxplot\u00b6","text":"<p>When you have lots of data points, you can't no longer use the scatter plots. Even when you don't have millions of data points, you often want to get a quick summary of the distribution rather than seeing the whole dataset. For 1-D datasets, two major approaches are histogram and boxplot. Histogram is about aggregating and counting the data while boxplot is about summarizing the data. Let's first draw some histograms.</p>"},{"location":"w06-encoding/assigment_m06A_visual_encoding/#histogram","title":"Histogram\u00b6","text":"<p>It's very easy to draw a histogram with pandas.</p>"},{"location":"w06-encoding/assigment_m06A_visual_encoding/#boxplot","title":"Boxplot\u00b6","text":"<p>Boxplot can be created with pandas very easily. Check out the <code>plot</code> documentation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html</p> <p>Q: create a box plot of <code>Acceleration</code></p>"},{"location":"w06-encoding/assigment_m06A_visual_encoding/#1d-scatter-plot-with-seaborn-and-altair","title":"1D scatter plot with Seaborn and Altair\u00b6","text":"<p>As you may have noticed, it is not very easy to use <code>matplotlib</code>. The organization of plot functions and parameters are not very systematic. Whenever you draw something, you should search how to do it, what are the parameters you can tweak, etc. You need to manually tweak a lot of things when you work with <code>matplotlib</code>.</p> <p>There are more systematic approaches towards data visualization, such as the \"Grammar of Graphics\". This idea of grammar led to the famous <code>ggplot2</code> (http://ggplot2.tidyverse.org) package in R as well as the Vega &amp; Vega-lite for the web. The grammar-based approach lets you work with tidy data in a natural way, and also lets you approach the data visualization systematically. In other words, they are very cool. \ud83d\ude0e</p> <p>I'd like to introduce two nice Python libraries. One is called <code>seaborn</code> (https://seaborn.pydata.org), which is focused on creating complex statistical data visualizations, and the other is called <code>altair</code> (https://altair-viz.github.io/) and it is a Python library that lets you define a visualization and translates it into vega-lite json.</p> <p>Seaborn would be useful when you are doing exploratory data analysis; altair may be useful if you are thinking about creating and putting an interactive visualization on the web.</p> <p>If you don't have them yet, check the installation page of altair. In <code>conda</code>,</p> <pre><code>$ conda install -c conda-forge altair vega_datasets jupyterlab </code></pre> <p>Let's play with it.</p>"},{"location":"w06-encoding/assigment_m06A_visual_encoding/#beeswarm-plots-with-seaborn","title":"Beeswarm plots with seaborn\u00b6","text":"<p>Seaborn has a built-in function to create 1D scatter plots with multiple categories.</p>"},{"location":"w06-encoding/assigment_m06A_visual_encoding/#altair-basics","title":"Altair basics\u00b6","text":""},{"location":"w06-encoding/assigment_m06B_visual_encoding/","title":"Homework 6B - Visual Encoding","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport altair as alt\n\n# If you're running in a notebook environment that limits rows for interactive charts:\nalt.data_transformers.disable_max_rows()\n\n# Read the dataset\ndf = pd.read_json(\"../../Datasets/movies.json\")\ndf.head()\n\n# Let's create a new column for the ratio of US Gross to Production Budget\ndf['US Gross to Production Budget'] = df['US Gross'] / df['Production Budget']\n</pre> import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import altair as alt  # If you're running in a notebook environment that limits rows for interactive charts: alt.data_transformers.disable_max_rows()  # Read the dataset df = pd.read_json(\"../../Datasets/movies.json\") df.head()  # Let's create a new column for the ratio of US Gross to Production Budget df['US Gross to Production Budget'] = df['US Gross'] / df['Production Budget']  In\u00a0[\u00a0]: Copied! <pre># Fill Code Here\n</pre> # Fill Code Here  In\u00a0[\u00a0]: Copied! <pre># Fill Code Here\n</pre> # Fill Code Here  In\u00a0[\u00a0]: Copied! <pre># Fill Code Here\n</pre> # Fill Code Here  In\u00a0[\u00a0]: Copied! <pre># Fill Code Here\n</pre> # Fill Code Here   <p>Your comments and insights here:</p> <p>Any interesting observations from any of the plots you created so far?</p> In\u00a0[\u00a0]: Copied! <pre># Fill Code Here\n</pre> # Fill Code Here  In\u00a0[\u00a0]: Copied! <pre># Fill Code Here\n</pre> # Fill Code Here  In\u00a0[\u00a0]: Copied! <pre># Fill Code Here\n</pre> # Fill Code Here  In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here"},{"location":"w06-encoding/assigment_m06B_visual_encoding/#homework-6b-visual-encoding","title":"Homework 6B - Visual Encoding\u00b6","text":"<p>In this assignment, you will explore how to encode different data features to visual properties using the movies dataset located in <code>Datasets/movies.json</code>.</p> <p>We will start with simpler visualizations and then move to more complex (and potentially cluttered) visualizations. Finally, you will create a focused visualization that uses only a few selected encodings.</p>"},{"location":"w06-encoding/assigment_m06B_visual_encoding/#instructions","title":"Instructions\u00b6","text":"<p>The assignment is divided into three parts:</p> <ol> <li><p>Project Setup:</p> <ul> <li><p>Set up your Python and Jupyter (or VSCode) environment.</p> </li> <li><p>Clone or download the repository provided in class (refer to the class notes).</p> </li> </ul> </li> <li><p>Simple Visualizations: Create basic visualizations using one or two encodings.</p> </li> <li><p>Cluttered Visualization: Create a visualization that encodes many features simultaneously.</p> </li> <li><p>Focused Visualization: Create one or more visualizations using a few chosen encodings, and discuss your design choices.</p> </li> <li><p>Documentation:</p> <ul> <li>Comment your code and add markdown explanations for each part of your analysis.</li> </ul> </li> <li><p>Submission:</p> <ul> <li>Save your notebook and export as either PDF or HTML. If the visualization is not shown in the html, submit a separated version with altair html. Refer to: https://altair-viz.github.io/getting_started/starting.html#publishing-your-visualization (you can use the <code>chart.save('chart_file.html')</code> method).</li> <li>Submit to Canvas.</li> </ul> </li> </ol> <p>You are free to use any libraries you prefer for visualization, but we recommend using <code>matplotlib</code>, <code>seaborn</code>, and/or <code>altair</code> for this assignment.</p> <p>Happy coding and visualizing!</p>"},{"location":"w06-encoding/assigment_m06B_visual_encoding/#part-0-simple-visualizations","title":"Part 0: Simple Visualizations\u00b6","text":"<p>In this section, you will build simple plots that use minimal visual encodings. This will help you get comfortable with basic mappings before adding complexity.</p>"},{"location":"w06-encoding/assigment_m06B_visual_encoding/#task-01-simple-scatterplot","title":"Task 0.1: Simple Scatterplot\u00b6","text":"<p>Create a basic scatterplot that maps:</p> <ul> <li><p>x-axis: <code>Production Budget</code></p> </li> <li><p>y-axis: <code>US Gross</code></p> </li> </ul> <p>Hint: You can use:</p> <ul> <li><p><code>plt.scatter()</code> from <code>matplotlib</code></p> </li> <li><p><code>sns.scatterplot()</code> from <code>seaborn</code></p> </li> <li><p><code>alt.Chart()</code> from <code>altair</code> with <code>mark_point()</code></p> </li> </ul>"},{"location":"w06-encoding/assigment_m06B_visual_encoding/#task-02-simple-bar-plot","title":"Task 0.2: Simple Bar Plot\u00b6","text":"<p>Now, create a bar plot that shows the average <code>IMDB Rating</code> for each <code>Major Genre</code>.</p> <p>Hint: You can use:</p> <ul> <li><p><code>plt.bar()</code> from <code>matplotlib</code></p> </li> <li><p><code>sns.barplot()</code> from <code>seaborn</code></p> </li> <li><p><code>alt.Chart()</code> from <code>altair</code> with <code>mark_bar()</code></p> </li> </ul>"},{"location":"w06-encoding/assigment_m06B_visual_encoding/#task-03-another-simple-scatterplot","title":"Task 0.3: Another Simple Scatterplot\u00b6","text":"<p>Create a basic scatterplot that maps:</p> <ul> <li><p>x-axis: <code>IMDB Rating</code></p> </li> <li><p>y-axis: <code>US Gross to Production Budget</code></p> </li> </ul> <p>Important: <code>US Gross to Production Budget</code> is a ratio. Thus, you should use the logarithmic scale for the y-axis to better visualize the data.</p> <p>Hint: You can use:</p> <ul> <li><p><code>plt.yscale('log')</code> from <code>matplotlib</code> (also works with <code>seaborn</code>)</p> </li> <li><p><code>y=alt.Y('...').scale(type=\"log\")</code> from <code>altair</code></p> </li> </ul>"},{"location":"w06-encoding/assigment_m06B_visual_encoding/#task-04-another-simple-bar-plot","title":"Task 0.4 : Another Simple Bar Plot\u00b6","text":"<p>Create a bar plot that shows the average <code>US Gross to Production Budget</code> for each <code>Major Genre</code>.</p> <p>Remember to use the logarithmic scale for the y-axis to better visualize the data.</p> <p>What are the insights you can draw from this plot?</p>"},{"location":"w06-encoding/assigment_m06B_visual_encoding/#part-1-intermediate-visual-encodings","title":"Part 1: Intermediate Visual Encodings\u00b6","text":"<p>Now that you have built simple plots, let's gradually add more visual encodings.</p>"},{"location":"w06-encoding/assigment_m06B_visual_encoding/#task-11-adding-color","title":"Task 1.1: Adding Color\u00b6","text":"<p>Extend one of the scatterplots from Tasks 0.1 or 0.3 by encoding:</p> <ul> <li>Color to <code>MPAA Rating</code></li> </ul> <p>Hint: You can use the <code>hue</code> parameter in <code>seaborn</code> or the <code>color</code> parameter in <code>matplotlib</code>.</p> <p>Note: You can also use <code>alt.Chart()</code> with <code>encode(color='MPAA Rating')</code> for color encoding.</p> <p>Complete the code cell below.</p>"},{"location":"w06-encoding/assigment_m06B_visual_encoding/#task-12-adding-size","title":"Task 1.2: Adding Size\u00b6","text":"<p>Further extend the scatterplot by encoding another feature:</p> <ul> <li>Size to e.g. <code>Running Time min</code>, <code>IMDB Votes</code></li> </ul> <p>This adds another dimension to the plot. Fill in the code below.</p>"},{"location":"w06-encoding/assigment_m06B_visual_encoding/#part-2-cluttered-visualization","title":"Part 2: Cluttered Visualization\u00b6","text":"<p>The next task is to create a visualization that encodes as many features as possible. Try mapping:</p> <ul> <li><p>Color</p> </li> <li><p>Size</p> </li> <li><p>Shape/Marker</p> </li> <li><p>Position</p> </li> <li><p>Transparency (alpha)</p> </li> <li><p>Any other visual property you can think of?</p> </li> </ul> <p>Note: The goal is to demonstrate how quickly a visualization can become cluttered when too many encodings are used.</p> <p>You are free to choose any combination of the above encodings. Do your worst!</p> <p>Fill in the code below with your own choices for mappings.</p>"},{"location":"w06-encoding/assigment_m06B_visual_encoding/#part-3-focused-visualization","title":"Part 3: Focused Visualization\u00b6","text":"<p>For the final part, create more visualizations that use only a few well-chosen encodings in each. For example, you might decide to just use 3 encodings (2 positions + color, or two positions + size, or color + size + shape).</p> <ul> <li><p>Map color</p> </li> <li><p>Map size</p> </li> <li><p>Use position (X,Y)</p> </li> </ul> <p>Create a few different visualizations maintaining x and y positions, but changing the color, size, or shape encodings, to showcase different aspects of the data.</p> <p>You can also use redundant encodings, e.g., using both color and shape to represent the same feature.</p> <p>This will help you focus on the most important aspects of the data without overwhelming the viewer with too much information.</p> <p>After creating the visualization, write a brief discussion (just a few sentences) in the markdown cell that follows. Explain:</p> <ul> <li><p>Why you selected these specific encodings.</p> </li> <li><p>How these choices improve interpretability.</p> </li> <li><p>Any trade-offs involved in reducing the number of encoded features.</p> </li> </ul>"},{"location":"w06-encoding/assigment_m06B_visual_encoding/#discussion","title":"Discussion\u00b6","text":"<p>Please write your reflections here:</p> <ul> <li><p>Why did you choose the specific encodings for the focused visualization?</p> </li> <li><p>How do the selected visual properties (color, size, position) help in understanding the data better?</p> </li> <li><p>What improvements in clarity do you observe compared to the cluttered visualization?</p> </li> <li><p>What trade-offs do you notice when reducing the number of encoded features?</p> </li> </ul> <p>Use this cell to provide a brief explanation of your design decisions.</p>"},{"location":"w06-encoding/mapping_data/","title":"Mapping data","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nimport altair as alt\n\n# If you're running in a notebook environment that limits rows for interactive charts:\nalt.data_transformers.disable_max_rows()\n\n# Read the dataset\ndf = pd.read_csv(\"../../Datasets/heart_data.csv\")\ndf.head()\n</pre> import pandas as pd import matplotlib.pyplot as plt import altair as alt  # If you're running in a notebook environment that limits rows for interactive charts: alt.data_transformers.disable_max_rows()  # Read the dataset df = pd.read_csv(\"../../Datasets/heart_data.csv\") df.head()  <p>The dataset columns are:</p> <ul> <li><p>id: Patient identifier.</p> </li> <li><p>age: Age in days (we might convert it to years if needed).</p> </li> <li><p>gender: 1 (women), 2 (men).</p> </li> <li><p>height: Height in centimeters.</p> </li> <li><p>weight: Weight in kilograms.</p> </li> <li><p>ap_hi: Systolic blood pressure.</p> </li> <li><p>ap_lo: Diastolic blood pressure.</p> </li> <li><p>cholesterol: 1 (normal), 2 (above normal), 3 (well above normal).</p> </li> <li><p>gluc: Glucose level (1, 2, 3 with similar meaning as cholesterol).</p> </li> <li><p>smoke: Binary (0 if not smoking, 1 if smoking).</p> </li> <li><p>alco: Binary (0 if not an alcoholic, 1 if alcoholic).</p> </li> <li><p>active: Binary (0 if not physically active, 1 if physically active).</p> </li> <li><p>cardio: Binary (0 if no cardiovascular disease, 1 if it is present).</p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre># Simple histogram of 'weight' using matplotlib\nplt.figure(figsize=(8, 4))\nplt.hist(df['weight'], bins=20, color='lightblue', edgecolor='black')\nplt.title('Weight Distribution (Matplotlib)')\nplt.xlabel('Weight (kg)')\nplt.ylabel('Frequency')\nplt.show()\n</pre> # Simple histogram of 'weight' using matplotlib plt.figure(figsize=(8, 4)) plt.hist(df['weight'], bins=20, color='lightblue', edgecolor='black') plt.title('Weight Distribution (Matplotlib)') plt.xlabel('Weight (kg)') plt.ylabel('Frequency') plt.show()  <p>Pandas can also do quick visualizations since it integrates with matplotlib under the hood.</p> In\u00a0[\u00a0]: Copied! <pre>df['height'].plot(kind='box', title='Height Box Plot (Pandas)', ylabel='Height (cm)')\nplt.show()\n</pre> df['height'].plot(kind='box', title='Height Box Plot (Pandas)', ylabel='Height (cm)') plt.show()  <p>As you can see, matplotlib and pandas are powerful but can sometimes be verbose or less straightforward for interactive or layered visualizations.</p> In\u00a0[\u00a0]: Copied! <pre># A minimal Altair chart:\n\nbasic_1dpoints = alt.Chart(df).mark_point().encode(\n    # We can specify just the field names if we don't need special config yet.\n    x='height',\n)\nbasic_1dpoints\n</pre> # A minimal Altair chart:  basic_1dpoints = alt.Chart(df).mark_point().encode(     # We can specify just the field names if we don't need special config yet.     x='height', ) basic_1dpoints  <p>Basic, right? We just plotted the <code>height</code> column as a 1D scatter plot by mapping it to the x-axis.</p> <p>Now, let's create a bar chart counting the number of patients for each <code>gender</code>.</p> In\u00a0[\u00a0]: Copied! <pre>basic_bar = alt.Chart(df).mark_bar().encode(\n    # For a bar chart, if we only mention one dimension, Altair automatically does a 'count()' on the y-axis.\n    x='gender',\n    y='count()'\n)\n\nbasic_bar\n</pre> basic_bar = alt.Chart(df).mark_bar().encode(     # For a bar chart, if we only mention one dimension, Altair automatically does a 'count()' on the y-axis.     x='gender',     y='count()' )  basic_bar  <p>How does this work?</p> <ul> <li><p><code>alt.Chart(df)</code>: We create a new Chart object using our DataFrame.</p> </li> <li><p><code>.mark_bar()</code>: We want bar marks.</p> </li> <li><p><code>.encode(x='gender', y='count()')</code>: We encode the <code>gender</code> column on the x-axis.</p> <p>By using <code>count()</code>, the bar\u2019s height represents the number of rows for each <code>gender</code>.</p> </li> </ul> <p>Note that we didn\u2019t specify any data types (<code>N</code>, <code>Q</code>, etc.) explicitly. In many cases, Altair can infer them. However, let's be explicit to demonstrate best practices.</p> In\u00a0[\u00a0]: Copied! <pre># Let's do the same chart but specifying types:\nbasic_bar_explicit = alt.Chart(df).mark_bar().encode(\n    x='gender:N',      # Treat gender as Nominal (categorical)\n    y='count()'        # A built-in aggregation for counting rows\n)\n\nbasic_bar_explicit\n</pre> # Let's do the same chart but specifying types: basic_bar_explicit = alt.Chart(df).mark_bar().encode(     x='gender:N',      # Treat gender as Nominal (categorical)     y='count()'        # A built-in aggregation for counting rows )  basic_bar_explicit  In\u00a0[\u00a0]: Copied! <pre>hist_weight = alt.Chart(df).mark_bar().encode(\n    alt.X('weight:Q', bin=True, title='Weight (kg)'),  # Bin the weight values\n    alt.Y('count()', title='Count of Patients')\n).properties(\n    width=400,\n    height=200,\n    title='Histogram of Weight'\n)\n\nhist_weight\n</pre> hist_weight = alt.Chart(df).mark_bar().encode(     alt.X('weight:Q', bin=True, title='Weight (kg)'),  # Bin the weight values     alt.Y('count()', title='Count of Patients') ).properties(     width=400,     height=200,     title='Histogram of Weight' )  hist_weight  In\u00a0[\u00a0]: Copied! <pre>box_height_gender1 = alt.Chart(df.query(\"gender == 1\")).mark_boxplot().encode(\n    y='height:Q'\n).properties(\n    width=300,\n    title='Distribution of Height for Gender 1'\n)\n\nbox_height_gender2 = alt.Chart(df.query(\"gender == 2\")).mark_boxplot().encode(\n    y='height:Q'\n).properties(\n    width=300,\n    title='Distribution of Height for Gender 2'\n)\n\n                               \n\nbox_height_gender1|box_height_gender2\n\n\n\n# ## 5. Customizing Encodings\n#\n# Altair allows us to map columns to various visual channels such as:\n#\n# - `alt.X()` and `alt.Y()` for positions on horizontal/vertical axes.\n# - `alt.Color()` for color.\n# - `alt.Size()` for size.\n# - `alt.Opacity()`, `alt.Shape()`, etc.\n#\n# We can also customize scales, legends, and axis labels.\n#\n# Let\u2019s try something a bit more interesting: a **box plot** of `height` by `gender`. Since gender is nominal, we want to ensure we specify that, and let Altair do the rest.\n#\n# ### 5.1 Box Plot of Height Grouped by Gender\n</pre>  box_height_gender1 = alt.Chart(df.query(\"gender == 1\")).mark_boxplot().encode(     y='height:Q' ).properties(     width=300,     title='Distribution of Height for Gender 1' )  box_height_gender2 = alt.Chart(df.query(\"gender == 2\")).mark_boxplot().encode(     y='height:Q' ).properties(     width=300,     title='Distribution of Height for Gender 2' )                                   box_height_gender1|box_height_gender2    # ## 5. Customizing Encodings # # Altair allows us to map columns to various visual channels such as: # # - `alt.X()` and `alt.Y()` for positions on horizontal/vertical axes. # - `alt.Color()` for color. # - `alt.Size()` for size. # - `alt.Opacity()`, `alt.Shape()`, etc. # # We can also customize scales, legends, and axis labels. # # Let\u2019s try something a bit more interesting: a **box plot** of `height` by `gender`. Since gender is nominal, we want to ensure we specify that, and let Altair do the rest. # # ### 5.1 Box Plot of Height Grouped by Gender  In\u00a0[\u00a0]: Copied! <pre>box_height_gender = alt.Chart(df).mark_boxplot().encode(\n    x='gender:N',       # Nominal\n    y='height:Q',        # Quantitative\n    # color=\"gender:N\",\n).properties(\n    width=300,\n    title='Distribution of Height by Gender'\n)\n\nbox_height_gender\n</pre> box_height_gender = alt.Chart(df).mark_boxplot().encode(     x='gender:N',       # Nominal     y='height:Q',        # Quantitative     # color=\"gender:N\", ).properties(     width=300,     title='Distribution of Height by Gender' )  box_height_gender  <p>This gives us a quick statistical summary of the <code>height</code> distribution for each gender.</p> In\u00a0[\u00a0]: Copied! <pre>scatter_wh = alt.Chart(df).mark_point().encode(\n    x='height:Q',\n    y='weight:Q'\n).properties(\n    width=400,\n    height=300,\n    title='Scatter Plot: Weight vs. Height'\n)\n\nscatter_wh\n</pre> scatter_wh = alt.Chart(df).mark_point().encode(     x='height:Q',     y='weight:Q' ).properties(     width=400,     height=300,     title='Scatter Plot: Weight vs. Height' )  scatter_wh  <p>This basic scatter plot shows how height (x-axis) relates to weight (y-axis), but it's just a simple cloud of points.</p> In\u00a0[\u00a0]: Copied! <pre>scatter_wh_color = alt.Chart(df).mark_point(\n    filled=True,  # Filled circles\n    size=50       # Larger size\n).encode(\n    x='height:Q',\n    y='weight:Q',\n    color='gender:N'  # Color points by gender\n).properties(\n    width=400,\n    height=300,\n    title='Weight vs. Height Colored by Gender'\n)\n\nscatter_wh_color\n</pre> scatter_wh_color = alt.Chart(df).mark_point(     filled=True,  # Filled circles     size=50       # Larger size ).encode(     x='height:Q',     y='weight:Q',     color='gender:N'  # Color points by gender ).properties(     width=400,     height=300,     title='Weight vs. Height Colored by Gender' )  scatter_wh_color  <p>Now we can see how male and female patients (in this dataset, labeled 2 and 1, respectively) may occupy different regions in the height-weight space.</p> <p>Tip: If you want to make the legend or color scale more descriptive, you can replace the numeric codes (1 and 2) with actual labels. One way is to create a mapping in your DataFrame before plotting:</p> <pre>gender_map = {1: 'Female', 2: 'Male'}\n\ndf['gender_label'] = df['gender'].map(gender_map)\n</pre> <p>Then encode:</p> <pre>color='gender_label:N'\n</pre> In\u00a0[\u00a0]: Copied! <pre># Histogram of height (binned)\nhist_height = alt.Chart(df).mark_bar().encode(\n    alt.X('height:Q', bin=True, title='Height (cm)'),\n    alt.Y('count()', title='Count')\n).properties(\n    width=300,\n    height=200,\n    title='Histogram of Height'\n)\n\n# Scatter plot of height vs. weight colored by gender\nscatter_hw_gender = alt.Chart(df).mark_point().encode(\n    alt.X('height:Q', title='Height (cm)'),\n    alt.Y('weight:Q', title='Weight (kg)'),\n    alt.Color('gender:N', title='Gender')\n).properties(\n    width=300,\n    height=200,\n    title='Scatter: Height vs Weight'\n)\n\n# Combine the two charts side by side\ncombined_charts = hist_height | scatter_hw_gender\ncombined_charts\n</pre> # Histogram of height (binned) hist_height = alt.Chart(df).mark_bar().encode(     alt.X('height:Q', bin=True, title='Height (cm)'),     alt.Y('count()', title='Count') ).properties(     width=300,     height=200,     title='Histogram of Height' )  # Scatter plot of height vs. weight colored by gender scatter_hw_gender = alt.Chart(df).mark_point().encode(     alt.X('height:Q', title='Height (cm)'),     alt.Y('weight:Q', title='Weight (kg)'),     alt.Color('gender:N', title='Gender') ).properties(     width=300,     height=200,     title='Scatter: Height vs Weight' )  # Combine the two charts side by side combined_charts = hist_height | scatter_hw_gender combined_charts   In\u00a0[\u00a0]: Copied! <pre>cholesterol_cvd_chart = (\n    alt.Chart(df)\n    .mark_bar()\n    .encode(\n        x=alt.X('cholesterol:O', title='Cholesterol Level'),\n        # Use stack='normalize' on the y-axis to see proportions instead of raw counts\n        y=alt.Y('count()', stack='normalize', title='Proportion of Patients'),\n        color=alt.Color('cardio:N', title='Has CVD?'),\n        tooltip=['count()']  # Optional: see raw counts on hover\n    )\n    .properties(title='Proportion of Cardiovascular Disease by Cholesterol Level')\n)\n\ncholesterol_cvd_chart\n</pre> cholesterol_cvd_chart = (     alt.Chart(df)     .mark_bar()     .encode(         x=alt.X('cholesterol:O', title='Cholesterol Level'),         # Use stack='normalize' on the y-axis to see proportions instead of raw counts         y=alt.Y('count()', stack='normalize', title='Proportion of Patients'),         color=alt.Color('cardio:N', title='Has CVD?'),         tooltip=['count()']  # Optional: see raw counts on hover     )     .properties(title='Proportion of Cardiovascular Disease by Cholesterol Level') )  cholesterol_cvd_chart In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"w06-encoding/mapping_data/#lecture-10-1d-and-data-mapping","title":"Lecture 10: 1D and Data Mapping\u00b6","text":"<p>In this notebook, we will explore how to visualize data in a 1D context and how to map data to basic visual elements. We will start by showing some older plotting methods using matplotlib and pandas, just for context, and then we will move on to Altair.</p> <p>The focus here is on taking a gradual approach to Altair. We'll begin with very simple examples, then we will progressively introduce more advanced features like <code>alt.X()</code> and <code>alt.Y()</code>.</p> <p>Dataset: We'll use the <code>heart_data.csv</code> dataset containing medical examination information. Let\u2019s load it and inspect the first few rows.</p>"},{"location":"w06-encoding/mapping_data/#1-a-quick-look-at-matplotlib-and-pandas","title":"1. A Quick Look at Matplotlib and Pandas\u00b6","text":"<p>Although our main focus is Altair, let's briefly look at some older methods to understand the evolution of plotting libraries in Python.</p>"},{"location":"w06-encoding/mapping_data/#2-introduction-to-altair","title":"2. Introduction to Altair\u00b6","text":"<p>Altair is a declarative statistical visualization library. Here are some key ideas before we start:</p> <ul> <li><p>Data: The data source (typically a pandas DataFrame).</p> </li> <li><p>Mark: The basic graphical shape (like <code>mark_bar()</code>, <code>mark_circle()</code>, etc.).</p> </li> <li><p>Encoding: A mapping between your data columns and visual properties (e.g., color, size, and position).</p> </li> <li><p>Data Types: Altair uses short type codes when encoding data:</p> <ul> <li><p>Q: Quantitative (numeric values, e.g., <code>height</code>, <code>weight</code>).</p> </li> <li><p>T: Temporal (time or date).</p> </li> <li><p>O: Ordinal (data with an order, e.g., a numeric range grouped in bins).</p> </li> <li><p>N: Nominal (categorical data with no intrinsic order, e.g., <code>gender</code>).</p> </li> </ul> </li> </ul> <p>Let\u2019s start very simply. We'll make a bar chart counting how many patients fall under each <code>gender</code>.</p>"},{"location":"w06-encoding/mapping_data/#3-understanding-data-types-in-altair","title":"3. Understanding Data Types in Altair\u00b6","text":"<p>Altair recognizes several types of data and uses these type codes within the encoding channels:</p> <ul> <li><p>Quantitative (Q): Numeric values, such as <code>height</code>, <code>weight</code>, or any continuous measurement.</p> </li> <li><p>Temporal (T): Time or date values, often used for time-series data.</p> </li> <li><p>Ordinal (O): Data with a logical order or ranking (e.g., small &lt; medium &lt; large).</p> </li> <li><p>Nominal (N): Categorical data with no intrinsic order (e.g., <code>gender</code>, <code>cholesterol</code> levels if treated as categories).</p> </li> </ul> <p>By default, Altair often infers these data types, but being explicit can help avoid confusion\u2014especially when customizing how data is displayed.</p> <p>Let\u2019s explore a histogram of the <code>weight</code> column using Altair. Because <code>weight</code> is quantitative (Q), we can <code>bin</code> the values to see their distribution.</p>"},{"location":"w06-encoding/mapping_data/#4-composing-plots","title":"4. Composing plots\u00b6","text":"<p>Altair allows us to combine multiple plots into a single visualization. We can use the <code>|</code> operator to place them side by side or the <code>&amp;</code> operator to stack them vertically.</p> <p>Create a boxplot of height for each gender and combine</p>"},{"location":"w06-encoding/mapping_data/#6-plotting-multiple-variables-scatter-plots","title":"6. Plotting Multiple Variables: Scatter Plots\u00b6","text":"<p>One of the most common ways to look for relationships between two quantitative variables is through a scatter plot. In our dataset, <code>weight</code> and <code>height</code> are both good examples of continuous (Q) variables.</p>"},{"location":"w06-encoding/mapping_data/#61-basic-scatter-plot-of-weight-vs-height","title":"6.1 Basic Scatter Plot of Weight vs. Height\u00b6","text":""},{"location":"w06-encoding/mapping_data/#62-adding-color-to-represent-a-third-variable","title":"6.2 Adding Color to Represent a Third Variable\u00b6","text":"<p>Often, we want to see how a third variable might influence the relationship between two numeric variables. For example, let's color our points by <code>gender</code>.</p> <ul> <li><p><code>gender</code> is categorical, so we use <code>:N</code> or <code>:O</code>.</p> </li> <li><p>We can directly specify <code>alt.Color('gender:N')</code>.</p> </li> </ul>"},{"location":"w06-encoding/mapping_data/#7-putting-it-all-together","title":"7. Putting It All Together\u00b6","text":"<p>Below is a final example combining some of these concepts:</p> <ul> <li><p>Histogram to see the distribution of <code>height</code>.</p> </li> <li><p>Scatter plot of <code>height</code> vs. <code>weight</code> with color by <code>gender</code>.</p> </li> <li><p>We also add some descriptive properties like titles and axis labels.</p> </li> </ul>"},{"location":"w06-encoding/mapping_data/#8-cholesterol-levels-vs-cardiovascular-disease","title":"8. Cholesterol levels vs. Cardiovascular Disease\u00b6","text":"<p>High cholesterol levels have long been associated with an increased risk of heart disease. In this chart, we compare the proportion of cardiovascular disease (<code>cardio</code>) across three categories of cholesterol: Normal, Above Normal, and Well Above Normal.</p> <p>Create descriptive labels for cholesterol categories</p> <p>Stacked bar chart showing the proportion of patients with/without CVD in each cholesterol category</p>"},{"location":"w07-log_scale/assigment_m07_log_scale/","title":"Homework 7 - Log Scale","text":"In\u00a0[2]: Copied! <pre>import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport scipy.stats as ss\n</pre> import matplotlib.pyplot as plt import pandas as pd import seaborn as sns import numpy as np import scipy.stats as ss In\u00a0[2]: Copied! <pre>x = np.array([1,    1,   1,  1, 10, 100, 1000])\ny = np.array([1000, 100, 10, 1, 1,  1,   1   ])\nratio = x/y\nprint(ratio)\n</pre> x = np.array([1,    1,   1,  1, 10, 100, 1000]) y = np.array([1000, 100, 10, 1, 1,  1,   1   ]) ratio = x/y print(ratio) <pre>[1.e-03 1.e-02 1.e-01 1.e+00 1.e+01 1.e+02 1.e+03]\n</pre> <p>Q: Plot on the linear scale using the <code>scatter()</code> function. Also draw a horizontal line at ratio=1 for a reference. The x-axis will be simply the data ID that refers to each ratio data point. Y-axis will be the ratio values.</p> In\u00a0[3]: Copied! <pre>X = np.arange(len(ratio))\n\n# YOUR SOLUTION HERE\n</pre> X = np.arange(len(ratio))  # YOUR SOLUTION HERE Out[3]: <pre>Text(0, 0.5, 'Ratio')</pre> <p>Q: Is this a good visualization of the ratio data? Why? Why not? Explain.</p> <p>Q: Can you fix it?</p> In\u00a0[5]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE In\u00a0[\u00a0]: Copied! <pre>movies = pd.read_json(\"../../Datasets/movies.json\") # adjust the path as needed\n\n# YOUR SOLUTION HERE\n</pre> movies = pd.read_json(\"../../Datasets/movies.json\") # adjust the path as needed  # YOUR SOLUTION HERE <p>If you simply call <code>hist()</code> method with a dataframe object, it identifies all the numeric columns and draw a histogram for each.</p> <p>Q: draw all possible histograms of the movie dataframe. Adjust the size of the plots if needed.</p> In\u00a0[7]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE <p>As we can see, a majority of the columns are not normally distributed. In particular, if you look at the worldwide gross variable, you only see a couple of meaningful data from the histogram. Is this a problem of resolution? How about increasing the number of bins?</p> In\u00a0[\u00a0]: Copied! <pre>ax = movies[\"Worldwide Gross\"].hist(bins=200)\nax.set_xlabel(\"World wide gross\")\nax.set_ylabel(\"Frequency\")\n</pre> ax = movies[\"Worldwide Gross\"].hist(bins=200) ax.set_xlabel(\"World wide gross\") ax.set_ylabel(\"Frequency\") Out[\u00a0]: <pre>Text(0, 0.5, 'Frequency')</pre> <p>Maybe a bit more useful, but it doesn't tell anything about the data distribution above certain point. How about changing the vertical scale to logarithmic scale?</p> In\u00a0[9]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE Out[9]: <pre>Text(0, 0.5, 'Frequency')</pre> <p>Now, let's try log-bin. Recall that when plotting histgrams we can specify the edges of bins through the <code>bins</code> parameter. For example, we can specify the edges of bins to [1, 2, 3, ... , 10] as follows.</p> In\u00a0[\u00a0]: Copied! <pre>movies[\"IMDB Rating\"].hist(bins=range(0,11))\n</pre> movies[\"IMDB Rating\"].hist(bins=range(0,11)) Out[\u00a0]: <pre>&lt;Axes: &gt;</pre> <p>Here, we can specify the edges of bins in a similar way. Instead of specifying on the linear scale, we do it on the log space. Some useful resources:</p> <ul> <li>Google query: python log-bin</li> <li>numpy.logspace</li> <li>numpy.linspace vs numpy.logspace</li> </ul> <p>Hint: since $10^{\\text{start}}= \\text{min(Worldwide Gross)}$, $\\text{start} = \\log_{10}(\\text{min(Worldwide Gross)})$</p> In\u00a0[\u00a0]: Copied! <pre>min(movies[\"Worldwide Gross\"])\n</pre> min(movies[\"Worldwide Gross\"]) Out[\u00a0]: <pre>0.0</pre> <p>Because there seems to be movie(s) that made $0, and because log(0) is undefined &amp; log(1) = 0, let's add 1 to the variable.</p> In\u00a0[\u00a0]: Copied! <pre>movies[\"Worldwide Gross\"] = movies[\"Worldwide Gross\"]+1.0\n</pre> movies[\"Worldwide Gross\"] = movies[\"Worldwide Gross\"]+1.0 In\u00a0[\u00a0]: Copied! <pre># TODO: specify the edges of bins using np.logspace\n# bins = ...\n\n# YOUR SOLUTION HERE\n</pre> # TODO: specify the edges of bins using np.logspace # bins = ...  # YOUR SOLUTION HERE Out[\u00a0]: <pre>array([1.00000000e+00, 3.14018485e+00, 9.86076088e+00, 3.09646119e+01,\n       9.72346052e+01, 3.05334634e+02, 9.58807191e+02, 3.01083182e+03,\n       9.45456845e+03, 2.96890926e+04, 9.32292387e+04, 2.92757043e+05,\n       9.19311230e+05, 2.88680720e+06, 9.06510822e+06, 2.84661155e+07,\n       8.93888645e+07, 2.80697558e+08, 8.81442219e+08, 2.76789150e+09])</pre> <p>Now we can plot a histgram with log-bin. Set both axis to be log-scale.</p> In\u00a0[\u00a0]: Copied! <pre>ax = (movies[\"Worldwide Gross\"]+1.0).hist(bins=bins)\nax.set_yscale('log')\nax.set_xscale('log')\nax.set_xlabel(\"World wide gross\")\nax.set_ylabel(\"Frequency\")\n</pre> ax = (movies[\"Worldwide Gross\"]+1.0).hist(bins=bins) ax.set_yscale('log') ax.set_xscale('log') ax.set_xlabel(\"World wide gross\") ax.set_ylabel(\"Frequency\") Out[\u00a0]: <pre>Text(0, 0.5, 'Frequency')</pre> <p>What is going on? Is this the right plot?</p> <p>Q: explain and fix</p> In\u00a0[16]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE Out[16]: <pre>Text(0, 0.5, 'Probability density')</pre> In\u00a0[\u00a0]: Copied! <pre>gross_sorted = movies[\"Worldwide Gross\"].sort_values()\nN = len(gross_sorted)\nY = np.linspace(1/N, 1, num=N)\nplt.xlabel(\"World wide gross\")\nplt.ylabel(\"Empirical CDF\")\n_ = plt.plot(gross_sorted, Y)\n</pre> gross_sorted = movies[\"Worldwide Gross\"].sort_values() N = len(gross_sorted) Y = np.linspace(1/N, 1, num=N) plt.xlabel(\"World wide gross\") plt.ylabel(\"Empirical CDF\") _ = plt.plot(gross_sorted, Y) <p>Although the movies that are interesting are those with large worldwide gross, we don't see any details about their distribution as they are all close to 1. In other words, CDF sucks at revealing the details of the tail.</p> <p>CCDF is a nice alternative to examine distributions with heavy tails. The idea is same as CDF, but the direction of aggregation is opposite. Because we are starting from the largest value, it can reveal the details of those large values (tail).</p> <p>CCDF is defined as follows:</p> <p>$$ \\bar{F}_X(x) = P(X &gt; x)$$</p> <p>And thus,</p> <p>$$ \\bar{F}_X(x) = P(X &gt; x) = 1 - F_X(x)$$</p> <p>In other words, we can use CDF to calculate CCDF.</p> <p>Q: draw CCDF using the CDF code above.</p> In\u00a0[18]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE <p>How about making the y axis in log scale?</p> In\u00a0[19]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE <p>Although this is technically the correct CCDF plot, there is a very subtle issue. Do you see the vertical line at the rightmost side of the CCDF plot? To understand what's going on, let's look at the Y values of this plot. We used 1 - CDF to calculate CCDF. So,</p> In\u00a0[20]: Copied! <pre>1 - Y\n</pre> 1 - Y Out[20]: <pre>array([9.99556738e-01, 9.99113475e-01, 9.98670213e-01, ...,\n       8.86524823e-04, 4.43262411e-04, 0.00000000e+00])</pre> <p>What happens when we take the log of these values?</p> In\u00a0[21]: Copied! <pre>np.log(1-Y)\n</pre> np.log(1-Y) <pre>/var/folders/d0/wgh1l_5905x4crqpp1b7whz40000gn/T/ipykernel_89007/1767632406.py:1: RuntimeWarning: divide by zero encountered in log\n  np.log(1-Y)\n</pre> Out[21]: <pre>array([-4.43360681e-04, -8.86918018e-04, -1.33067219e-03, ...,\n       -7.02820143e+00, -7.72134861e+00,            -inf])</pre> <p>Because the last value of 1 - Y is 0.0, we got <code>-inf</code> as the log value. That means, the largest value's (let's say $x$) coordinate in our CCDF plot will be $(x, -inf)$ if we use a log scale for our y-axis. And thus we will not be able to see it in the plot. This occurs because we are drawing CDF in a simplified way. In reality, ECDF and ECCDF are step functions and this shouldn't matter. However, because we are drawing a line between the points, we are getting this issue.</p> <p>This is somewhat problematic because the largest value in our dataset can be quite important and therefore we want to see it in the plot!</p> <p>This is why, in practice, we sometimes use \"incorrect\" version of CCDF. We can consider $\\bar{F}_X(x)$ as a \"flipped\" version of CDF.</p> <p>$$ \\bar{F}_X(x) = P(X \\ge x) $$</p> <p>instead of</p> <p>$$ \\bar{F}_X(x) = P(X &gt; x) $$</p> <p>In doing so, we can see the largest value in the data in our CCDF plot. We can also draw the correct version of CCDF, but this quick-and-dirty version is often easier and good enough to show what we want to show.</p> <p>A simple way is just to define the y coordinates as follows:</p> In\u00a0[22]: Copied! <pre>Y = np.linspace( 1.0, 1/N, num=N)\n</pre> Y = np.linspace( 1.0, 1/N, num=N)  <p>Q: Draw a CCDF of worldwide gross data. Use log scale for y-axis.</p> In\u00a0[23]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE <p>A straight line in semilog scale means exponential decay (cf. a straight line in log-log scale means power-law decay). So it seems like the amount of money a movie makes across the world follows roughly an exponential distribution, while there are some outliers that make insane amount of money.</p> <p>Q: Which is the most successful movie in our dataset?</p> <p>You can use the following</p> <ul> <li><code>idxmax()</code>: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.idxmax.html</li> <li><code>loc</code>: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html</li> </ul> In\u00a0[24]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE Out[24]: <pre>Title                                  Avatar\nUS_Gross                          760167650.0\nWorldwide_Gross                  2767891500.0\nUS_DVD_Sales                      146153933.0\nProduction_Budget                 237000000.0\nRelease_Date                      Dec 18 2009\nMPAA_Rating                             PG-13\nRunning_Time_min                          NaN\nDistributor                  20th Century Fox\nSource                    Original Screenplay\nMajor_Genre                            Action\nCreative_Type                 Science Fiction\nDirector                        James Cameron\nRotten_Tomatoes_Rating                   83.0\nIMDB_Rating                               8.3\nIMDB_Votes                           261439.0\nName: 1234, dtype: object</pre> In\u00a0[13]: Copied! <pre>dtypes = { \n    \"Tweet Id\" : \"str\",\n    \"ConversationId\": \"str\",\n}\ndf_ukraine = pd.read_csv('../../Datasets/Ukraine_tweets.csv',dtype=dtypes)\ndf_ukraine.head()\n</pre> dtypes = {      \"Tweet Id\" : \"str\",     \"ConversationId\": \"str\", } df_ukraine = pd.read_csv('../../Datasets/Ukraine_tweets.csv',dtype=dtypes) df_ukraine.head() <pre>/var/folders/jh/xkyk5yn976z_y46xvbg2kjjm0000gn/T/ipykernel_19221/3066181497.py:5: DtypeWarning: Columns (0,2,3,4,5,6,7,11,13,14,15,16,17,18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df_ukraine = pd.read_csv('../../Datasets/Ukraine_tweets.csv',dtype=dtypes)\n</pre> Out[13]: Datetime Tweet Id Text Username Permalink User Outlinks CountLinks ReplyCount RetweetCount LikeCount QuoteCount ConversationId Language Source Media QuotedTweet MentionedUsers hashtag hastag_counts 0 2022-02-24 03:12:47+00:00 1496684505247141897 \ud83c\uddfa\ud83c\udde6 Massive explosions rocks #Kharkiv. #Russia ... IdeologyWars https://twitter.com/IdeologyWars/status/149668... https://twitter.com/IdeologyWars NaN NaN 2.0 17.0 47.0 1.0 1496493071495987201 en &lt;a href=\"https://mobile.twitter.com\" rel=\"nofo... [Photo(previewUrl='https://pbs.twimg.com/media... NaN NaN ['#Kharkiv.', '#Russia', '#Ukraine', '#Ukraine... 9.0 1 2022-02-24 03:18:54+00:00 1496686044275695616 \ud83c\uddfa\ud83c\udde6 Kharkiv is officially being struck by major... IdeologyWars https://twitter.com/IdeologyWars/status/149668... https://twitter.com/IdeologyWars NaN NaN 2.0 104.0 188.0 8.0 1496493071495987201 en &lt;a href=\"https://mobile.twitter.com\" rel=\"nofo... [Video(thumbnailUrl='https://pbs.twimg.com/ext... NaN NaN ['#Russia', '#Ukraine', '#UkraineWar', '#Russi... 8.0 2 2022-02-24 03:22:42+00:00 1496687000375726080 \ud83c\uddfa\ud83c\udde6 More angles on that strike in Kharkiv. #Rus... IdeologyWars https://twitter.com/IdeologyWars/status/149668... https://twitter.com/IdeologyWars NaN NaN 1.0 41.0 75.0 5.0 1496493071495987201 en &lt;a href=\"https://mobile.twitter.com\" rel=\"nofo... [Video(thumbnailUrl='https://pbs.twimg.com/ext... NaN NaN ['#Russia', '#Ukraine', '#UkraineWar', '#Russi... 8.0 3 2022-02-24 03:25:36+00:00 1496687731434565636 \ud83c\uddfa\ud83c\udde6 BM-21 Grad strikes opening on #Mariupol cit... IdeologyWars https://twitter.com/IdeologyWars/status/149668... https://twitter.com/IdeologyWars NaN NaN 21.0 407.0 1099.0 149.0 1496493071495987201 en &lt;a href=\"https://mobile.twitter.com\" rel=\"nofo... [Video(thumbnailUrl='https://pbs.twimg.com/ext... NaN NaN ['#Mariupol', '#Russia', '#Ukraine', '#Ukraine... 9.0 4 2022-02-24 03:27:28+00:00 1496688201242759177 \ud83c\uddfa\ud83c\udde6 Damage caused by strike in Kharkiv... #Russ... IdeologyWars https://twitter.com/IdeologyWars/status/149668... https://twitter.com/IdeologyWars NaN NaN 7.0 152.0 207.0 22.0 1496493071495987201 en &lt;a href=\"https://mobile.twitter.com\" rel=\"nofo... [Video(thumbnailUrl='https://pbs.twimg.com/ext... NaN NaN ['#Russia', '#Ukraine', '#UkraineWar', '#Russi... 8.0 <p>Q: Why do you need to define data types for specific columns?</p> <p><code>YOUR SOLUTION HERE</code></p> <p>Let's visualize the distribution of the number of times a tweet was retweeted.</p> In\u00a0[14]: Copied! <pre># distribution of number of retweets\nplt.figure(figsize=(5, 3))\nretweetCount = df_ukraine['RetweetCount'].dropna() # Remove missing values\nplt.hist(retweetCount, density=True)\nplt.xlabel('Retweet Count')\nplt.ylabel('Density')\nplt.title('Histogram of Retweet Count')\nplt.tight_layout()\nplt.show()\n\n# increase number of bins\n</pre> # distribution of number of retweets plt.figure(figsize=(5, 3)) retweetCount = df_ukraine['RetweetCount'].dropna() # Remove missing values plt.hist(retweetCount, density=True) plt.xlabel('Retweet Count') plt.ylabel('Density') plt.title('Histogram of Retweet Count') plt.tight_layout() plt.show()  # increase number of bins <p>Q: What is the problem with this plot?</p> <p><code>YOUR SOLUTION HERE</code></p> <p>Let's try to use a log-log scale to visualize the distribution of retweet counts.</p> In\u00a0[\u00a0]: Copied! <pre># distribution of number of retweets\nplt.figure(figsize=(5, 3))\nretweetCount = df_ukraine['RetweetCount'].dropna() # Remove missing values\nbins = _ # YOUR SOLUTION HERE: specify the bins using np.logspace! Try a few different number of bins (suggested: ~21)\nplt.hist(retweetCount, density=_, bins=bins) # YOUR SOLUTION HERE. Do we need to specify density or not? Which value?\n# YOUR SOLUTION HERE: specify log scale for x-axis and y-axis\n\n# Don't forget to name the axes and title the plot\nplt.tight_layout()\nplt.show()\n</pre> # distribution of number of retweets plt.figure(figsize=(5, 3)) retweetCount = df_ukraine['RetweetCount'].dropna() # Remove missing values bins = _ # YOUR SOLUTION HERE: specify the bins using np.logspace! Try a few different number of bins (suggested: ~21) plt.hist(retweetCount, density=_, bins=bins) # YOUR SOLUTION HERE. Do we need to specify density or not? Which value? # YOUR SOLUTION HERE: specify log scale for x-axis and y-axis  # Don't forget to name the axes and title the plot plt.tight_layout() plt.show() <p>We can also use a circles instead of bars. This way, we can see the distribution more clearly.</p> In\u00a0[\u00a0]: Copied! <pre># distribution of number of retweets\nplt.figure(figsize=(5, 3))\nbins = _ # YOUR SOLUTION HERE: specify the bins using np.logspace or use those from the previous histogram\nhist, bins = np.histogram(retweetCount, bins=_, density=_) # YOUR SOLUTION HERE \ncenterBins = 0.5 * (bins[1:] + bins[:-1]) # get the center of each bin for plotting\nplt.plot(centerBins, hist, \"o \", ms=4) # Plotting the histogram using the center of bins\n# YOUR SOLUTION HERE: specify log scale for x-axis and y-axis and add labels\nplt.tight_layout()\nplt.show()\n</pre> # distribution of number of retweets plt.figure(figsize=(5, 3)) bins = _ # YOUR SOLUTION HERE: specify the bins using np.logspace or use those from the previous histogram hist, bins = np.histogram(retweetCount, bins=_, density=_) # YOUR SOLUTION HERE  centerBins = 0.5 * (bins[1:] + bins[:-1]) # get the center of each bin for plotting plt.plot(centerBins, hist, \"o \", ms=4) # Plotting the histogram using the center of bins # YOUR SOLUTION HERE: specify log scale for x-axis and y-axis and add labels plt.tight_layout() plt.show()  <p>Q: What do you see now?</p> <p><code>YOUR SOLUTION HERE</code></p> <p>Q: Can you guess what this distribution looks like?</p> <p><code>YOUR SOLUTION HERE</code></p> <p>Q: What are the chracteristics of this distribution type?</p> <p><code>YOUR SOLUTION HERE</code></p> <p>Q: Can you guess why this distribution type may appear in social media metrics such as retweet count?</p> <p><code>YOUR SOLUTION HERE</code></p> <p>Q: Can you give examples where it may appear?</p> <p><code>YOUR SOLUTION HERE</code></p>"},{"location":"w07-log_scale/assigment_m07_log_scale/#homework-7-log-scale","title":"Homework 7 - Log Scale\u00b6","text":"<p>In this homework, we will be using different datasets to explore the use of log scales in visualizations. Logarithmic scales are useful when dealing with data that spans several orders of magnitude and ratio-based comparisons. By the end of this assignment, you will create visualizations using log scales and interpret the results.</p>"},{"location":"w07-log_scale/assigment_m07_log_scale/#instructions","title":"Instructions\u00b6","text":"<ol> <li><p>Project Setup:</p> <ul> <li>Set up your Python and Jupyter (or VSCode) environment.</li> <li>Clone or download the repository provided in class (refer to the class notes).</li> </ul> </li> <li><p>Fill the cells:</p> <ul> <li>Fill in the cells with the code provided in the instructions.</li> <li>You can use the provided code as a starting point and modify it as needed.</li> <li>Make sure to run the code in each cell to see the output.</li> <li>Also respond the markdown questions in the notebook where indicated.</li> </ul> </li> <li><p>Documentation:</p> <ul> <li>Comment your code and add markdown explanations for each part of your analysis.</li> </ul> </li> <li><p>Submission:</p> <ul> <li>Save your notebook and export as either PDF or HTML. If the visualizations using altair are not being shown in the html, submit a separated version with altair html. Refer to: https://altair-viz.github.io/getting_started/starting.html#publishing-your-visualization (you can use the <code>chart.save('chart_file.html')</code> method).</li> <li>Submit to Canvas.</li> </ul> </li> </ol>"},{"location":"w07-log_scale/assigment_m07_log_scale/#ratio-and-logarithm","title":"Ratio and logarithm\u00b6","text":"<p>If you use linear scale to visualize ratios, it can be quite misleading. As learned in the class, ratio values larger than 1 can vary between 1 and infinite, while ratio values smaller than 1 can vary only between 0 and 1. For instance, the ratios of 100:1 (100/1) or 1000:1 (1000/1) are represented as 100 and 1000. The corresponding distances from 1:1 (1) are 99 and 999, respectively. On the other hand, the ratios of 1:100 (1/100) or 1:1000 (1/1000) are represented as 0.01 and 0.001. The corresponding distances from 1:1 (1) are 0.99 and 0.999, respectively. In other words, there is no symmetry between symmetric ratios!</p> <p>To see this clearly, let's first create some ratios.</p>"},{"location":"w07-log_scale/assigment_m07_log_scale/#your-solution-here","title":"YOUR SOLUTION HERE\u00b6","text":""},{"location":"w07-log_scale/assigment_m07_log_scale/#log-binning","title":"Log-binning\u00b6","text":"<p>One way to draw a histogram in log-scale, with a broadly distributed data, is by using log-binning.</p> <p>Let's first see what happens if we do not use the log scale for a dataset with a heavy tail.</p> <p>Q: Load the movie dataset from our Datasets folderand remove the NaN rows based on the following columns: <code>IMDB Rating</code>, <code>IMDB Votes</code>, <code>Worldwide Gross</code>, <code>Rotten Tomatoes Rating</code>.</p>"},{"location":"w07-log_scale/assigment_m07_log_scale/#ccdf","title":"CCDF\u00b6","text":"<p>The cumulative distribution function $F_X(x)$ at $x$ is defined by</p> <p>$$F_X(x) = P(X \\le x),$$</p> <p>which is, in other words, the probability that $X$ takes a value less than or equal to $x$. When empirically calculated (empirical CDF), $F_X(x)$ is the fraction of data points that are less than or equal to $x$. CDF allows us to examine any percentile of the data distribution and is also useful for comparing distributions.</p> <p>However, when the data spans multiple orders of magnitude, CDF may not be useful. Let's try.</p>"},{"location":"w07-log_scale/assigment_m07_log_scale/#part-2-ukraine-russia-conflict-tweets","title":"Part 2: Ukraine-Russia Conflict Tweets\u00b6","text":"<p>Now let's see some social media data. Tweets about the Ukraine-Russia conflict.</p> <p>We load the dataset, define data types for specific columns, and display a sample of the data. The analysis will include Visualizing the distribution of the 'RetweetCount' field.</p>"},{"location":"w07-log_scale/class/","title":"Logarithmic Scale Example","text":"In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate a large number of values (1e7) from a Pareto distribution.\n# np.random.pareto(a, size) returns samples for x &gt;= 0; adding 1 shifts it so that x &gt;= 1.\nN = 10**7\na = 2.0  # shape parameter; the resulting PDF is f(x) = a / x**(a + 1) = 2 / x^3\nsamples = np.random.pareto(a, size=N) + 1\n</pre> import numpy as np import matplotlib.pyplot as plt  # Generate a large number of values (1e7) from a Pareto distribution. # np.random.pareto(a, size) returns samples for x &gt;= 0; adding 1 shifts it so that x &gt;= 1. N = 10**7 a = 2.0  # shape parameter; the resulting PDF is f(x) = a / x**(a + 1) = 2 / x^3 samples = np.random.pareto(a, size=N) + 1  <p>Plot 1: Full-range linear histogram</p> In\u00a0[3]: Copied! <pre>plt.figure(figsize=(8, 6))\nplt.hist(samples, bins=100, density=True, alpha=0.7, color='blue')\nplt.xlabel(\"Value\")\nplt.ylabel(\"Probability Density\")\nplt.title(\"Histogram in Linear Scale (Full Range)\")\nplt.show()\n</pre> plt.figure(figsize=(8, 6)) plt.hist(samples, bins=100, density=True, alpha=0.7, color='blue') plt.xlabel(\"Value\") plt.ylabel(\"Probability Density\") plt.title(\"Histogram in Linear Scale (Full Range)\") plt.show()  <p>Plot 2: Zoomed linear histogram (range 1 to 10)</p> In\u00a0[4]: Copied! <pre>plt.figure(figsize=(8, 6))\nbins = np.linspace(1, 5, 100)\nplt.hist(samples, bins=bins, density=True, alpha=0.7, color='green')\nplt.xlabel(\"Value\")\nplt.ylabel(\"Probability Density\")\nplt.title(\"Histogram in Linear Scale (Zoomed: 1 to 10)\")\nplt.show()\n</pre> plt.figure(figsize=(8, 6)) bins = np.linspace(1, 5, 100) plt.hist(samples, bins=bins, density=True, alpha=0.7, color='green') plt.xlabel(\"Value\") plt.ylabel(\"Probability Density\") plt.title(\"Histogram in Linear Scale (Zoomed: 1 to 10)\") plt.show()  <p>Plot 3: Log-log histogram with logarithmically spaced bins</p> In\u00a0[5]: Copied! <pre>bins = np.logspace(np.log10(1), np.log10(samples.max()), 50)\nplt.figure(figsize=(8, 6))\ny, x = np.histogram(samples, bins=bins, density=True)\nbin_centers = 0.5 * (x[:-1] + x[1:])\nplt.scatter(bin_centers, y, marker='o', color='black')\nplt.xscale('log')\nplt.yscale('log')\nplt.xlabel(\"Value (log scale)\")\nplt.ylabel(\"Probability Density (log scale)\")\nplt.title(\"Histogram in Log-Log Scale with Log Bins\")\nplt.show()\n</pre> bins = np.logspace(np.log10(1), np.log10(samples.max()), 50) plt.figure(figsize=(8, 6)) y, x = np.histogram(samples, bins=bins, density=True) bin_centers = 0.5 * (x[:-1] + x[1:]) plt.scatter(bin_centers, y, marker='o', color='black') plt.xscale('log') plt.yscale('log') plt.xlabel(\"Value (log scale)\") plt.ylabel(\"Probability Density (log scale)\") plt.title(\"Histogram in Log-Log Scale with Log Bins\") plt.show() In\u00a0[7]: Copied! <pre># # CDF\n# Let's try Cumulative Distribution Function (CDF) in log-log scale\n\n# Compute the CDF\nsorted_samples = np.sort(samples)\ncdf = np.arange(1, N + 1) / N  # CDF values\nplt.figure(figsize=(8, 6))\nplt.plot(sorted_samples, cdf, marker='o', linestyle='none', color='red', markersize=2)\nplt.xscale('log')\nplt.yscale('log')\nplt.xlabel(\"Value (log scale)\")\nplt.ylabel(\"Cumulative Probability (log scale)\")\nplt.title(\"CDF in Log-Log Scale\")\nplt.show()\n</pre> # # CDF # Let's try Cumulative Distribution Function (CDF) in log-log scale  # Compute the CDF sorted_samples = np.sort(samples) cdf = np.arange(1, N + 1) / N  # CDF values plt.figure(figsize=(8, 6)) plt.plot(sorted_samples, cdf, marker='o', linestyle='none', color='red', markersize=2) plt.xscale('log') plt.yscale('log') plt.xlabel(\"Value (log scale)\") plt.ylabel(\"Cumulative Probability (log scale)\") plt.title(\"CDF in Log-Log Scale\") plt.show() <p>Was it helpful? Probably not.</p> In\u00a0[\u00a0]: Copied! <pre># ### CCDF\n# Instead, let's plot the Complementary Cumulative Distribution Function (CCDF) in log-log scale, which is more meaningful for power-law distributions.\n\n# Compute the CCDF\n# You can easily compute the CCDF from the CDF by subtracting it from 1.\nccdf = 1 - cdf  # CCDF values\nplt.figure(figsize=(8, 6))\nplt.plot(sorted_samples, ccdf, marker='o', linestyle='none', color='purple', markersize=2)\nplt.xscale('log')\nplt.yscale('log')\nplt.xlabel(\"Value (log scale)\")\nplt.ylabel(\"Complementary Cumulative Probability (log scale)\")\nplt.title(\"CCDF in Log-Log Scale\")\nplt.grid(True)\nplt.show()\n</pre> # ### CCDF # Instead, let's plot the Complementary Cumulative Distribution Function (CCDF) in log-log scale, which is more meaningful for power-law distributions.  # Compute the CCDF # You can easily compute the CCDF from the CDF by subtracting it from 1. ccdf = 1 - cdf  # CCDF values plt.figure(figsize=(8, 6)) plt.plot(sorted_samples, ccdf, marker='o', linestyle='none', color='purple', markersize=2) plt.xscale('log') plt.yscale('log') plt.xlabel(\"Value (log scale)\") plt.ylabel(\"Complementary Cumulative Probability (log scale)\") plt.title(\"CCDF in Log-Log Scale\") plt.grid(True) plt.show() <p>What about now? Can you see the power-law behavior more clearly?</p>"},{"location":"w07-log_scale/class/#logarithmic-scale-example","title":"Logarithmic Scale Example\u00b6","text":"<p>Example of using a logarithmic scale in a plot using Python\u2019s matplotlib library.</p> <p>We will use the Pareto distribution, which follows a power-law behavior:</p> <p>$f(x) = \\frac{\\alpha}{x^{\\alpha + 1}}, \\quad x \\geq 1$</p> <p>NumPy\u2019s <code>np.random.pareto(a)</code> generates samples from a related distribution with support $x \\geq 0$, so we add $1$ to shift the values to match the standard Pareto form with $x \\geq 1$.</p>"},{"location":"w07-log_scale/class/#cdf","title":"CDF\u00b6","text":"<p>Let's try Cumulative Distribution Function (CDF) in log-log scale</p>"},{"location":"w07-log_scale/class/#ccdf","title":"CCDF\u00b6","text":"<p>Instead, let's plot the Complementary Cumulative Distribution Function (CCDF) in log-log scale, which is more meaningful for power-law distributions.</p>"},{"location":"w08-beyond1D_web/assignment_w08_beyond1D/","title":"Homework 8 - Beyond 1D visualizations","text":"<p>Ok let's import the packages</p> In\u00a0[1]: Copied! <pre># For Jupyter notebooks, use 'widget' backend for interactivity\n%matplotlib widget\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport seaborn as sns\n# make text editable in Illustrator\nplt.rcParams['pdf.fonttype'] = 42\nplt.rcParams['ps.fonttype'] = 42\n\n# import altair\nimport altair as alt\nalt.data_transformers.disable_max_rows()\n</pre> # For Jupyter notebooks, use 'widget' backend for interactivity %matplotlib widget  import matplotlib.pyplot as plt import numpy as np import pandas as pd from scipy import stats import seaborn as sns # make text editable in Illustrator plt.rcParams['pdf.fonttype'] = 42 plt.rcParams['ps.fonttype'] = 42  # import altair import altair as alt alt.data_transformers.disable_max_rows()  Out[1]: <pre>DataTransformerRegistry.enable('default')</pre> <p>Now it is time to load a dataset. You can choose your own dataset or use the Cars dataset. If you want to use the Cars dataset, you can load it using the following code:</p> In\u00a0[2]: Copied! <pre># Load ../Datasets/carfeatures.csv\ndf = pd.read_csv(\"../../Datasets/carfeatures.csv\") # adjust the path as needed\ndf.head()\n</pre>  # Load ../Datasets/carfeatures.csv df = pd.read_csv(\"../../Datasets/carfeatures.csv\") # adjust the path as needed df.head() Out[2]: Make Model Year Engine Fuel Type Engine HP Engine Cylinders Transmission Type Driven_Wheels Number of Doors Market Category Vehicle Size Vehicle Style highway MPG city mpg Popularity MSRP 0 BMW 1 Series M 2011 premium unleaded (required) 335.0 6.0 MANUAL rear wheel drive 2.0 Factory Tuner,Luxury,High-Performance Compact Coupe 26 19 3916 46135 1 BMW 1 Series 2011 premium unleaded (required) 300.0 6.0 MANUAL rear wheel drive 2.0 Luxury,Performance Compact Convertible 28 19 3916 40650 2 BMW 1 Series 2011 premium unleaded (required) 300.0 6.0 MANUAL rear wheel drive 2.0 Luxury,High-Performance Compact Coupe 28 20 3916 36350 3 BMW 1 Series 2011 premium unleaded (required) 230.0 6.0 MANUAL rear wheel drive 2.0 Luxury,Performance Compact Coupe 28 18 3916 29450 4 BMW 1 Series 2011 premium unleaded (required) 230.0 6.0 MANUAL rear wheel drive 2.0 Luxury Compact Convertible 28 18 3916 34500 In\u00a0[9]: Copied! <pre># If you are using the car dataset, let's remove a few entries that\n# are problematic for our analysis.\n# The dataset seems to have some issues:\n# Entry with highest Highway MPG\nweirdEntry = df.loc[df['highway MPG'].idxmax()]\nprint(weirdEntry)\n# probably a mistake\n# remove it\ndf_cars = df[df['highway MPG'] &lt; 100]\n# also removing electric cars\ndf_cars = df_cars[df_cars['Engine Fuel Type'] != 'electric']\ndf_cars.head()\n</pre> # If you are using the car dataset, let's remove a few entries that # are problematic for our analysis. # The dataset seems to have some issues: # Entry with highest Highway MPG weirdEntry = df.loc[df['highway MPG'].idxmax()] print(weirdEntry) # probably a mistake # remove it df_cars = df[df['highway MPG'] &lt; 100] # also removing electric cars df_cars = df_cars[df_cars['Engine Fuel Type'] != 'electric'] df_cars.head() <pre>Make                                           Audi\nModel                                            A6\nYear                                           2017\nEngine Fuel Type     premium unleaded (recommended)\nEngine HP                                     252.0\nEngine Cylinders                                4.0\nTransmission Type                  AUTOMATED_MANUAL\nDriven_Wheels                     front wheel drive\nNumber of Doors                                 4.0\nMarket Category                              Luxury\nVehicle Size                                Midsize\nVehicle Style                                 Sedan\nhighway MPG                                     354\ncity mpg                                         24\nPopularity                                     3105\nMSRP                                          51600\nName: 1119, dtype: object\n</pre> Out[9]: Make Model Year Engine Fuel Type Engine HP Engine Cylinders Transmission Type Driven_Wheels Number of Doors Market Category Vehicle Size Vehicle Style highway MPG city mpg Popularity MSRP 0 BMW 1 Series M 2011 premium unleaded (required) 335.0 6.0 MANUAL rear wheel drive 2.0 Factory Tuner,Luxury,High-Performance Compact Coupe 26 19 3916 46135 1 BMW 1 Series 2011 premium unleaded (required) 300.0 6.0 MANUAL rear wheel drive 2.0 Luxury,Performance Compact Convertible 28 19 3916 40650 2 BMW 1 Series 2011 premium unleaded (required) 300.0 6.0 MANUAL rear wheel drive 2.0 Luxury,High-Performance Compact Coupe 28 20 3916 36350 3 BMW 1 Series 2011 premium unleaded (required) 230.0 6.0 MANUAL rear wheel drive 2.0 Luxury,Performance Compact Coupe 28 18 3916 29450 4 BMW 1 Series 2011 premium unleaded (required) 230.0 6.0 MANUAL rear wheel drive 2.0 Luxury Compact Convertible 28 18 3916 34500 In\u00a0[\u00a0]: Copied! <pre># YOUR CODE HERE\n</pre> # YOUR CODE HERE In\u00a0[\u00a0]: Copied! <pre># YOUR CODE HERE\n</pre> # YOUR CODE HERE In\u00a0[6]: Copied! <pre># list of numeric features (get from data)\nnumeric_features = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n# remove Year\nnumeric_features.remove('Year')\nnumeric_features\n\n# Remove any other column you do not want to analyze (like Ids)\n</pre> # list of numeric features (get from data) numeric_features = df.select_dtypes(include=['float64', 'int64']).columns.tolist() # remove Year numeric_features.remove('Year') numeric_features  # Remove any other column you do not want to analyze (like Ids) Out[6]: <pre>['Engine HP',\n 'Engine Cylinders',\n 'Number of Doors',\n 'highway MPG',\n 'city mpg',\n 'Popularity',\n 'MSRP']</pre> In\u00a0[\u00a0]: Copied! <pre># Pearson correlation matrix\n# YOUR CODE HERE\n</pre> # Pearson correlation matrix # YOUR CODE HERE In\u00a0[\u00a0]: Copied! <pre># Spearman correlation matrix\n# YOUR CODE HERE\n</pre> # Spearman correlation matrix # YOUR CODE HERE In\u00a0[7]: Copied! <pre># YOUR CODE HERE\n</pre> # YOUR CODE HERE In\u00a0[8]: Copied! <pre># YOUR CODE HERE\n</pre> # YOUR CODE HERE <p><code>YOUR DISCUSSION HERE</code></p>"},{"location":"w08-beyond1D_web/assignment_w08_beyond1D/#homework-8-beyond-1d-visualizations","title":"Homework 8 - Beyond 1D visualizations\u00b6","text":"<p>In this homework, you will repeat the exploratory analysis from class hands-on but with another dataset. And also provide your own observations of the data.</p> <p>Feel free to choose your own dataset (like the one you are using for the final project), or to keep things simple, you can use the Cars dataset.</p>"},{"location":"w08-beyond1D_web/assignment_w08_beyond1D/#instructions","title":"Instructions\u00b6","text":"<ol> <li><p>Project Setup:</p> <ul> <li>Set up your Python and Jupyter (or VSCode) environment.</li> <li>Clone or download the repository provided in class (refer to the class notes).</li> </ul> </li> <li><p>Fill the cells:</p> <ul> <li>Fill in the cells with the code provided in the instructions.</li> <li>You can use the provided code as a starting point and modify it as needed.</li> <li>Make sure to run the code in each cell to see the output.</li> <li>Also respond the markdown questions in the notebook where indicated.</li> </ul> </li> <li><p>Documentation:</p> <ul> <li>Comment your code and add markdown explanations for each part of your analysis.</li> </ul> </li> <li><p>Submission:</p> <ul> <li>Ensure your notebook is complete and all cells are executed without errors.</li> <li>Save your notebook and export as either PDF or HTML. If the visualizations using altair are not being shown in the html, submit a separated version with altair html. Refer to: https://altair-viz.github.io/getting_started/starting.html#publishing-your-visualization (you can use the <code>chart.save('chart_file.html')</code> method).</li> <li>Submit to Canvas.</li> </ul> </li> </ol>"},{"location":"w08-beyond1D_web/assignment_w08_beyond1D/#2d-scatter-plot","title":"2D Scatter plot\u00b6","text":"<p>Choose two numeric variables from the dataset and plot a scatterplot using either matplotlib, seaborn, or altair.</p> <ul> <li>Ensure to include appropriate labels and a title for the plot.</li> </ul> <p>Try to encode an additional categorical variable (if available) in the scatterplot using color. For instance, for the car dataset, you can use Make or Fuel Type</p>"},{"location":"w08-beyond1D_web/assignment_w08_beyond1D/#3d-scatter-plot","title":"3D Scatter plot\u00b6","text":"<p>Now choose three numeric variables from the dataset and plot a 3D scatterplot. You can use matplotlib's <code>mpl_toolkits.mplot3d</code> or any other library that supports 3D plotting.</p>"},{"location":"w08-beyond1D_web/assignment_w08_beyond1D/#find-the-numeric-variables-in-the-dataset","title":"Find the numeric variables in the dataset\u00b6","text":"<p>Now we will find all the numeric variables in the dataset. This will help us choose the variables for the further analysis.</p>"},{"location":"w08-beyond1D_web/assignment_w08_beyond1D/#correlation-matrix","title":"Correlation Matrix\u00b6","text":"<p>For your selection of numeric variables, plot the correlation matrix using seaborn's <code>heatmap</code> or any other visualization library of your choice.</p> <p>First Pearson, then Spearman correlation. Do you see any differences?</p>"},{"location":"w08-beyond1D_web/assignment_w08_beyond1D/#scatterplot-matrix","title":"Scatterplot matrix\u00b6","text":"<p>To visualize the relationships between all numeric variables, create a scatterplot matrix (also known as a pair plot). You can use seaborn's <code>pairplot</code>.</p>"},{"location":"w08-beyond1D_web/assignment_w08_beyond1D/#parallel-coordinates-plot","title":"Parallel Coordinates Plot\u00b6","text":"<p>Another way to visualize multi-dimensional data is by using a parallel coordinates plot. You can either use pandas or altair for this.</p>"},{"location":"w08-beyond1D_web/assignment_w08_beyond1D/#discussion","title":"Discussion\u00b6","text":"<p>Write a brief discussion on your observations from the visualizations above. Consider the following questions:</p> <ul> <li>Did you gain any insights from the scatter plots (2D and 3D)?</li> <li>Any interesting correlations you found in the correlation matrix?</li> <li>Do you think the scatterplot matrix was helpful in understanding the relationships between variables? Was it too crowded or too sparse?</li> <li>What about the parallel coordinates plot? Did it help in understanding the multi-dimensional relationships? Or was it too messy?</li> </ul>"},{"location":"w08-beyond1D_web/class/","title":"Visualizing Beyond 1D","text":"In\u00a0[1]: Copied! <pre># For Jupyter notebooks, use 'widget' backend for interactivity\n%matplotlib widget\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport seaborn as sns\n# make text editable in Illustrator\nplt.rcParams['pdf.fonttype'] = 42\nplt.rcParams['ps.fonttype'] = 42\n\n# import altair\nimport altair as alt\nalt.data_transformers.disable_max_rows()\n</pre> # For Jupyter notebooks, use 'widget' backend for interactivity %matplotlib widget  import matplotlib.pyplot as plt import numpy as np import pandas as pd from scipy import stats import seaborn as sns # make text editable in Illustrator plt.rcParams['pdf.fonttype'] = 42 plt.rcParams['ps.fonttype'] = 42  # import altair import altair as alt alt.data_transformers.disable_max_rows()  Out[1]: <pre>DataTransformerRegistry.enable('default')</pre> <p>Let's start with the Iris dataset, which is widely used in machine learning and statistics. This dataset contains measurements of different Iris flower species, including:</p> <ul> <li>Sepal Length and Width</li> <li>Petal Length and Width</li> </ul> <p>Let's load the dataset:</p> In\u00a0[2]: Copied! <pre>df_iris = pd.read_csv('../../Datasets/iris.csv') # remember to change the path if needed\ndf_iris.head()\n</pre> df_iris = pd.read_csv('../../Datasets/iris.csv') # remember to change the path if needed df_iris.head() Out[2]: Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species 0 1 5.1 3.5 1.4 0.2 Iris-setosa 1 2 4.9 3.0 1.4 0.2 Iris-setosa 2 3 4.7 3.2 1.3 0.2 Iris-setosa 3 4 4.6 3.1 1.5 0.2 Iris-setosa 4 5 5.0 3.6 1.4 0.2 Iris-setosa <p>Let's first pick up two variables and see how they are related. For this example, we will use the Sepal Length and Sepal Width from the Iris dataset.</p> In\u00a0[3]: Copied! <pre># Let's first pick up two variables and see how they are related. For this example, we will use the Sepal Length and Sepal Width from the Iris dataset.\n# We will use Altair to create a scatter plot of Sepal Length vs Sepal Width.\n# Create a scatter plot using Altair\nscatter_plot = alt.Chart(df_iris).mark_circle(size=60).encode(\n    x=alt.X('SepalLengthCm', scale=alt.Scale(zero=False)), # This ensures that the x-axis does not start from zero\n    y=alt.Y('SepalWidthCm', scale=alt.Scale(zero=False)),\n    color='Species',  # Color by species\n    tooltip=['SepalLengthCm', 'SepalWidthCm', 'Species']  # Add tooltips\n).properties(\n    title='Scatter Plot of Sepal Length vs Sepal Width'\n)\nscatter_plot\n</pre>  # Let's first pick up two variables and see how they are related. For this example, we will use the Sepal Length and Sepal Width from the Iris dataset. # We will use Altair to create a scatter plot of Sepal Length vs Sepal Width. # Create a scatter plot using Altair scatter_plot = alt.Chart(df_iris).mark_circle(size=60).encode(     x=alt.X('SepalLengthCm', scale=alt.Scale(zero=False)), # This ensures that the x-axis does not start from zero     y=alt.Y('SepalWidthCm', scale=alt.Scale(zero=False)),     color='Species',  # Color by species     tooltip=['SepalLengthCm', 'SepalWidthCm', 'Species']  # Add tooltips ).properties(     title='Scatter Plot of Sepal Length vs Sepal Width' ) scatter_plot Out[3]: <p>Now let's pick 3 variables from the Iris dataset and visualize them in a 3D scatter plot using Matplotlib. If you enabled interactive mode, you can rotate the plot to see how the data is distributed in 3D.</p> <p>You can comment the lines that draw the vertical lines in case the plot is too crowded.</p> In\u00a0[4]: Copied! <pre># Let's now do a 3D scatter plot using Matplotlib to visualize the relationship between Sepal Length, Sepal Width, and Petal Length.\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(111, projection='3d')\n# Extract the data for plotting\ncolorsFunction = plt.get_cmap('tab10') # get a colormap for coloring the points\nfor group_index,(group, group_data) in enumerate(df_iris.groupby('Species')):\n        x = group_data['SepalLengthCm']\n        y = group_data['SepalWidthCm']\n        z = group_data['PetalLengthCm']\n        # Plot each group with a different color\n        color = colorsFunction(group_index)\n        ax.scatter(x, y, z, label=group, s=60, color=color) # s is the size of the markers\n        # You can try to add vertical lines from the points to the z-axis to show the height of each point\n        # add vertical lines to show the height of each point\n        # It can be very confusing with many points, so use it carefully\n        for i in range(len(x)):\n            ax.plot([x.iloc[i], x.iloc[i]], [y.iloc[i], y.iloc[i]], [0, z.iloc[i]], color=color, alpha=0.5)\n# Set legend\nax.legend(title='Species', loc='upper left')\n# Set labels\nax.set_xlabel('Sepal Length (cm)')\nax.set_ylabel('Sepal Width (cm)')\nax.set_zlabel('Petal Length (cm)')\n# Set title\nax.set_title('3D Scatter Plot of Sepal Length, Sepal Width, and Petal Length')\nplt.show()\n</pre> # Let's now do a 3D scatter plot using Matplotlib to visualize the relationship between Sepal Length, Sepal Width, and Petal Length. from mpl_toolkits.mplot3d import Axes3D fig = plt.figure(figsize=(5, 5)) ax = fig.add_subplot(111, projection='3d') # Extract the data for plotting colorsFunction = plt.get_cmap('tab10') # get a colormap for coloring the points for group_index,(group, group_data) in enumerate(df_iris.groupby('Species')):         x = group_data['SepalLengthCm']         y = group_data['SepalWidthCm']         z = group_data['PetalLengthCm']         # Plot each group with a different color         color = colorsFunction(group_index)         ax.scatter(x, y, z, label=group, s=60, color=color) # s is the size of the markers         # You can try to add vertical lines from the points to the z-axis to show the height of each point         # add vertical lines to show the height of each point         # It can be very confusing with many points, so use it carefully         for i in range(len(x)):             ax.plot([x.iloc[i], x.iloc[i]], [y.iloc[i], y.iloc[i]], [0, z.iloc[i]], color=color, alpha=0.5) # Set legend ax.legend(title='Species', loc='upper left') # Set labels ax.set_xlabel('Sepal Length (cm)') ax.set_ylabel('Sepal Width (cm)') ax.set_zlabel('Petal Length (cm)') # Set title ax.set_title('3D Scatter Plot of Sepal Length, Sepal Width, and Petal Length') plt.show()                      Figure                  <p>What do you think? Maybe not super clear, and we still need one more dimension we never observed. Instead, let's see a heatmap of the Pearson correlation matrix of the Iris dataset. Let's plot that using Seaborn.</p> In\u00a0[12]: Copied! <pre>numeric_variables = df_iris.select_dtypes(include=np.number)\n# let's remove the 'Id' column if it exists, as it is not a part of the original iris dataset\nif 'Id' in numeric_variables.columns:\n    numeric_variables = numeric_variables.drop(columns=['Id'])\ncorrelation_table = numeric_variables.corr()\ncorrelation_table\n\n\n# Create heatmap of the correlation matrix using seaborn\nplt.figure(figsize=(6, 4))\nsns.heatmap(correlation_table, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n# when plotting a correlation matrix, it is always a good idea to set the color\n# limits (vmin and vmax) to -1 and 1 to ensure the color scale is consistent across\n# the plot. Also use a diverging colormap like 'coolwarm' to show positive and\n# negative correlations clearly.\nplt.title('Correlation Matrix')\nplt.tight_layout()\nplt.show()\n</pre> numeric_variables = df_iris.select_dtypes(include=np.number) # let's remove the 'Id' column if it exists, as it is not a part of the original iris dataset if 'Id' in numeric_variables.columns:     numeric_variables = numeric_variables.drop(columns=['Id']) correlation_table = numeric_variables.corr() correlation_table   # Create heatmap of the correlation matrix using seaborn plt.figure(figsize=(6, 4)) sns.heatmap(correlation_table, annot=True, cmap='coolwarm', vmin=-1, vmax=1) # when plotting a correlation matrix, it is always a good idea to set the color # limits (vmin and vmax) to -1 and 1 to ensure the color scale is consistent across # the plot. Also use a diverging colormap like 'coolwarm' to show positive and # negative correlations clearly. plt.title('Correlation Matrix') plt.tight_layout() plt.show()                      Figure                  <p>We can also take a look at the Spearman correlation matrix, which is a non-parametric measure of correlation. This can be useful when the data does not follow a normal distribution.</p> In\u00a0[6]: Copied! <pre># same but for Spearman\ncorrelation_table = numeric_variables.corr(method='spearman')\n\n# Create heatmap\nplt.figure(figsize=(6, 4))\nsns.heatmap(correlation_table, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('Spearman Corr. Table')\nplt.tight_layout()\nplt.show()\n</pre> # same but for Spearman correlation_table = numeric_variables.corr(method='spearman')  # Create heatmap plt.figure(figsize=(6, 4)) sns.heatmap(correlation_table, annot=True, cmap='coolwarm', vmin=-1, vmax=1) plt.title('Spearman Corr. Table') plt.tight_layout() plt.show()                      Figure                  <p>Ok super cool. So now let's try a scatterplot matrix (also known as a pair plot) to visualize the relationships between all pairs of features in the Iris dataset. This will give us a good overview of how each feature relates to the others.</p> <p>A scatterplot matrix is a grid of scatterplots that allows us to visualize the pairwise relationships between multiple variables in a dataset. It is a powerful tool for identifying correlations, trends, and potential outliers across different dimensions of the data.</p> <p>Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. Seaborn works well with pandas data structures and integrates closely with matplotlib. With seaborn it is easy to create scatterplot matrices.</p> <p>For more examples check out the seaborn documentation: https://seaborn.pydata.org/examples/scatterplot_matrix.html</p> In\u00a0[7]: Copied! <pre># for the iris dataset\ndf_iris_without_id = df_iris.copy()\ndf_iris_without_id = df_iris_without_id.drop(columns=['Id']) # remove 'Id' if it exists\n\nsns.set_theme(font_scale=0.75)  # Set the font scale to reduce the font size\nsns.pairplot(df_iris_without_id, hue='Species', height=1.5, markers='.', plot_kws={'s': 30}) \nplt.show()\n</pre> # for the iris dataset df_iris_without_id = df_iris.copy() df_iris_without_id = df_iris_without_id.drop(columns=['Id']) # remove 'Id' if it exists  sns.set_theme(font_scale=0.75)  # Set the font scale to reduce the font size sns.pairplot(df_iris_without_id, hue='Species', height=1.5, markers='.', plot_kws={'s': 30})  plt.show()                      Figure                  <p>Nice! So now let's try something new, let's visualize a parallel plot using Altair. Parallel coordinates plots are a common way to visualize high-dimensional data. Each feature is represented by a vertical line, and each observation is represented by a polyline that connects the values of each feature.</p> <p>Creating a parallel plot is not straightforward because each feature needs to be reescaled to fit within the same range.</p> In\u00a0[\u00a0]: Copied! <pre>from altair import datum\n# datumm is used to refer to the current datum in Altair's transform_calculate\n# it refers to the current row of data being processed in the Altair chart pipeline.\n\nchart = alt.Chart(df_iris).transform_window(\n    # Create a separated plot group for each species.\n    index='count()'\n).transform_fold(\n    # Fold the numeric columns to long format for min-max normalization\n    ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n).transform_joinaggregate(\n    # Compute the min and max for each key (variable)\n     min='min(value)',\n     max='max(value)',\n     groupby=['key']\n).transform_calculate(\n    # Calculate the min-max normalized value for each point\n    # This will be used to plot the normalized values on the y-axis\n    minmax_value=(datum.value-datum.min)/(datum.max-datum.min),\n    mid=(datum.min+datum.max)/2\n).mark_line().encode(\n    # Use the key as the x-axis, and the normalized value as the y-axis\n    x='key:N',\n    y='minmax_value:Q',\n    color='Species:N',\n    detail='index:N',\n    opacity=alt.value(0.5)\n).properties(width=500)\n\nchart\n</pre> from altair import datum # datumm is used to refer to the current datum in Altair's transform_calculate # it refers to the current row of data being processed in the Altair chart pipeline.  chart = alt.Chart(df_iris).transform_window(     # Create a separated plot group for each species.     index='count()' ).transform_fold(     # Fold the numeric columns to long format for min-max normalization     ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'] ).transform_joinaggregate(     # Compute the min and max for each key (variable)      min='min(value)',      max='max(value)',      groupby=['key'] ).transform_calculate(     # Calculate the min-max normalized value for each point     # This will be used to plot the normalized values on the y-axis     minmax_value=(datum.value-datum.min)/(datum.max-datum.min),     mid=(datum.min+datum.max)/2 ).mark_line().encode(     # Use the key as the x-axis, and the normalized value as the y-axis     x='key:N',     y='minmax_value:Q',     color='Species:N',     detail='index:N',     opacity=alt.value(0.5) ).properties(width=500)  chart  Out[\u00a0]: <p>Pandas offers the <code>parallel_coordinates</code> function to visualize high-dimensional data in a parallel coordinates plot. This function is less flexible than Altair but can be used for quick visualizations. Let's use it to visualize the Iris dataset.</p> In\u00a0[18]: Copied! <pre># pd.plotting.parallel_coordinates\n\ndf_iris_without_id = df_iris.copy()\ndf_iris_without_id = df_iris_without_id.drop(columns=['Id']) # remove 'Id' if it exists\n\nplt.figure(figsize=(6, 4))\npd.plotting.parallel_coordinates(df_iris_without_id, 'Species', colormap=\"tab10\")\nplt.title('Parallel Coordinates Plot for Iris Dataset')\nplt.xlabel('Features')\nplt.ylabel('Normalized Value')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</pre> # pd.plotting.parallel_coordinates  df_iris_without_id = df_iris.copy() df_iris_without_id = df_iris_without_id.drop(columns=['Id']) # remove 'Id' if it exists  plt.figure(figsize=(6, 4)) pd.plotting.parallel_coordinates(df_iris_without_id, 'Species', colormap=\"tab10\") plt.title('Parallel Coordinates Plot for Iris Dataset') plt.xlabel('Features') plt.ylabel('Normalized Value') plt.grid(True) plt.tight_layout() plt.show()                      Figure                  <p>Exporting to PNG, SVG or PDF can be done using the <code>save</code> method in Altair. But you may need to first install the package <code>vl-convert-python</code>.</p> In\u00a0[9]: Copied! <pre>!pip install vl-convert-python\n</pre> !pip install vl-convert-python <pre>Requirement already satisfied: vl-convert-python in /Users/filsilva/miniforge3/envs/dataviz/lib/python3.11/site-packages (1.7.0)\n</pre> <p>Then you can save using:</p> In\u00a0[10]: Copied! <pre># save as pdf, png and svg\nchart.save(\"iris.pdf\")\nchart.save(\"iris.png\")\nchart.save(\"iris.svg\")\n</pre> # save as pdf, png and svg chart.save(\"iris.pdf\") chart.save(\"iris.png\") chart.save(\"iris.svg\") <p>To save plots from matplotlib or seaborn, you can use the <code>savefig</code> method from plt.</p> In\u00a0[11]: Copied! <pre>plt.figure(figsize=(8, 6))\n# Scatter plot for Sepal Length vs Sepal Width\nfor species, group_data in df_iris.groupby('Species'):\n    plt.scatter(group_data['SepalLengthCm'], group_data['SepalWidthCm'], label=species, s=60)\nplt.title('Sepal Length vs Sepal Width by Species')\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Sepal Width (cm)')\nplt.legend(title='Species')\nplt.tight_layout()\n\n# Save the figure in different formats\nplt.savefig('sepal_length_vs_sepal_width.png', dpi=300)  # Save the figure as a PNG file with high resolution (defined by dpi)\nplt.savefig('sepal_length_vs_sepal_width.pdf')  # Save the figure as a PDF file\nplt.show()\n</pre> plt.figure(figsize=(8, 6)) # Scatter plot for Sepal Length vs Sepal Width for species, group_data in df_iris.groupby('Species'):     plt.scatter(group_data['SepalLengthCm'], group_data['SepalWidthCm'], label=species, s=60) plt.title('Sepal Length vs Sepal Width by Species') plt.xlabel('Sepal Length (cm)') plt.ylabel('Sepal Width (cm)') plt.legend(title='Species') plt.tight_layout()  # Save the figure in different formats plt.savefig('sepal_length_vs_sepal_width.png', dpi=300)  # Save the figure as a PNG file with high resolution (defined by dpi) plt.savefig('sepal_length_vs_sepal_width.pdf')  # Save the figure as a PDF file plt.show()                      Figure                  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"w08-beyond1D_web/class/#visualizing-beyond-1d","title":"Visualizing Beyond 1D\u00b6","text":"<p>In this section we will explore how to visualize data in higher dimensions beyond 1D. We will use Python libraries such as Matplotlib, Seaborn and Altair to create 2D and 3D visualizations. Also scatterplot matrices and parallel coordinates plots will be discussed for higher dimensional data.</p>"},{"location":"w09-dim_reduction/assignment_w09_dim_reduction/","title":"Homework 9 - Dimensionality Reduction","text":"<p>Ok let's import the packages</p> In\u00a0[\u00a0]: Copied! <pre># For Jupyter notebooks, use 'widget' backend for interactivity\n%matplotlib widget\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport seaborn as sns\n# make text editable in Illustrator\nplt.rcParams['pdf.fonttype'] = 42\nplt.rcParams['ps.fonttype'] = 42\n\n# import altair\nimport altair as alt\nalt.data_transformers.disable_max_rows()\n</pre> # For Jupyter notebooks, use 'widget' backend for interactivity %matplotlib widget  import matplotlib.pyplot as plt import numpy as np import pandas as pd from scipy import stats import seaborn as sns # make text editable in Illustrator plt.rcParams['pdf.fonttype'] = 42 plt.rcParams['ps.fonttype'] = 42  # import altair import altair as alt alt.data_transformers.disable_max_rows()  <pre>DataTransformerRegistry.enable('default')</pre> <p>Now it is time to load a dataset. You can choose your own dataset or use the Cars dataset. If you want to use the Cars dataset, you can load it using the following code:</p> In\u00a0[\u00a0]: Copied! <pre># Load ../Datasets/carfeatures.csv\ndf = pd.read_csv(\"../../Datasets/carfeatures.csv\") # adjust the path as needed\ndf.head()\n</pre>  # Load ../Datasets/carfeatures.csv df = pd.read_csv(\"../../Datasets/carfeatures.csv\") # adjust the path as needed df.head() Make Model Year Engine Fuel Type Engine HP Engine Cylinders Transmission Type Driven_Wheels Number of Doors Market Category Vehicle Size Vehicle Style highway MPG city mpg Popularity MSRP 0 BMW 1 Series M 2011 premium unleaded (required) 335.0 6.0 MANUAL rear wheel drive 2.0 Factory Tuner,Luxury,High-Performance Compact Coupe 26 19 3916 46135 1 BMW 1 Series 2011 premium unleaded (required) 300.0 6.0 MANUAL rear wheel drive 2.0 Luxury,Performance Compact Convertible 28 19 3916 40650 2 BMW 1 Series 2011 premium unleaded (required) 300.0 6.0 MANUAL rear wheel drive 2.0 Luxury,High-Performance Compact Coupe 28 20 3916 36350 3 BMW 1 Series 2011 premium unleaded (required) 230.0 6.0 MANUAL rear wheel drive 2.0 Luxury,Performance Compact Coupe 28 18 3916 29450 4 BMW 1 Series 2011 premium unleaded (required) 230.0 6.0 MANUAL rear wheel drive 2.0 Luxury Compact Convertible 28 18 3916 34500 In\u00a0[\u00a0]: Copied! <pre># If you are using the car dataset, let's remove a few entries that\n# are problematic for our analysis.\n# The dataset seems to have some issues:\n# Entry with highest Highway MPG\nweirdEntry = df.loc[df['highway MPG'].idxmax()]\nprint(weirdEntry)\n# probably a mistake\n# remove it\ndf_cars = df[df['highway MPG'] &lt; 100]\n# also removing electric cars\ndf_cars = df_cars[df_cars['Engine Fuel Type'] != 'electric']\ndf_cars.head()\n</pre> # If you are using the car dataset, let's remove a few entries that # are problematic for our analysis. # The dataset seems to have some issues: # Entry with highest Highway MPG weirdEntry = df.loc[df['highway MPG'].idxmax()] print(weirdEntry) # probably a mistake # remove it df_cars = df[df['highway MPG'] &lt; 100] # also removing electric cars df_cars = df_cars[df_cars['Engine Fuel Type'] != 'electric'] df_cars.head() <pre>Make                                           Audi\nModel                                            A6\nYear                                           2017\nEngine Fuel Type     premium unleaded (recommended)\nEngine HP                                     252.0\nEngine Cylinders                                4.0\nTransmission Type                  AUTOMATED_MANUAL\nDriven_Wheels                     front wheel drive\nNumber of Doors                                 4.0\nMarket Category                              Luxury\nVehicle Size                                Midsize\nVehicle Style                                 Sedan\nhighway MPG                                     354\ncity mpg                                         24\nPopularity                                     3105\nMSRP                                          51600\nName: 1119, dtype: object\n</pre> Make Model Year Engine Fuel Type Engine HP Engine Cylinders Transmission Type Driven_Wheels Number of Doors Market Category Vehicle Size Vehicle Style highway MPG city mpg Popularity MSRP 0 BMW 1 Series M 2011 premium unleaded (required) 335.0 6.0 MANUAL rear wheel drive 2.0 Factory Tuner,Luxury,High-Performance Compact Coupe 26 19 3916 46135 1 BMW 1 Series 2011 premium unleaded (required) 300.0 6.0 MANUAL rear wheel drive 2.0 Luxury,Performance Compact Convertible 28 19 3916 40650 2 BMW 1 Series 2011 premium unleaded (required) 300.0 6.0 MANUAL rear wheel drive 2.0 Luxury,High-Performance Compact Coupe 28 20 3916 36350 3 BMW 1 Series 2011 premium unleaded (required) 230.0 6.0 MANUAL rear wheel drive 2.0 Luxury,Performance Compact Coupe 28 18 3916 29450 4 BMW 1 Series 2011 premium unleaded (required) 230.0 6.0 MANUAL rear wheel drive 2.0 Luxury Compact Convertible 28 18 3916 34500 In\u00a0[\u00a0]: Copied! <pre># YOUR CODE HERE\n</pre> # YOUR CODE HERE In\u00a0[\u00a0]: Copied! <pre># YOUR CODE HERE\n</pre> # YOUR CODE HERE <p><code>YOUR DISCUSSION HERE</code></p>"},{"location":"w09-dim_reduction/assignment_w09_dim_reduction/#homework-9-dimensionality-reduction","title":"Homework 9 - Dimensionality Reduction\u00b6","text":"<p>In this homework, you will repeat the dimensionality reduction exploration from class hands-on but with another dataset. And also provide your own observations of the data.</p> <p>Feel free to choose your own dataset (like the one you are using for the final project), or to keep things simple, you can use the Cars dataset.</p>"},{"location":"w09-dim_reduction/assignment_w09_dim_reduction/#instructions","title":"Instructions\u00b6","text":"<ol> <li><p>Project Setup:</p> <ul> <li>Set up your Python and Jupyter (or VSCode) environment.</li> <li>Clone or download the repository provided in class (refer to the class notes).</li> </ul> </li> <li><p>Choose a Dataset:</p> <ul> <li>You can use the Cars dataset or any other dataset of your choice.</li> <li>Ensure your dataset has multiple numeric features and at least one categorical feature for coloring.</li> </ul> </li> <li><p>Identify Features that numeric:</p> <ul> <li>Identify which features in your dataset are numeric. You can use the <code>select_dtypes</code> method in pandas to help with this.</li> </ul> </li> <li><p>Choose and apply at least 2 dimensionality reduction techniques:</p> <ul> <li>You can use PCA, t-SNE, UMAP, or any other dimensionality reduction technique you learned about in class or from the literature.</li> <li>Apply these techniques to your dataset and visualize the results.</li> <li>Tell us if you find any interesting patterns or clusters visually in the data.</li> </ul> </li> <li><p>Documentation:</p> <ul> <li>Comment your code and add markdown explanations for each part of your analysis.</li> </ul> </li> <li><p>Submission:</p> <ul> <li>Ensure your notebook is complete and all cells are executed without errors.</li> <li>Save your notebook and export as either PDF or HTML. If the visualizations using altair are not being shown in the html, submit a separated version with altair html. Refer to: https://altair-viz.github.io/getting_started/starting.html#publishing-your-visualization (you can use the <code>chart.save('chart_file.html')</code> method).</li> <li>Submit to Canvas.</li> </ul> </li> </ol>"},{"location":"w09-dim_reduction/assignment_w09_dim_reduction/#dimension-reduction-choice-1","title":"Dimension reduction choice 1\u00b6","text":"<p>Choose one dimension reduction method and apply it to the dataset. Remember that some methods work better if you scale the data first. You can use the <code>StandardScaler</code> from <code>sklearn.preprocessing</code> to scale your data.</p> <p>Create a plot using matplotlib or any other library of your choice to visualize the reduced dimensions. Try to color the points by a categorical variable in your dataset, if available. Alternativelly you can use a continuous variable to color the points.</p>"},{"location":"w09-dim_reduction/assignment_w09_dim_reduction/#dimension-reduction-choice-2","title":"Dimension reduction choice 2\u00b6","text":"<p>Repeat the previous step with a different dimension reduction method.</p>"},{"location":"w09-dim_reduction/assignment_w09_dim_reduction/#discussion","title":"Discussion\u00b6","text":"<p>Discuss briefly the results of the dimension reduction methods you applied. What do you observe? Do the reduced dimensions capture any structure of the data? How do the two methods compare? Are there any interesting patterns or clusters in the data that can be observed visually?</p>"},{"location":"w09-dim_reduction/class/","title":"Dimension Reduction Techniques","text":"In\u00a0[2]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport sklearn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport umap\nfrom pathlib import Path\nimport warnings\n\ndf = pd.read_feather(\"../../Datasets/coil-20-proc.feather\")\n</pre> import pandas as pd import numpy as np import sklearn import matplotlib.pyplot as plt import seaborn as sns import umap from pathlib import Path import warnings  df = pd.read_feather(\"../../Datasets/coil-20-proc.feather\") <p>You should find several features named p# in the dataset, which represent pixel values of the images.</p> In\u00a0[3]: Copied! <pre>df.head()\n</pre> df.head() Out[3]: p0 p1 p2 p3 p4 p5 p6 p7 p8 p9 p10 p11 p12 p13 p14 p15 p16 p17 p18 p19 p20 p21 p22 p23 p24 p25 p26 p27 p28 p29 p30 p31 p32 p33 p34 p35 p36 p37 p38 p39 ... p16347 p16348 p16349 p16350 p16351 p16352 p16353 p16354 p16355 p16356 p16357 p16358 p16359 p16360 p16361 p16362 p16363 p16364 p16365 p16366 p16367 p16368 p16369 p16370 p16371 p16372 p16373 p16374 p16375 p16376 p16377 p16378 p16379 p16380 p16381 p16382 p16383 Filename Object Image 0 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 ... 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 obj1__0.png 1 0 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 ... 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 obj1__1.png 1 1 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 ... 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 obj1__2.png 1 2 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 ... 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 obj1__3.png 1 3 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ... 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 obj1__4.png 1 4 <p>5 rows \u00d7 16387 columns</p> <p>Let's visualize the images first. Each image has 128x128 pixels, and we will reshape them to visualize.</p> In\u00a0[4]: Copied! <pre># Let's take a look at the dataset\n# dimensions of each entry are 128x128 grayscale images\n# columns named p0, p1, ..., p16383 (128*128 = 16384 pixels)\npixelColumns = [f'p{i}' for i in range(16384)]\n\ndef get_image(row):\n    \"\"\"\n    Get the image from a row of the dataframe\n    :param row: a row of the dataframe\n    :return: a 128x128 numpy array\n    \"\"\"\n    # Get the pixel values from the row\n    # Columns starting with p\n    pixels = row[pixelColumns].values\n    # Reshape to 128x128\n    image = pixels.reshape(128, 128)\n    return (image/255).tolist()\n\n# Let's create a matrix of images to visualize the dataset\nnum_objects = df.Object.nunique()\nnum_images = min(10,df.Image.nunique()) # Limit to 10 images per object for visualization purposes\n# Plot a grid of images\n# no spacing between images, just to visualize the dataset\nfig, axes = plt.subplots(num_objects, num_images, figsize=(num_images, num_objects)\n                          , constrained_layout=True)\n# Loop through each object and image to plot\n# use objectIndex and imageIndex\nfor objectIndex, objectId in enumerate(df.Object.unique()):\n    # Get all images for this object\n    object_images = df[df.Object == objectId]\n    for imageIndex in range(min(num_images, len(object_images))):\n        # Get the row corresponding to the image\n        row = object_images.iloc[imageIndex]\n        # Get the image from the row\n        image = get_image(row)\n        # Plot the image in the corresponding subplot\n        axes[objectIndex, imageIndex].imshow(image, cmap='gray')\n        # Hide axes ticks and labels\n        # add labels for the first column with the object ID\n        if imageIndex == 0:\n            # Label the first column with the object ID\n            axes[objectIndex, imageIndex].set_ylabel(f'Object {objectId}', fontsize=10)\n            axes[objectIndex, imageIndex].axis('on')  # Ensure the axis is on for the first column to show the label\n            # but no ticks on the x or y axis\n            axes[objectIndex, imageIndex].set_xticks([])\n            axes[objectIndex, imageIndex].set_yticks([])\n        else:\n            axes[objectIndex, imageIndex].axis('off')\n            axes[objectIndex, imageIndex].set_ylabel('')\nplt.suptitle('Sample Images from the COIL-20 Dataset', fontsize=20)\nplt.show()\n</pre> # Let's take a look at the dataset # dimensions of each entry are 128x128 grayscale images # columns named p0, p1, ..., p16383 (128*128 = 16384 pixels) pixelColumns = [f'p{i}' for i in range(16384)]  def get_image(row):     \"\"\"     Get the image from a row of the dataframe     :param row: a row of the dataframe     :return: a 128x128 numpy array     \"\"\"     # Get the pixel values from the row     # Columns starting with p     pixels = row[pixelColumns].values     # Reshape to 128x128     image = pixels.reshape(128, 128)     return (image/255).tolist()  # Let's create a matrix of images to visualize the dataset num_objects = df.Object.nunique() num_images = min(10,df.Image.nunique()) # Limit to 10 images per object for visualization purposes # Plot a grid of images # no spacing between images, just to visualize the dataset fig, axes = plt.subplots(num_objects, num_images, figsize=(num_images, num_objects)                           , constrained_layout=True) # Loop through each object and image to plot # use objectIndex and imageIndex for objectIndex, objectId in enumerate(df.Object.unique()):     # Get all images for this object     object_images = df[df.Object == objectId]     for imageIndex in range(min(num_images, len(object_images))):         # Get the row corresponding to the image         row = object_images.iloc[imageIndex]         # Get the image from the row         image = get_image(row)         # Plot the image in the corresponding subplot         axes[objectIndex, imageIndex].imshow(image, cmap='gray')         # Hide axes ticks and labels         # add labels for the first column with the object ID         if imageIndex == 0:             # Label the first column with the object ID             axes[objectIndex, imageIndex].set_ylabel(f'Object {objectId}', fontsize=10)             axes[objectIndex, imageIndex].axis('on')  # Ensure the axis is on for the first column to show the label             # but no ticks on the x or y axis             axes[objectIndex, imageIndex].set_xticks([])             axes[objectIndex, imageIndex].set_yticks([])         else:             axes[objectIndex, imageIndex].axis('off')             axes[objectIndex, imageIndex].set_ylabel('') plt.suptitle('Sample Images from the COIL-20 Dataset', fontsize=20) plt.show()  <p>Let's create a version of the data that is easier to input to the models, with just the numeric values for pixels.</p> <p>Also let's create a version that is standardized (<code>X_scaled</code>).</p> In\u00a0[5]: Copied! <pre>from sklearn.preprocessing import StandardScaler\n# standardize\nscaler = StandardScaler()\nX = df[pixelColumns].values  # Get the pixel values as a numpy array\nX_scaled = scaler.fit_transform(X)\n</pre> from sklearn.preprocessing import StandardScaler # standardize scaler = StandardScaler() X = df[pixelColumns].values  # Get the pixel values as a numpy array X_scaled = scaler.fit_transform(X) <p>The first technique we will explore is PCA (Principal Component Analysis). PCA is a linear dimension reduction technique that projects the data onto the directions of maximum variance. You can test it with and without standardization.</p> In\u00a0[6]: Copied! <pre># Apply PCA (standardize and reduce dimensionality)\nfrom sklearn.decomposition import PCA\n\ndimensions = 2\n\n# PCA\npca = PCA(n_components=dimensions)\nX_pca = pca.fit_transform(X_scaled)\n# add back to df named PC1,PC2,PC3...\nfor i in range(dimensions):\n    df[f\"PC{i+1}\"] = X_pca[:,i]\n\n# visualize first 2 components\nplt.figure(figsize=(10, 10))\n# color by Object\ncmap = \"tab20\"\ncmapFunction = plt.get_cmap(cmap)\nfor i,objectName in enumerate(df[\"Object\"].unique()):\n    color = cmapFunction(i)\n    groupData = df[df[\"Object\"] == objectName]\n    plt.scatter(groupData[\"PC1\"], groupData[\"PC2\"], label=objectName, color=color)\n# add legend\nplt.legend()\n# calculate the explained variance ratio\nexplained_variance = pca.explained_variance_ratio_\nplt.xlabel(f\"PC1 ({explained_variance[0]:.2%})\")\nplt.ylabel(f\"PC2 ({explained_variance[1]:.2%})\")\n\n\nplt.title(\"PCA\")\nplt.show()\n</pre> # Apply PCA (standardize and reduce dimensionality) from sklearn.decomposition import PCA  dimensions = 2  # PCA pca = PCA(n_components=dimensions) X_pca = pca.fit_transform(X_scaled) # add back to df named PC1,PC2,PC3... for i in range(dimensions):     df[f\"PC{i+1}\"] = X_pca[:,i]  # visualize first 2 components plt.figure(figsize=(10, 10)) # color by Object cmap = \"tab20\" cmapFunction = plt.get_cmap(cmap) for i,objectName in enumerate(df[\"Object\"].unique()):     color = cmapFunction(i)     groupData = df[df[\"Object\"] == objectName]     plt.scatter(groupData[\"PC1\"], groupData[\"PC2\"], label=objectName, color=color) # add legend plt.legend() # calculate the explained variance ratio explained_variance = pca.explained_variance_ratio_ plt.xlabel(f\"PC1 ({explained_variance[0]:.2%})\") plt.ylabel(f\"PC2 ({explained_variance[1]:.2%})\")   plt.title(\"PCA\") plt.show()  <p>Now let's try to visualize the dataset using MDS (Multidimensional Scaling). MDS is a non-linear technique that tries to preserve the pairwise distances between points in the high-dimensional space when mapping to a lower-dimensional space. Try to use different dissimilarity metrics (e.g., euclidean, cosine) to see how it affects the results.</p> In\u00a0[\u00a0]: Copied! <pre># MDS\nfrom sklearn.manifold import MDS\n\nX_input = X_scaled\n\n# You will see it takes a long time to run MDS on the full dataset.\n# We can reduce the dimensionality first with PCA or use a smaller dataset for MDS.\n# you can uncomment the following lines to reduce the dimensionality first with PCA before applying MDS.\n\n# pcaDimensions = 50\n# pca = PCA(n_components=pcaDimensions)\n# X_input = pca.fit_transform(X_input)\n\n\nmds = MDS(n_components=dimensions,\n          dissimilarity='euclidean')\n# Fit MDS to the pairwise data\nX_mds = mds.fit_transform(X_input)\n# add back to df named MDS1,MDS2,MDS3...\nfor i in range(dimensions):\n    df[f\"MDS{i+1}\"] = X_mds[:,i]\n# visualize first 2 components\nplt.figure(figsize=(10, 10))\n# color by Object\ncmap = \"tab20\"\ncmapFunction = plt.get_cmap(cmap)\nfor i,objectName in enumerate(df[\"Object\"].unique()):\n    color = cmapFunction(i)\n    groupData = df[df[\"Object\"] == objectName]\n    plt.scatter(groupData[\"MDS1\"], groupData[\"MDS2\"], label=objectName, color=color)\n# add legend\nplt.legend()\nplt.xlabel(\"MDS1\")\nplt.ylabel(\"MDS2\")\nplt.title(\"MDS\")\nplt.show()\n</pre> # MDS from sklearn.manifold import MDS  X_input = X_scaled  # You will see it takes a long time to run MDS on the full dataset. # We can reduce the dimensionality first with PCA or use a smaller dataset for MDS. # you can uncomment the following lines to reduce the dimensionality first with PCA before applying MDS.  # pcaDimensions = 50 # pca = PCA(n_components=pcaDimensions) # X_input = pca.fit_transform(X_input)   mds = MDS(n_components=dimensions,           dissimilarity='euclidean') # Fit MDS to the pairwise data X_mds = mds.fit_transform(X_input) # add back to df named MDS1,MDS2,MDS3... for i in range(dimensions):     df[f\"MDS{i+1}\"] = X_mds[:,i] # visualize first 2 components plt.figure(figsize=(10, 10)) # color by Object cmap = \"tab20\" cmapFunction = plt.get_cmap(cmap) for i,objectName in enumerate(df[\"Object\"].unique()):     color = cmapFunction(i)     groupData = df[df[\"Object\"] == objectName]     plt.scatter(groupData[\"MDS1\"], groupData[\"MDS2\"], label=objectName, color=color) # add legend plt.legend() plt.xlabel(\"MDS1\") plt.ylabel(\"MDS2\") plt.title(\"MDS\") plt.show()  <p>The next technique we will explore is Isomap. Isomap is a non-linear dimension reduction technique that extends MDS by incorporating geodesic distances on a nearest-neighbor graph. This allows it to capture the underlying manifold structure of the data.</p> In\u00a0[8]: Copied! <pre># Now try Isomap\nfrom sklearn.manifold import Isomap\n\ndimensions = 2\n\nX_input = X_scaled\n\n# Isomap\nisomap = Isomap(n_neighbors=5,\n                n_components=dimensions)\n\n# disable warnings for this part:\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")  # ignore warnings from Isomap\n    # fit and transform\n    X_isomap = isomap.fit_transform(X_input)\n\n# add back to df named Isomap1,Isomap2,Isomap3...\nfor i in range(dimensions):\n    df[f\"Isomap{i+1}\"] = X_isomap[:,i]\n\n# visualize first 2 components\n\nplt.figure(figsize=(10, 10))\n# color by Object\n\ncmap = \"tab20\"\n\ncmapFunction = plt.get_cmap(cmap)\n\nfor i,objectName in enumerate(df[\"Object\"].unique()):\n    color = cmapFunction(i)\n    groupData = df[df[\"Object\"] == objectName]\n    plt.scatter(groupData[\"Isomap1\"], groupData[\"Isomap2\"], label=objectName, color=color)\n# add legend\nplt.legend()\nplt.xlabel(\"Isomap1\")\nplt.ylabel(\"Isomap2\")\nplt.title(\"Isomap\")\nplt.show()\n</pre> # Now try Isomap from sklearn.manifold import Isomap  dimensions = 2  X_input = X_scaled  # Isomap isomap = Isomap(n_neighbors=5,                 n_components=dimensions)  # disable warnings for this part: with warnings.catch_warnings():     warnings.simplefilter(\"ignore\")  # ignore warnings from Isomap     # fit and transform     X_isomap = isomap.fit_transform(X_input)  # add back to df named Isomap1,Isomap2,Isomap3... for i in range(dimensions):     df[f\"Isomap{i+1}\"] = X_isomap[:,i]  # visualize first 2 components  plt.figure(figsize=(10, 10)) # color by Object  cmap = \"tab20\"  cmapFunction = plt.get_cmap(cmap)  for i,objectName in enumerate(df[\"Object\"].unique()):     color = cmapFunction(i)     groupData = df[df[\"Object\"] == objectName]     plt.scatter(groupData[\"Isomap1\"], groupData[\"Isomap2\"], label=objectName, color=color) # add legend plt.legend() plt.xlabel(\"Isomap1\") plt.ylabel(\"Isomap2\") plt.title(\"Isomap\") plt.show() <p>Finally, we will explore UMAP (Uniform Manifold Approximation and Projection). UMAP is a non-linear dimension reduction technique that is particularly effective for visualizing complex data structures. It is based on manifold learning and can be thought of as a generalization of t-SNE.</p> In\u00a0[9]: Copied! <pre># repeat same for umap\nreducer = umap.UMAP(n_components=dimensions,\n                    # metric=\"euclidean\",\n                    # n_epochs=1000,\n                    # n_neighbors=15,\n                    # min_dist=0.1,\n                    # negative_sample_rate=5,\n                    )\nX_umap = reducer.fit_transform(X_scaled)\n\n# add back to df named UMAP1,UMAP2,UMAP3...\nfor i in range(dimensions):\n    df[f\"UMAP{i+1}\"] = X_umap[:,i]\n\n# visualize first 2 components\n\nplt.figure(figsize=(10, 10))\n# color by Object\ncmap = \"tab20\"\n\ncmapFunction = plt.get_cmap(cmap)\n\nfor i,objectName in enumerate(df[\"Object\"].unique()):\n    color = cmapFunction(i)\n    groupData = df[df[\"Object\"] == objectName]\n    plt.scatter(groupData[\"UMAP1\"], groupData[\"UMAP2\"], label=objectName, color=color)\n    # place a label on each group centroid\n    # calculate centroid\n    centroid_x = groupData[\"UMAP1\"].mean()\n    centroid_y = groupData[\"UMAP2\"].mean()\n    # place label at centroid\n    plt.text(centroid_x, centroid_y, objectName,\n             fontsize=9,\n             ha='center', va='center',\n             color='black', alpha=1.0,\n             zorder=5, # to ensure text is above the scatter points\n    )\n# add legend\nplt.legend()\n# Do not label UMAP axes.\n# disable axes labels for UMAP, as they do not have a direct interpretation like PCA\nplt.axis('off') \nplt.title(\"UMAP\")\n\nplt.show()\n</pre> # repeat same for umap reducer = umap.UMAP(n_components=dimensions,                     # metric=\"euclidean\",                     # n_epochs=1000,                     # n_neighbors=15,                     # min_dist=0.1,                     # negative_sample_rate=5,                     ) X_umap = reducer.fit_transform(X_scaled)  # add back to df named UMAP1,UMAP2,UMAP3... for i in range(dimensions):     df[f\"UMAP{i+1}\"] = X_umap[:,i]  # visualize first 2 components  plt.figure(figsize=(10, 10)) # color by Object cmap = \"tab20\"  cmapFunction = plt.get_cmap(cmap)  for i,objectName in enumerate(df[\"Object\"].unique()):     color = cmapFunction(i)     groupData = df[df[\"Object\"] == objectName]     plt.scatter(groupData[\"UMAP1\"], groupData[\"UMAP2\"], label=objectName, color=color)     # place a label on each group centroid     # calculate centroid     centroid_x = groupData[\"UMAP1\"].mean()     centroid_y = groupData[\"UMAP2\"].mean()     # place label at centroid     plt.text(centroid_x, centroid_y, objectName,              fontsize=9,              ha='center', va='center',              color='black', alpha=1.0,              zorder=5, # to ensure text is above the scatter points     ) # add legend plt.legend() # Do not label UMAP axes. # disable axes labels for UMAP, as they do not have a direct interpretation like PCA plt.axis('off')  plt.title(\"UMAP\")  plt.show()  <pre>/Users/filsilva/miniforge3/envs/dataviz/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n</pre>"},{"location":"w09-dim_reduction/class/#dimension-reduction-techniques","title":"Dimension Reduction Techniques\u00b6","text":"<p>In this hands-on notebook, we will explore various dimension reduction techniques using Python. Dimension reduction is powerful way to visualize high-dimensional data. We will cover the following techniques: PCA, MDS, Isomap, and UMAP.</p> <p>Let's first import some packages and load the COIL-20 dataset, which is a popular dataset for testing dimension reduction techniques. The COIL-20 dataset contains images of 20 different objects, each captured at 72 different angles.</p>"},{"location":"w10-maps/assignment_w10_maps/","title":"Homework 10 - Maps (optional)","text":""},{"location":"w10-maps/assignment_w10_maps/#homework-10-maps-optional","title":"Homework 10 - Maps (optional)\u00b6","text":"<p>In this homework, we want to see a map visualization. Feel free to use any dataset you like. We recommend the use of Altair.</p>"},{"location":"w10-maps/assignment_w10_maps/#instructions","title":"Instructions\u00b6","text":"<ol> <li><p>Project Setup:</p> <ul> <li>Set up your Python and Jupyter (or VSCode) environment.</li> <li>Clone or download the repository provided in class (refer to the class notes).</li> </ul> </li> <li><p>Choose a Dataset:</p> <ul> <li>You can use any dataset with geographic data. Even a dataset you have used for your project is fine.</li> </ul> </li> <li><p>Identify The Geolocated Features:</p> <ul> <li>Identify which features can be used for geolocation in your dataset. Examples include latitude and longitude, city names, state names, country names, or any other geographic identifiers.</li> </ul> </li> <li><p>Choose a map visualization approach:</p> <ul> <li>Select a map visualization based on the methods explored in class or in the previous materials.</li> <li>We recommend using Altair for its simplicity and effectiveness in creating interactive visualizations. But you are free to use other libraries.</li> </ul> </li> <li><p>Documentation:</p> <ul> <li>Comment your code and add markdown explanations for each part of your analysis.</li> </ul> </li> <li><p>Example datasets</p> </li> </ol> <ul> <li><p>Global Earthquakes - USGS (CSV feeds) https://earthquake.usgs.gov/earthquakes/feed/v1.0/csv.php</p> </li> <li><p>Airbnb Listings - Inside Airbnb (CSV per city) http://insideairbnb.com/get-the-data/</p> </li> <li><p>Meteorite Landings - NASA Open Data (CSV) https://www.kaggle.com/datasets/nasa/meteorite-landings</p> </li> <li><p>US County Population (2018 estimates) - Census Bureau (CSV) https://www2.census.gov/programs-surveys/popest/datasets/2010-2018/counties/totals/co-est2018-alldata.csv</p> </li> <li><p>World Happiness Report (by country) (CSV) https://worldhappiness.report/data-sharing/</p> </li> </ul> <ol> <li>Submission:<ul> <li>Ensure your notebook is complete and all cells are executed without errors.</li> <li>Save your notebook and export as either PDF or HTML. If the visualizations using altair are not being shown in the html, submit a separated version with altair html. Refer to: https://altair-viz.github.io/getting_started/starting.html#publishing-your-visualization (you can use the <code>chart.save('chart_file.html')</code> method).</li> <li>Submit to Canvas.</li> </ul> </li> </ol>"},{"location":"w10-maps/class/","title":"Visualizing Maps","text":"In\u00a0[1]: Copied! <pre>import altair as alt\n\n# saving data into a file rather than embedding into the chart\nalt.data_transformers.enable('json')\n</pre> import altair as alt  # saving data into a file rather than embedding into the chart alt.data_transformers.enable('json') Out[1]: <pre>DataTransformerRegistry.enable('json')</pre> <p>We need a dataset with geographical coordinates. This <code>zipcodes</code> dataset contains the location and zipcode of each zip code area.</p> In\u00a0[3]: Copied! <pre>from vega_datasets import data\n\nzipcodes_url = data.zipcodes.url\nzipcodes = data.zipcodes()\nzipcodes.head()\n</pre> from vega_datasets import data  zipcodes_url = data.zipcodes.url zipcodes = data.zipcodes() zipcodes.head() Out[3]: zip_code latitude longitude city state county 0 00501 40.922326 -72.637078 Holtsville NY Suffolk 1 00544 40.922326 -72.637078 Holtsville NY Suffolk 2 00601 18.165273 -66.722583 Adjuntas PR Adjuntas 3 00602 18.393103 -67.180953 Aguada PR Aguada 4 00603 18.455913 -67.145780 Aguadilla PR Aguadilla In\u00a0[4]: Copied! <pre>zipcodes = data.zipcodes(dtype={'zip_code': 'category'})\nzipcodes.head()\n</pre> zipcodes = data.zipcodes(dtype={'zip_code': 'category'}) zipcodes.head() Out[4]: zip_code latitude longitude city state county 0 00501 40.922326 -72.637078 Holtsville NY Suffolk 1 00544 40.922326 -72.637078 Holtsville NY Suffolk 2 00601 18.165273 -66.722583 Adjuntas PR Adjuntas 3 00602 18.393103 -67.180953 Aguada PR Aguada 4 00603 18.455913 -67.145780 Aguadilla PR Aguadilla In\u00a0[5]: Copied! <pre>zipcodes.zip_code.dtype\n</pre> zipcodes.zip_code.dtype Out[5]: <pre>CategoricalDtype(categories=['00501', '00544', '00601', '00602', '00603', '00604',\n                  '00605', '00606', '00610', '00611',\n                  ...\n                  '99919', '99921', '99922', '99923', '99925', '99926',\n                  '99927', '99928', '99929', '99950'],\n, ordered=False, categories_dtype=object)</pre> <p>Btw, you'll have fewer issues if you pass URL instead of a dataframe to <code>alt.Chart</code>.</p> In\u00a0[6]: Copied! <pre>alt.Chart(zipcodes_url).mark_circle().encode(\n    x='longitude:Q',\n    y='latitude:Q',\n)\n</pre> alt.Chart(zipcodes_url).mark_circle().encode(     x='longitude:Q',     y='latitude:Q', ) Out[6]: <p>Actually this itself is a map projection called Equirectangular projection. This projection (or almost a non-projection) is super straight-forward and doesn't require any processing of the data. So, often it is used to just quickly explore geographical data. As you dig deeper, you still want to think about which map projection fits your need best. Don't just use equirectangular projection without any thoughts!</p> <p>Anyway, let's make it look slighly better by reducing the size of the circles and adjusting the aspect ratio. Q: Can you adjust the width and height?</p> In\u00a0[\u00a0]: Copied! <pre>alt.Chart(zipcodes_url).mark_circle(\n    size=0.5,\n).encode(\n    x='longitude:Q',\n    y='latitude:Q',\n    tooltip=['zip_code:N', 'city:N', 'state:N']\n).properties(\n    width=800,\n    height=300\n).interactive()\n</pre> alt.Chart(zipcodes_url).mark_circle(     size=0.5, ).encode(     x='longitude:Q',     y='latitude:Q',     tooltip=['zip_code:N', 'city:N', 'state:N'] ).properties(     width=800,     height=300 ).interactive() Out[\u00a0]: <p>But, a much better way to do this is explicitly specifying that they are lat, lng coordinates by using <code>longitude=</code> and <code>latitude=</code>, rather than <code>x=</code> and <code>y=</code>. If you do that, altair automatically adjust the aspect ratio. Q: Can you try it?</p> In\u00a0[57]: Copied! <pre>alt.Chart(zipcodes_url).mark_circle(\n    size=0.5,\n).encode(\n    longitude='longitude:Q',\n    latitude='latitude:Q',\n    tooltip=['zip_code:N', 'city:N', 'state:N']\n).properties(\n    width=800,\n    height=300\n)\n</pre> alt.Chart(zipcodes_url).mark_circle(     size=0.5, ).encode(     longitude='longitude:Q',     latitude='latitude:Q',     tooltip=['zip_code:N', 'city:N', 'state:N'] ).properties(     width=800,     height=300 ) Out[57]: <p>Because the American empire is far-reaching and complicated, the information density of this map is very low (although interesting). Moreover, the US looks twisted because a default projection that is not focused on the US is used.</p> <p>A common projection for visualizing US data is AlbersUSA, which uses Albers (equal-area) projection. This is a standard projection used in United States Geological Survey and the United States Census Bureau. Albers USA contains a composition of US main land, Alaska, and Hawaii.</p> <p>To use it, we  call <code>project</code> method and specify which variables are <code>longitude</code> and <code>latitude</code>.</p> <p>Q: use the <code>project</code> method to draw the map in the AlbersUsa projection.</p> In\u00a0[59]: Copied! <pre>alt.Chart(zipcodes_url).mark_circle(\n    size=2,\n).encode(\n    longitude='longitude:Q',\n    latitude='latitude:Q',\n    tooltip=['zip_code:N', 'city:N', 'state:N']\n).properties(\n    width=800,\n    height=300\n).project(\n    type='albersUsa'\n    # Supported projections:\n    # 'albers', 'albersUsa', 'azimuthalEqualArea', 'azimuthalEquidistant',\n    # 'conicConformal', 'conicEqualArea', 'conicEquidistant', 'equalEarth', 'equirectangular', 'gnomonic',\n    # 'identity', 'mercator', 'naturalEarth1', 'orthographic', 'stereographic', 'transverseMercator'\n).interactive()\n</pre> alt.Chart(zipcodes_url).mark_circle(     size=2, ).encode(     longitude='longitude:Q',     latitude='latitude:Q',     tooltip=['zip_code:N', 'city:N', 'state:N'] ).properties(     width=800,     height=300 ).project(     type='albersUsa'     # Supported projections:     # 'albers', 'albersUsa', 'azimuthalEqualArea', 'azimuthalEquidistant',     # 'conicConformal', 'conicEqualArea', 'conicEquidistant', 'equalEarth', 'equirectangular', 'gnomonic',     # 'identity', 'mercator', 'naturalEarth1', 'orthographic', 'stereographic', 'transverseMercator' ).interactive() Out[59]: <p>Now we're talking. \ud83d\ude0e</p> <p>Let's visualize the large-scale zipcode patterns. We can use the fact that the zipcodes are hierarchically organized. That is, the first digit captures the largest area divisions and the other digits are about smaller geographical divisions.</p> <p>Altair provides some data transformation functionalities. One of them is extracting a substring from a variable.</p> In\u00a0[22]: Copied! <pre>from altair import datum, expr\n\nalt.Chart(zipcodes_url).mark_circle(size=2).transform_calculate(\n    'first_digit', expr.substring(datum.zip_code, 0, 1)\n).encode(\n    longitude='longitude:Q',\n    latitude='latitude:Q',\n    color='first_digit:N',\n).project(\n    type='albersUsa'\n).properties(\n    width=700,\n    height=400,\n)\n</pre> from altair import datum, expr  alt.Chart(zipcodes_url).mark_circle(size=2).transform_calculate(     'first_digit', expr.substring(datum.zip_code, 0, 1) ).encode(     longitude='longitude:Q',     latitude='latitude:Q',     color='first_digit:N', ).project(     type='albersUsa' ).properties(     width=700,     height=400, ) Out[22]: <p>For each row (<code>datum</code>), you obtain the <code>zip_code</code> variable and get the substring (imagine Python list slicing), and then you call the result <code>first_digit</code>. Now, you can use this <code>first_digit</code> variable to color the circles. Also note that we specify <code>first_digit</code> as a nominal variable, not quantitative, to obtain a categorical colormap. But we can also play with it too.</p> <p>Q: Why don't you extract the first two digits, name it as <code>two_digits</code>, and declare that as a quantitative variable? Any interesting patterns? What does it tell us about the history of US?</p> In\u00a0[60]: Copied! <pre>alt.Chart(zipcodes_url).mark_circle(size=2).transform_calculate(\n    'first_two_digits', expr.substring(datum.zip_code, 0, 2)\n).encode(\n    longitude='longitude:Q',\n    latitude='latitude:Q',\n    color=alt.Color('first_two_digits:Q', scale=alt.Scale(scheme='viridis')),\n).project(\n    type='albersUsa'\n).properties(\n    width=700,\n    height=400,\n)\n</pre> alt.Chart(zipcodes_url).mark_circle(size=2).transform_calculate(     'first_two_digits', expr.substring(datum.zip_code, 0, 2) ).encode(     longitude='longitude:Q',     latitude='latitude:Q',     color=alt.Color('first_two_digits:Q', scale=alt.Scale(scheme='viridis')), ).project(     type='albersUsa' ).properties(     width=700,     height=400, ) Out[60]: <p>Q: also try it with declaring the first two digits as a categorical variable</p> In\u00a0[11]: Copied! <pre>zipcodes\n</pre> zipcodes Out[11]: zip_code latitude longitude city state county 0 00501 40.922326 -72.637078 Holtsville NY Suffolk 1 00544 40.922326 -72.637078 Holtsville NY Suffolk 2 00601 18.165273 -66.722583 Adjuntas PR Adjuntas 3 00602 18.393103 -67.180953 Aguada PR Aguada 4 00603 18.455913 -67.145780 Aguadilla PR Aguadilla ... ... ... ... ... ... ... 42044 99926 55.094325 -131.566827 Metlakatla AK Prince Wales Ketchikan 42045 99927 55.517921 -132.003244 Point Baker AK Prince Wales Ketchikan 42046 99928 55.395359 -131.675370 Ward Cove AK Ketchikan Gateway 42047 99929 56.449893 -132.364407 Wrangell AK Wrangell Petersburg 42048 99950 55.542007 -131.432682 Ketchikan AK Ketchikan Gateway <p>42049 rows \u00d7 6 columns</p> In\u00a0[\u00a0]: Copied! <pre>alt.Chart(zipcodes_url).mark_circle(size=2).transform_calculate(\n    'first_two_digits', expr.substring(datum.zip_code, 0, 2)\n).encode(\n    longitude='longitude:Q',\n    latitude='latitude:Q',\n    color='first_two_digits:N',  # note the change here\n).project(\n    type='albersUsa'\n).properties(\n    width=700,\n    height=400,\n)\n</pre> alt.Chart(zipcodes_url).mark_circle(size=2).transform_calculate(     'first_two_digits', expr.substring(datum.zip_code, 0, 2) ).encode(     longitude='longitude:Q',     latitude='latitude:Q',     color='first_two_digits:N',  # note the change here ).project(     type='albersUsa' ).properties(     width=700,     height=400, ) Out[\u00a0]: <p>Btw, you can always click \"view source\" or \"open in Vega Editor\" to look at the json object that defines this visualization. You can embed this json object on your webpage and easily put up an interactive visualization.</p> <p>Q: Can you put a tooltip that displays the zipcode when you mouse-over?</p> In\u00a0[\u00a0]: Copied! <pre>alt.Chart(zipcodes_url).mark_circle(size=2).transform_calculate(\n    'first_two_digits', expr.substring(datum.zip_code, 0, 2)\n).encode(\n    longitude='longitude:Q',\n    latitude='latitude:Q',\n    color='first_two_digits:N',\n    tooltip=['zip_code:N', 'city:N', 'state:N']  # added tooltip\n).project(\n    type='albersUsa'\n).properties(\n    width=700,\n    height=400,\n)\n</pre>  alt.Chart(zipcodes_url).mark_circle(size=2).transform_calculate(     'first_two_digits', expr.substring(datum.zip_code, 0, 2) ).encode(     longitude='longitude:Q',     latitude='latitude:Q',     color='first_two_digits:N',     tooltip=['zip_code:N', 'city:N', 'state:N']  # added tooltip ).project(     type='albersUsa' ).properties(     width=700,     height=400, ) Out[\u00a0]: In\u00a0[14]: Copied! <pre>zipcodes\n</pre> zipcodes Out[14]: zip_code latitude longitude city state county 0 00501 40.922326 -72.637078 Holtsville NY Suffolk 1 00544 40.922326 -72.637078 Holtsville NY Suffolk 2 00601 18.165273 -66.722583 Adjuntas PR Adjuntas 3 00602 18.393103 -67.180953 Aguada PR Aguada 4 00603 18.455913 -67.145780 Aguadilla PR Aguadilla ... ... ... ... ... ... ... 42044 99926 55.094325 -131.566827 Metlakatla AK Prince Wales Ketchikan 42045 99927 55.517921 -132.003244 Point Baker AK Prince Wales Ketchikan 42046 99928 55.395359 -131.675370 Ward Cove AK Ketchikan Gateway 42047 99929 56.449893 -132.364407 Wrangell AK Wrangell Petersburg 42048 99950 55.542007 -131.432682 Ketchikan AK Ketchikan Gateway <p>42049 rows \u00d7 6 columns</p> In\u00a0[26]: Copied! <pre>usmap = data.us_10m()\nusmap.keys()\n</pre> usmap = data.us_10m() usmap.keys() Out[26]: <pre>dict_keys(['type', 'transform', 'objects', 'arcs'])</pre> In\u00a0[27]: Copied! <pre>usmap['type']\n</pre> usmap['type'] Out[27]: <pre>'Topology'</pre> In\u00a0[28]: Copied! <pre>usmap['transform']\n</pre> usmap['transform'] Out[28]: <pre>{'scale': [0.003589294092944858, 0.0005371535195261037],\n 'translate': [-179.1473400003406, 17.67439566600018]}</pre> <p>This <code>transformation</code> is used to quantize the data and store the coordinates in integer (easier to store than float type numbers).</p> <p>https://github.com/topojson/topojson-specification#212-transforms</p> In\u00a0[29]: Copied! <pre>usmap['objects'].keys()\n</pre> usmap['objects'].keys() Out[29]: <pre>dict_keys(['counties', 'states', 'land'])</pre> <p>This data contains not only county-level boundaries (objects) but also states and land boundaries.</p> In\u00a0[30]: Copied! <pre>usmap['objects']['land']['type'], usmap['objects']['states']['type'], usmap['objects']['counties']['type']\n</pre> usmap['objects']['land']['type'], usmap['objects']['states']['type'], usmap['objects']['counties']['type'] Out[30]: <pre>('MultiPolygon', 'GeometryCollection', 'GeometryCollection')</pre> <p><code>land</code> is a multipolygon (one object) and <code>states</code> and <code>counties</code> contains many geometrics (multipolygons) because there are many states (counties). We can look at a state as a set of arcs that define it. It's <code>id</code> captures the identity of the state and is the key to link to other datasets.</p> In\u00a0[31]: Copied! <pre>state1 = usmap['objects']['states']['geometries'][1]\nstate1\n</pre> state1 = usmap['objects']['states']['geometries'][1] state1 Out[31]: <pre>{'type': 'MultiPolygon',\n 'arcs': [[[10337]],\n  [[10342]],\n  [[10341]],\n  [[10343]],\n  [[10834, 10340]],\n  [[10344]],\n  [[10345]],\n  [[10338]]],\n 'id': 15}</pre> <p>The <code>arcs</code> referred here is defined in <code>usmap['arcs']</code>.</p> In\u00a0[32]: Copied! <pre>usmap['arcs'][:10]\n</pre> usmap['arcs'][:10] Out[32]: <pre>[[[15739, 57220], [0, 0]],\n [[15739, 57220], [29, 62], [47, -273]],\n [[15815, 57009], [-6, -86]],\n [[15809, 56923], [0, 0]],\n [[15809, 56923], [-36, -8], [6, -210], [32, 178]],\n [[15811, 56883], [9, -194], [44, -176], [-29, -151], [-24, -319]],\n [[15811, 56043], [-12, -216], [26, -171]],\n [[15825, 55656], [-2, 1]],\n [[15823, 55657], [-19, 10], [26, -424], [-26, -52]],\n [[15804, 55191], [-30, -72], [-47, -344]]]</pre> <p>It seems pretty daunting to work with this dataset, right? But fortunately people have already built tools to handle such data.</p> In\u00a0[33]: Copied! <pre>states = alt.topo_feature(data.us_10m.url, 'states')\n</pre> states = alt.topo_feature(data.us_10m.url, 'states') In\u00a0[34]: Copied! <pre>states\n</pre> states Out[34]: <pre>UrlData({\n  format: TopoDataFormat({\n    feature: 'states',\n    type: 'topojson'\n  }),\n  url: 'https://cdn.jsdelivr.net/npm/vega-datasets@v1.29.0/data/us-10m.json'\n})</pre> <p>Can you find a mark for geographical shapes from here https://altair-viz.github.io/user_guide/marks/index.html# and draw the states?</p> In\u00a0[37]: Copied! <pre># Can you find a mark for geographical shapes from here https://altair-viz.github.io/user_guide/marks/index.html# and draw the states?\nalt.Chart(states).mark_geoshape(\n    fill='lightgray',\n    stroke='black'\n).encode(\n    tooltip='id:N'\n).properties(\n    width=800,\n    height=400\n)\n</pre> # Can you find a mark for geographical shapes from here https://altair-viz.github.io/user_guide/marks/index.html# and draw the states? alt.Chart(states).mark_geoshape(     fill='lightgray',     stroke='black' ).encode(     tooltip='id:N' ).properties(     width=800,     height=400 ) Out[37]: <p>And then project it using the <code>albersUsa</code>?</p> In\u00a0[39]: Copied! <pre>alt.Chart(states).mark_geoshape(\n    fill='lightgray',\n    stroke='black'\n).encode(\n    tooltip='id:N'\n).properties(\n    width=800,\n    height=400\n).project(\n    type='albersUsa'\n)\n</pre> alt.Chart(states).mark_geoshape(     fill='lightgray',     stroke='black' ).encode(     tooltip='id:N' ).properties(     width=800,     height=400 ).project(     type='albersUsa' ) Out[39]: <p>Can you do the same thing with counties and draw county boundaries?</p> In\u00a0[62]: Copied! <pre>counties = alt.topo_feature(data.us_10m.url, 'counties')\nalt.Chart(counties).mark_geoshape(\n    fill='lightgray',\n    stroke='black'\n).encode(\n    tooltip='id:N'\n).properties(\n    width=800,\n    height=400\n).project(\n    type='albersUsa'\n)\n</pre> counties = alt.topo_feature(data.us_10m.url, 'counties') alt.Chart(counties).mark_geoshape(     fill='lightgray',     stroke='black' ).encode(     tooltip='id:N' ).properties(     width=800,     height=400 ).project(     type='albersUsa' ) Out[62]: <p>Let's load some county-level unemployment data.</p> In\u00a0[41]: Copied! <pre>unemp_data = data.unemployment(sep='\\t')\nunemp_data.head()\n</pre> unemp_data = data.unemployment(sep='\\t') unemp_data.head() Out[41]: id rate 0 1001 0.097 1 1003 0.091 2 1005 0.134 3 1007 0.121 4 1009 0.099 <p>This dataset has unemployment rate. When? I don't know. We don't care about data provenance here because the goal is quickly trying out choropleth. But if you're working with a real dataset, you should be very sensitive about the provenance of your dataset. Make sure you understand where the data came from and how it was processed.</p> <p>Anyway, for each county specified with <code>id</code>. To combine two datasets, we use \"Lookup transform\" - https://vega.github.io/vega/docs/transforms/lookup/. Essentially, we use the <code>id</code> in the map data to look up (again) <code>id</code> field in the <code>unemp_data</code> and then bring in the <code>rate</code> variable. Then, we can use that <code>rate</code> variable to encode the color of the <code>geoshape</code> mark.</p> In\u00a0[42]: Copied! <pre>alt.Chart(counties).mark_geoshape().project(\n    type='albersUsa'\n).transform_lookup(\n    lookup='id',\n    from_=alt.LookupData(unemp_data, 'id', ['rate'])\n).encode(\n    color='rate:Q'\n).properties(\n    width=700,\n    height=400\n)\n</pre> alt.Chart(counties).mark_geoshape().project(     type='albersUsa' ).transform_lookup(     lookup='id',     from_=alt.LookupData(unemp_data, 'id', ['rate']) ).encode(     color='rate:Q' ).properties(     width=700,     height=400 ) Out[42]: <p>There you have it, a nice choropleth map. \ud83d\ude0e</p> In\u00a0[48]: Copied! <pre>from ipyleaflet import Map, Marker\n\ncenter = (39.1737568,-86.5220677)\n\nm = Map(center=center, zoom=15)\n\nmarker = Marker(location=center, draggable=False)\nm.add(marker)\n\ndisplay(m)\n\nmarker.location = center\n</pre> from ipyleaflet import Map, Marker  center = (39.1737568,-86.5220677)  m = Map(center=center, zoom=15)  marker = Marker(location=center, draggable=False) m.add(marker)  display(m)  marker.location = center  <pre>Map(center=[39.1737568, -86.5220677], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_titl\u2026</pre> <p>ipyleaflet lets you use various basemaps. You can find the list of available basemaps here: https://leaflet-extras.github.io/leaflet-providers/preview/</p> <p>For instance, let's use OpenStreetMap and zoom into the Luddy School building.</p> In\u00a0[49]: Copied! <pre>from ipyleaflet import basemaps\n\ncenter = (39.172681590059604, -86.5233788123735)\nzoom = 17\n\nMap(basemap=basemaps.OpenStreetMap.HOT, center=center, zoom=zoom)\n</pre> from ipyleaflet import basemaps  center = (39.172681590059604, -86.5233788123735) zoom = 17  Map(basemap=basemaps.OpenStreetMap.HOT, center=center, zoom=zoom) Out[49]: <pre>Map(center=[39.172681590059604, -86.5233788123735], controls=(ZoomControl(options=['position', 'zoom_in_text',\u2026</pre> <p>Let's create a heatmap. First create a small list of random points around the center.</p> In\u00a0[50]: Copied! <pre>center\n</pre> center Out[50]: <pre>(39.172681590059604, -86.5233788123735)</pre> In\u00a0[52]: Copied! <pre>from random import uniform\n\nrandom_points = [\n    [\n        uniform(center[0]-0.01, center[0]+0.01), \n        uniform(center[1]-0.01, center[1]+0.01), \n        uniform(0, 5)\n    ] for x in range(100)\n]\nrandom_points[:2]\n</pre> from random import uniform  random_points = [     [         uniform(center[0]-0.01, center[0]+0.01),          uniform(center[1]-0.01, center[1]+0.01),          uniform(0, 5)     ] for x in range(100) ] random_points[:2] Out[52]: <pre>[[39.1668122403533, -86.52327515084703, 2.1730825811477006],\n [39.16729783526083, -86.52370948019752, 4.741391103458917]]</pre> <p>Q: using these random points, can you create a heatmap?</p> <p>documentation: https://ipyleaflet.readthedocs.io/en/latest/layers/heatmap.html</p> In\u00a0[56]: Copied! <pre>from ipyleaflet import Map, Heatmap\n\nm = Map(center=center, zoom=zoom)\nheatmap = Heatmap(\n    locations=random_points,\n    radius=20,\n    blur=15,\n    max_zoom=1,\n    gradient={\n        0.0: 'white',\n        1.0: 'red'\n    }\n)\nm.add_layer(heatmap)\nm\n</pre> from ipyleaflet import Map, Heatmap  m = Map(center=center, zoom=zoom) heatmap = Heatmap(     locations=random_points,     radius=20,     blur=15,     max_zoom=1,     gradient={         0.0: 'white',         1.0: 'red'     } ) m.add_layer(heatmap) m Out[56]: <pre>Map(center=[39.172681590059604, -86.5233788123735], controls=(ZoomControl(options=['position', 'zoom_in_text',\u2026</pre>"},{"location":"w10-maps/class/#visualizing-maps","title":"Visualizing Maps\u00b6","text":"<p>Let's draw some maps. \ud83d\uddfa\ud83e\uddd0</p>"},{"location":"w10-maps/class/#a-dotmap-with-altair","title":"A dotmap with Altair\u00b6","text":"<p>Let's start with altair.</p> <p>When your dataset is large, it is nice to enable something called \"json data transformer\" in altair. What it does is, instead of generating and holding the whole dataset in the memory, to transform the dataset and save into a temporary file. This makes the whole plotting process much more efficient. For more information, check out: https://altair-viz.github.io/user_guide/data_transformers.html</p>"},{"location":"w10-maps/class/#lets-draw-it","title":"Let's draw it\u00b6","text":"<p>Now we have the dataset loaded and start drawing some plots. Let's say you don't know anything about map projections. What would you try with geographical data? Probably the simplest way is considering (longitude, latitude) as a Cartesian coordinate and directly plot them.</p>"},{"location":"w10-maps/class/#choropleth","title":"Choropleth\u00b6","text":"<p>Let's try some choropleth now. Vega datasets have US county / state boundary data (<code>us_10m</code>) and world country boundary data (<code>world-110m</code>). You can take a look at the boundaries on GitHub (they renders topoJSON files):</p> <ul> <li>https://github.com/vega/vega-datasets/blob/main/data/us-10m.json</li> <li>https://github.com/vega/vega-datasets/blob/main/data/world-110m.json</li> </ul> <p>If you click \"Raw\" then you can take a look at the actual file, which is hard to read.</p> <p>Essentially, each file is a large dictionary with the following keys.</p>"},{"location":"w10-maps/class/#leaflet","title":"Leaflet\u00b6","text":"<p>Another useful tool is Leaflet. It allows you to use various map tile data (Google maps, Open streetmap, ...) with many types of marks (points, heatmap, etc.). Leaflet.js is one of the easiest options to do that on the web, and there is a Python bridge of it: https://github.com/jupyter-widgets/ipyleaflet.</p> <p>You can install it simply by</p> <pre>pip install ipyleaflet\n</pre> <p>It is quite easy to display an interactive map with it. You can also add markers, polygons, etc.</p>"},{"location":"w11-text/assignment_w11_text/","title":"Homework 11 - Text (optional)","text":"In\u00a0[\u00a0]: Copied! <pre># Start YOUR CODE HERE\n</pre> # Start YOUR CODE HERE <p><code>YOUR DISCUSSION HERE</code></p>"},{"location":"w11-text/assignment_w11_text/#homework-11-text-optional","title":"Homework 11 - Text (optional)\u00b6","text":"<p>In this homework, you will create a text visualization of a dataset of your choice. It can be either a collection of text documents (like tweets, news articles, etc) or a single text document. You can get text data from various sources, such as Kaggle datasets, or Gutenberg Project, or any other source that provides text data. Then you will use any tecniques you learned in class to visualize the text data. Explain the visualization and its insights or limitations in case you don't find any insights.</p>"},{"location":"w11-text/assignment_w11_text/#instructions","title":"Instructions\u00b6","text":"<ol> <li><p>Project Setup:</p> <ul> <li>Set up your Python and Jupyter (or VSCode) environment.</li> <li>Clone or download the repository provided in class (refer to the class notes).</li> </ul> </li> <li><p>Choose a Dataset:</p> <ul> <li>Select a dataset that contains text data. It can be anything from tweets, news articles, books, or any other text-based dataset. Ensure that the dataset is in a format that can be easily read into Python (like CSV, JSON, TXT, etc.).</li> </ul> </li> <li><p>Identify the text content:</p> <ul> <li>Identify which features in the dataset contain the text data you want to visualize. For example, if you are using a dataset of tweets, the text content will be in the 'text' or 'content' column.</li> </ul> </li> <li><p>Choose at least two text visualization techniques and apply them to the data:</p> <ul> <li>You can use techniques such as:<ul> <li>Word clouds</li> <li>Frequency distribution of words</li> <li>Adjacency networks</li> <li>Syntactic parsing visualization</li> <li>Topic modeling visualization</li> <li>Named entity recognition visualization</li> <li>Any other text visualization technique you learned in class</li> </ul> </li> <li>You can use the hands-on class examples as reference (feel free to copy and modify the code as needed).</li> </ul> </li> <li><p>Documentation and discussion:</p> <ul> <li>Comment your code and add markdown explanations for each part of your analysis.</li> <li>Discuss the insights you gained from the visualizations. If you do not find any insights, explain the limitations of the dataset or the visualization techniques used.</li> </ul> </li> <li><p>Submission:</p> <ul> <li>Ensure your notebook is complete and all cells are executed without errors.</li> <li>Save your notebook and export as either PDF or HTML. If the visualizations using altair are not being shown in the html, submit a separated version with altair html. Refer to: https://altair-viz.github.io/getting_started/starting.html#publishing-your-visualization (you can use the <code>chart.save('chart_file.html')</code> method).</li> <li>Submit to Canvas.</li> </ul> </li> </ol>"},{"location":"w11-text/assignment_w11_text/#discussion","title":"Discussion\u00b6","text":"<p>Discuss briefly the results of the dimension reduction methods you applied. What do you observe? Do the reduced dimensions capture any structure of the data? How do the two methods compare? Are there any interesting patterns or clusters in the data that can be observed visually?</p>"},{"location":"w11-text/class/","title":"Text Visualization with Python","text":"In\u00a0[15]: Copied! <pre># !pip install nltk spacy matplotlib scikit-learn gensim umap-learn wordcloud sentence-transformers\n# !python -m spacy download en_core_web_sm\n</pre> # !pip install nltk spacy matplotlib scikit-learn gensim umap-learn wordcloud sentence-transformers # !python -m spacy download en_core_web_sm In\u00a0[16]: Copied! <pre>import nltk\nimport spacy\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n# remove max_rows limit for Altair\nalt.data_transformers.enable('default', max_rows=None)\n\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom wordcloud import WordCloud\n\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize, pos_tag\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\n\nimport umap\n\nfrom gensim.models import Doc2Vec\nfrom sentence_transformers import SentenceTransformer\n\nfrom spacy import displacy\n\n# Download NLTK resources the first time you run the notebook\nnltk.download(\"punkt\")\nnltk.download('punkt_tab')\nnltk.download(\"stopwords\")\nnltk.download(\"averaged_perceptron_tagger\")\nnltk.download('averaged_perceptron_tagger_eng')\nnltk.download(\"wordnet\")\n</pre> import nltk import spacy import altair as alt import pandas as pd import numpy as np # remove max_rows limit for Altair alt.data_transformers.enable('default', max_rows=None)  import matplotlib.pyplot as plt from tqdm.auto import tqdm from wordcloud import WordCloud  from nltk.corpus import stopwords from nltk import word_tokenize, pos_tag from nltk.stem import WordNetLemmatizer  from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.decomposition import PCA  import umap  from gensim.models import Doc2Vec from sentence_transformers import SentenceTransformer  from spacy import displacy  # Download NLTK resources the first time you run the notebook nltk.download(\"punkt\") nltk.download('punkt_tab') nltk.download(\"stopwords\") nltk.download(\"averaged_perceptron_tagger\") nltk.download('averaged_perceptron_tagger_eng') nltk.download(\"wordnet\")  <pre>[nltk_data] Downloading package punkt to /Users/filsilva/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /Users/filsilva/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/filsilva/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/filsilva/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /Users/filsilva/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/filsilva/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n</pre> Out[16]: <pre>True</pre> In\u00a0[17]: Copied! <pre>df = pd.read_csv(\"../../Datasets/ukraine_tweets.csv\") # adjust your path accordingly\n# drop nans\ndf.dropna(subset=[\"Text\"], inplace=True)\ntexts = df[\"Text\"].astype(str).tolist()\nprint(f\"Loaded {len(texts):,} documents.\")\nprint(\"\\n\".join(texts[:5]))  # Display the first 5 documents\n</pre> df = pd.read_csv(\"../../Datasets/ukraine_tweets.csv\") # adjust your path accordingly # drop nans df.dropna(subset=[\"Text\"], inplace=True) texts = df[\"Text\"].astype(str).tolist() print(f\"Loaded {len(texts):,} documents.\") print(\"\\n\".join(texts[:5]))  # Display the first 5 documents <pre>/var/folders/jh/xkyk5yn976z_y46xvbg2kjjm0000gn/T/ipykernel_25174/74004665.py:1: DtypeWarning: Columns (0,1,2,3,4,5,6,7,11,12,13,14,15,16,17,18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(\"../../Datasets/ukraine_tweets.csv\") # adjust your path accordingly\n</pre> <pre>Loaded 90,892 documents.\n\ud83c\uddfa\ud83c\udde6 Massive explosions rocks #Kharkiv. #Russia #Ukraine #UkraineWar #RussiaWar #Europe #EU #NATO #US [29] https://t.co/tpTeuYSufO\n\ud83c\uddfa\ud83c\udde6 Kharkiv is officially being struck by major strikes. #Russia #Ukraine #UkraineWar #RussiaWar #Europe #EU #NATO #US [30] https://t.co/RI5nVtzXHQ\n\ud83c\uddfa\ud83c\udde6 More angles on that strike in Kharkiv. #Russia #Ukraine #UkraineWar #RussiaWar #Europe #EU #NATO #US [31] https://t.co/j37FksnIzk\n\ud83c\uddfa\ud83c\udde6 BM-21 Grad strikes opening on #Mariupol city. #Russia #Ukraine #UkraineWar #RussiaWar #Europe #EU #NATO #US [32] https://t.co/bIGoSZ1DN6\n\ud83c\uddfa\ud83c\udde6 Damage caused by strike in Kharkiv... #Russia #Ukraine #UkraineWar #RussiaWar #Europe #EU #NATO #US [33] https://t.co/XSVb1hy1so\n</pre> <p>Let's remove tweets starting with trending now as they are just links to news articles and not actual tweets.</p> In\u00a0[18]: Copied! <pre># Let's remove tweets with term trending now: as they are just links to news articles and not actual tweets.\ntexts = [text for text in texts if not \"trending now\" in text.strip().lower()]\nprint(f\"After removing 'trending now:', we have {len(texts):,} documents out of {len(df):,}.\")\n</pre> # Let's remove tweets with term trending now: as they are just links to news articles and not actual tweets. texts = [text for text in texts if not \"trending now\" in text.strip().lower()] print(f\"After removing 'trending now:', we have {len(texts):,} documents out of {len(df):,}.\") <pre>After removing 'trending now:', we have 44,069 documents out of 90,892.\n</pre> <p>There are too many tweets, let's limit ourselves to 10000 sampled randomly from the dataset.</p> In\u00a0[19]: Copied! <pre># sample 5000 randomly uniformly\nselectionIndices = np.random.choice(len(texts), size=5000, replace=False)\ntexts = [texts[i] for i in selectionIndices]  # select the sampled texts\n</pre> # sample 5000 randomly uniformly selectionIndices = np.random.choice(len(texts), size=5000, replace=False) texts = [texts[i] for i in selectionIndices]  # select the sampled texts  In\u00a0[20]: Copied! <pre>text_blob = \" \".join(texts)\nwc = WordCloud(width=800, height=400, background_color=\"white\").generate(text_blob)\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Raw Tweet Word\u2011Cloud\", fontsize=16)\nplt.show()\n</pre> text_blob = \" \".join(texts) wc = WordCloud(width=800, height=400, background_color=\"white\").generate(text_blob)  plt.figure(figsize=(10, 5)) plt.imshow(wc, interpolation=\"bilinear\") plt.axis(\"off\") plt.title(\"Raw Tweet Word\u2011Cloud\", fontsize=16) plt.show()  <p>Not very useful, right? # Let's do some preprocessing to make it more meaningful in the next steps.</p> In\u00a0[21]: Copied! <pre># remove links from the text\n# detect links using regex\nimport re\ndef remove_links(text):\n    return re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n\n# remove links from all texts\ntexts = [remove_links(text) for text in texts]\n</pre> # remove links from the text # detect links using regex import re def remove_links(text):     return re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)  # remove links from all texts texts = [remove_links(text) for text in texts] <p>The helper <code>nltk_preprocess</code> returns a list of clean lemmas ready for vectorisation.</p> In\u00a0[22]: Copied! <pre>stop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\ndef nltk_preprocess(doc: str):\n    \"\"\"Tokenise, clean, POS\u2011tag and lemmatise a single document.\"\"\"\n    tokens = word_tokenize(doc.lower())\n    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n\n    pos_tags = pos_tag(tokens)\n\n    def tb2wn(tag):\n        return {\n            \"J\": \"a\", \"V\": \"v\", \"N\": \"n\", \"R\": \"r\"\n        }.get(tag[0], \"n\")\n\n    lemmas = [lemmatizer.lemmatize(tok, tb2wn(pos)) for tok, pos in pos_tags]\n    return lemmas\n\n# Apply cleaning to entire corpus\ntokenised = [nltk_preprocess(t) for t in tqdm(texts,\"preprocessing documents\")]\n# only keep documents with more than 3 tokens\nindices_to_keep = [i for i, toks in enumerate(tokenised) if len(toks) &gt; 3]\ntokenised = [tokenised[i] for i in indices_to_keep]\ntexts = [texts[i] for i in indices_to_keep]\n\nclean_docs = [\" \".join(toks) for toks in tokenised]\n\n# Number of documents after cleaning\nprint(f\"Number of documents after cleaning: {len(clean_docs):,}\")\n# Display the first 5 cleaned documents\nprint(\"First 5 cleaned documents:\")\nfor i, doc in enumerate(clean_docs[:5]):\n    print(f\"{i+1}: {doc}\")\n</pre> stop_words = set(stopwords.words(\"english\")) lemmatizer = WordNetLemmatizer()  def nltk_preprocess(doc: str):     \"\"\"Tokenise, clean, POS\u2011tag and lemmatise a single document.\"\"\"     tokens = word_tokenize(doc.lower())     tokens = [t for t in tokens if t.isalpha() and t not in stop_words]      pos_tags = pos_tag(tokens)      def tb2wn(tag):         return {             \"J\": \"a\", \"V\": \"v\", \"N\": \"n\", \"R\": \"r\"         }.get(tag[0], \"n\")      lemmas = [lemmatizer.lemmatize(tok, tb2wn(pos)) for tok, pos in pos_tags]     return lemmas  # Apply cleaning to entire corpus tokenised = [nltk_preprocess(t) for t in tqdm(texts,\"preprocessing documents\")] # only keep documents with more than 3 tokens indices_to_keep = [i for i, toks in enumerate(tokenised) if len(toks) &gt; 3] tokenised = [tokenised[i] for i in indices_to_keep] texts = [texts[i] for i in indices_to_keep]  clean_docs = [\" \".join(toks) for toks in tokenised]  # Number of documents after cleaning print(f\"Number of documents after cleaning: {len(clean_docs):,}\") # Display the first 5 cleaned documents print(\"First 5 cleaned documents:\") for i, doc in enumerate(clean_docs[:5]):     print(f\"{i+1}: {doc}\") <pre>preprocessing documents:   0%|          | 0/5000 [00:00&lt;?, ?it/s]</pre> <pre>Number of documents after cleaning: 4,683\nFirst 5 cleaned documents:\n1: ahora putin russia ucrania ukraine en la ciudad de dubno regi\u00f3n de rivne los invasores dispararon contra el dep\u00f3sito de petr\u00f3leo el jefe de la ovum local vitaliy koval habl\u00f3 sobre el ataque las acciones enemigas en la regi\u00f3n\n2: talk putin context alexander dugin read good summary glad biden say say\n3: smoke bomb mum fearful georgiy want leave house mistake morning fog missile smoke ukrainewar ukraineconflict\n4: long ago gerhard schr\u00f6der call putin pure democrat stil run gag whenever putin thing\n5: drone footage show strike ukrainian armor outskirt kyiv ukraine ukrainewar\n</pre> <p>Now let's create a new word cloud using the pre\u2011processed text.</p> In\u00a0[23]: Copied! <pre># new word cloud with cleaned documents\ntext_blob = \" \".join(clean_docs)\nwc = WordCloud(width=800, height=400, background_color=\"white\").generate(text_blob)\nplt.figure(figsize=(10, 5))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Cleaned Tweet Word\u2011Cloud\", fontsize=16)\nplt.show()\n</pre> # new word cloud with cleaned documents text_blob = \" \".join(clean_docs) wc = WordCloud(width=800, height=400, background_color=\"white\").generate(text_blob) plt.figure(figsize=(10, 5)) plt.imshow(wc, interpolation=\"bilinear\") plt.axis(\"off\") plt.title(\"Cleaned Tweet Word\u2011Cloud\", fontsize=16) plt.show()  In\u00a0[24]: Copied! <pre># Vectorise\nvectoriser = TfidfVectorizer(max_features=2_000)\nX_tfidf = vectoriser.fit_transform(clean_docs)\n</pre> # Vectorise vectoriser = TfidfVectorizer(max_features=2_000) X_tfidf = vectoriser.fit_transform(clean_docs) In\u00a0[25]: Copied! <pre># We can now craete a wordcloud from the TF-IDF scores\ntfidf_scores = X_tfidf.sum(axis=0).A1\ntfidf_words = vectoriser.get_feature_names_out()\ntfidf_dict = dict(zip(tfidf_words, tfidf_scores))\nwc = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(tfidf_dict)\nplt.figure(figsize=(10, 5))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"TF-IDF Word\u2011Cloud\", fontsize=16)\nplt.show()\n</pre> # We can now craete a wordcloud from the TF-IDF scores tfidf_scores = X_tfidf.sum(axis=0).A1 tfidf_words = vectoriser.get_feature_names_out() tfidf_dict = dict(zip(tfidf_words, tfidf_scores)) wc = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(tfidf_dict) plt.figure(figsize=(10, 5)) plt.imshow(wc, interpolation=\"bilinear\") plt.axis(\"off\") plt.title(\"TF-IDF Word\u2011Cloud\", fontsize=16) plt.show() <p>Ok let's compute PCA. However for TF-IDF vectors we need to use another version of PCA that works with sparse matrices.</p> In\u00a0[26]: Copied! <pre># Use truncated SVD (very similar to PCA but for sparse matrices)\nfrom sklearn.decomposition import TruncatedSVD\ncoords_pca_t = TruncatedSVD(n_components=2, random_state=0).fit_transform(X_tfidf.toarray())\n</pre> # Use truncated SVD (very similar to PCA but for sparse matrices) from sklearn.decomposition import TruncatedSVD coords_pca_t = TruncatedSVD(n_components=2, random_state=0).fit_transform(X_tfidf.toarray()) <p>We will also look at UMAP, which is a non-linear manifold learning technique that can reveal local clusters in the data.</p> In\u00a0[27]: Copied! <pre># UMAP\ncoords_umap_t = umap.UMAP(n_neighbors=10, min_dist=0.2, random_state=0, n_epochs=1000, metric='euclidean').fit_transform(X_tfidf)\n</pre> # UMAP coords_umap_t = umap.UMAP(n_neighbors=10, min_dist=0.2, random_state=0, n_epochs=1000, metric='euclidean').fit_transform(X_tfidf)  <pre>/Users/filsilva/miniforge3/envs/dataviz/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/Users/filsilva/miniforge3/envs/dataviz/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\n</pre> <p>Let's plot them</p> In\u00a0[28]: Copied! <pre># Prepare data for Altair\npca_df = pd.DataFrame(coords_pca_t, columns=[\"x\", \"y\"])\npca_df[\"method\"] = \"PCA\"\npca_df[\"text\"] = clean_docs[:len(coords_pca_t)]  # Add text for PCA\n\numap_df = pd.DataFrame(coords_umap_t, columns=[\"x\", \"y\"])\numap_df[\"method\"] = \"UMAP\"\numap_df[\"text\"] = clean_docs[:len(coords_umap_t)]  # Add text for UMAP\n\npcaChart = alt.Chart(pca_df).mark_circle(size=20, opacity=0.5).encode(\n    x=\"x\",\n    y=\"y\",\n    tooltip=[\"text\", \"method\"]\n).properties(\n    title=\"PCA of Tweets\"\n).interactive()\n\numapChart = alt.Chart(umap_df).mark_circle(size=20, opacity=0.5).encode(\n    x=\"x\",\n    y=\"y\",\n    tooltip=[\"text\", \"method\"]\n).properties(\n    title=\"UMAP of Tweets\"\n).interactive()\n\npcaChart | umapChart\n</pre> # Prepare data for Altair pca_df = pd.DataFrame(coords_pca_t, columns=[\"x\", \"y\"]) pca_df[\"method\"] = \"PCA\" pca_df[\"text\"] = clean_docs[:len(coords_pca_t)]  # Add text for PCA  umap_df = pd.DataFrame(coords_umap_t, columns=[\"x\", \"y\"]) umap_df[\"method\"] = \"UMAP\" umap_df[\"text\"] = clean_docs[:len(coords_umap_t)]  # Add text for UMAP  pcaChart = alt.Chart(pca_df).mark_circle(size=20, opacity=0.5).encode(     x=\"x\",     y=\"y\",     tooltip=[\"text\", \"method\"] ).properties(     title=\"PCA of Tweets\" ).interactive()  umapChart = alt.Chart(umap_df).mark_circle(size=20, opacity=0.5).encode(     x=\"x\",     y=\"y\",     tooltip=[\"text\", \"method\"] ).properties(     title=\"UMAP of Tweets\" ).interactive()  pcaChart | umapChart  Out[28]: <p>First let's download the small English model for spaCy, which is needed for NER and dependency parsing.</p> In\u00a0[29]: Copied! <pre>import spacy.cli\nimport spacy\nspacy.cli.download(\"en_core_web_lg\")\n</pre> import spacy.cli import spacy spacy.cli.download(\"en_core_web_lg\") <pre>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n</pre> <pre>Collecting en-core-web-lg==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 400.7/400.7 MB 80.5 MB/s eta 0:00:0000:0100:01\n\u2714 Download and installation successful\nYou can now load the package via spacy.load('en_core_web_lg')\n\u26a0 Restart to reload dependencies\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n</pre> <p>Now let's load the model and process the text.</p> In\u00a0[30]: Copied! <pre>nlp = spacy.load(\"en_core_web_lg\")\n# create a sample document to visualise\ntext_sample = \"This is a sample text for entity recognition. Ana is a great person, and she works at Indiana University. The capital of France is Paris, and the founder of Microsoft is Bill Gates.\"\nexample_doc = nlp(text_sample)\n\nprint(\"Entities found:\\n\")\nfor ent in example_doc.ents:\n    print(f\" \u2022 {ent.text:&lt;30} \u2192 {ent.label_}\")\n\n# Visualise (uncomment when running inside Jupyter)\ndisplacy.render(example_doc, style=\"ent\", jupyter=True)\ndisplacy.render(example_doc, style=\"dep\", jupyter=True, options={\"distance\": 90})\n</pre>  nlp = spacy.load(\"en_core_web_lg\") # create a sample document to visualise text_sample = \"This is a sample text for entity recognition. Ana is a great person, and she works at Indiana University. The capital of France is Paris, and the founder of Microsoft is Bill Gates.\" example_doc = nlp(text_sample)  print(\"Entities found:\\n\") for ent in example_doc.ents:     print(f\" \u2022 {ent.text:&lt;30} \u2192 {ent.label_}\")  # Visualise (uncomment when running inside Jupyter) displacy.render(example_doc, style=\"ent\", jupyter=True) displacy.render(example_doc, style=\"dep\", jupyter=True, options={\"distance\": 90})  <pre>Entities found:\n\n \u2022 Ana                            \u2192 PERSON\n \u2022 Indiana University             \u2192 ORG\n \u2022 France                         \u2192 GPE\n \u2022 Paris                          \u2192 GPE\n \u2022 Microsoft                      \u2192 ORG\n \u2022 Bill Gates                     \u2192 PERSON\n</pre> This is a sample text for entity recognition.       Ana     PERSON   is a great person, and she works at       Indiana University     ORG  . The capital of       France     GPE   is       Paris     GPE  , and the founder of       Microsoft     ORG   is       Bill Gates     PERSON  . This PRON is AUX a DET sample NOUN text NOUN for ADP entity NOUN recognition. NOUN Ana PROPN is AUX a DET great ADJ person, NOUN and CCONJ she PRON works VERB at ADP Indiana PROPN University. PROPN The DET capital NOUN of ADP France PROPN is AUX Paris, PROPN and CCONJ the DET founder NOUN of ADP Microsoft PROPN is AUX Bill PROPN Gates. PROPN nsubj det compound attr prep compound pobj nsubj det amod attr cc nsubj conj prep compound pobj det nsubj prep pobj attr cc det nsubj prep pobj conj compound attr <p>Let's get the named entities for all the tweets in the dataset and plot the word cloud.</p> In\u00a0[31]: Copied! <pre>entities = []\nfor doc in tqdm(nlp.pipe(clean_docs, batch_size=1000), desc=\"Extracting entities\"):\n    for ent in doc.ents:\n        entities.append(ent.text)\n# Create a word cloud from the entities\nentity_blob = \" \".join(entities)\nwc = WordCloud(width=800, height=400, background_color=\"white\").generate(entity_blob)\nplt.figure(figsize=(10, 5))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Entity Word\u2011Cloud\", fontsize=16)\nplt.show()\n</pre> entities = [] for doc in tqdm(nlp.pipe(clean_docs, batch_size=1000), desc=\"Extracting entities\"):     for ent in doc.ents:         entities.append(ent.text) # Create a word cloud from the entities entity_blob = \" \".join(entities) wc = WordCloud(width=800, height=400, background_color=\"white\").generate(entity_blob) plt.figure(figsize=(10, 5)) plt.imshow(wc, interpolation=\"bilinear\") plt.axis(\"off\") plt.title(\"Entity Word\u2011Cloud\", fontsize=16) plt.show()  <pre>Extracting entities: 0it [00:00, ?it/s]</pre> In\u00a0[32]: Copied! <pre># Train (skip very infrequent words to keep it fast)\nfrom gensim.models import Doc2Vec\nfrom gensim.models.doc2vec import TaggedDocument\nfrom gensim.utils import simple_preprocess\n\ntagged_corpus = [\n    TaggedDocument(words=simple_preprocess(doc),  # token list\n                   tags=[f'DOC_{i}'])             # unique tag\n    for i, doc in enumerate(texts)\n]\n\n# Hyper-parameters below are sensible starting points:\nmodel = Doc2Vec(\n    vector_size=128,    # dimensionality of the embeddings\n    window=10,          # context window for skip-gram\n    min_count=2,        # ignore words that appear &lt; 2 times\n    dm=1,               # 1 = Distributed Memory (PV-DM), 0 = DBOW\n    workers=4,          # CPU cores\n    epochs=40,          # training iterations\n    seed=42\n)\n\n# Build vocabulary \u2013 scans corpus once\nmodel.build_vocab(tagged_corpus)\n\n# Train \u2013 multiple epochs; shuffle each pass for robustness\nfor epoch in tqdm(range(model.epochs), desc=\"Training\"):\n    model.train(tagged_corpus,\n                total_examples=model.corpus_count,\n                epochs=1)\n\n# As NumPy arrays, in the order of your original list\ndoc_vectors = [model.dv[f'DOC_{i}'] for i in range(len(texts))]\n</pre> # Train (skip very infrequent words to keep it fast) from gensim.models import Doc2Vec from gensim.models.doc2vec import TaggedDocument from gensim.utils import simple_preprocess  tagged_corpus = [     TaggedDocument(words=simple_preprocess(doc),  # token list                    tags=[f'DOC_{i}'])             # unique tag     for i, doc in enumerate(texts) ]  # Hyper-parameters below are sensible starting points: model = Doc2Vec(     vector_size=128,    # dimensionality of the embeddings     window=10,          # context window for skip-gram     min_count=2,        # ignore words that appear &lt; 2 times     dm=1,               # 1 = Distributed Memory (PV-DM), 0 = DBOW     workers=4,          # CPU cores     epochs=40,          # training iterations     seed=42 )  # Build vocabulary \u2013 scans corpus once model.build_vocab(tagged_corpus)  # Train \u2013 multiple epochs; shuffle each pass for robustness for epoch in tqdm(range(model.epochs), desc=\"Training\"):     model.train(tagged_corpus,                 total_examples=model.corpus_count,                 epochs=1)  # As NumPy arrays, in the order of your original list doc_vectors = [model.dv[f'DOC_{i}'] for i in range(len(texts))]  <pre>Training:   0%|          | 0/40 [00:00&lt;?, ?it/s]</pre> <p>Let's create the PCA and UMAP projections.</p> In\u00a0[33]: Copied! <pre>coords_doc2vec_pca = PCA(n_components=2, random_state=0).fit_transform(doc_vectors)\n\ncoords_doc2vec_umap = umap.UMAP(n_neighbors=10, min_dist=0.2, random_state=0, n_epochs=1000, metric='cosine').fit_transform(doc_vectors)\n</pre> coords_doc2vec_pca = PCA(n_components=2, random_state=0).fit_transform(doc_vectors)  coords_doc2vec_umap = umap.UMAP(n_neighbors=10, min_dist=0.2, random_state=0, n_epochs=1000, metric='cosine').fit_transform(doc_vectors) <pre>/Users/filsilva/miniforge3/envs/dataviz/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/Users/filsilva/miniforge3/envs/dataviz/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\n</pre> <p>Now let's plot them.</p> In\u00a0[34]: Copied! <pre># Plots\npca_df = pd.DataFrame(coords_doc2vec_pca, columns=[\"x\", \"y\"])\npca_df[\"method\"] = \"PCA\"\npca_df[\"text\"] = texts\numap_df = pd.DataFrame(coords_doc2vec_umap, columns=[\"x\", \"y\"])\numap_df[\"method\"] = \"UMAP\"\numap_df[\"text\"] = texts\npcaChart = alt.Chart(pca_df).mark_circle(size=20, opacity=0.5).encode(\n    x=\"x\",\n    y=\"y\",\n    tooltip=[\"text\", \"method\"]\n).properties(\n    title=\"PCA of Doc2Vec Embeddings\"\n).interactive()\numapChart = alt.Chart(umap_df).mark_circle(size=20, opacity=0.5).encode(\n    x=\"x\",\n    y=\"y\",\n    tooltip=[\"text\", \"method\"]\n).properties(\n    title=\"UMAP of Doc2Vec Embeddings\"\n).interactive()\npcaChart | umapChart\n</pre> # Plots pca_df = pd.DataFrame(coords_doc2vec_pca, columns=[\"x\", \"y\"]) pca_df[\"method\"] = \"PCA\" pca_df[\"text\"] = texts umap_df = pd.DataFrame(coords_doc2vec_umap, columns=[\"x\", \"y\"]) umap_df[\"method\"] = \"UMAP\" umap_df[\"text\"] = texts pcaChart = alt.Chart(pca_df).mark_circle(size=20, opacity=0.5).encode(     x=\"x\",     y=\"y\",     tooltip=[\"text\", \"method\"] ).properties(     title=\"PCA of Doc2Vec Embeddings\" ).interactive() umapChart = alt.Chart(umap_df).mark_circle(size=20, opacity=0.5).encode(     x=\"x\",     y=\"y\",     tooltip=[\"text\", \"method\"] ).properties(     title=\"UMAP of Doc2Vec Embeddings\" ).interactive() pcaChart | umapChart Out[34]: In\u00a0[35]: Copied! <pre>model = SentenceTransformer(\"all-MiniLM-L6-v2\")\nembeddings = model.encode(texts, show_progress_bar=True)\n# Visualise the embeddings with PCA and UMAP\ncoords_sentence_pca = PCA(n_components=2, random_state=0).fit_transform(embeddings)\ncoords_sentence_umap = umap.UMAP(n_neighbors=10, min_dist=0.2, random_state=0, n_epochs=1000, metric='cosine').fit_transform(embeddings)\n</pre> model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = model.encode(texts, show_progress_bar=True) # Visualise the embeddings with PCA and UMAP coords_sentence_pca = PCA(n_components=2, random_state=0).fit_transform(embeddings) coords_sentence_umap = umap.UMAP(n_neighbors=10, min_dist=0.2, random_state=0, n_epochs=1000, metric='cosine').fit_transform(embeddings)  <pre>Batches:   0%|          | 0/147 [00:00&lt;?, ?it/s]</pre> <pre>/Users/filsilva/miniforge3/envs/dataviz/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/Users/filsilva/miniforge3/envs/dataviz/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\n</pre> <p>Let's plot them...</p> In\u00a0[36]: Copied! <pre># Prepare data for Altair\npca_df = pd.DataFrame(coords_sentence_pca, columns=[\"x\", \"y\"])\npca_df[\"method\"] = \"PCA\"\npca_df[\"text\"] = texts  # Add text for PCA\numap_df = pd.DataFrame(coords_sentence_umap, columns=[\"x\", \"y\"])\numap_df[\"method\"] = \"UMAP\"\numap_df[\"text\"] = texts  # Add text for UMAP\npcaChart = alt.Chart(pca_df).mark_circle(size=20, opacity=0.5).encode(\n    x=\"x\",\n    y=\"y\",\n    tooltip=[\"text\", \"method\"]\n).properties(\n    title=\"PCA of Sentence Embeddings\"\n).interactive()\numapChart = alt.Chart(umap_df).mark_circle(size=20, opacity=0.5).encode(\n    x=\"x\",\n    y=\"y\",\n    tooltip=[\"text\", \"method\"]\n).properties(\n    title=\"UMAP of Sentence Embeddings\"\n).interactive()\npcaChart | umapChart\n</pre> # Prepare data for Altair pca_df = pd.DataFrame(coords_sentence_pca, columns=[\"x\", \"y\"]) pca_df[\"method\"] = \"PCA\" pca_df[\"text\"] = texts  # Add text for PCA umap_df = pd.DataFrame(coords_sentence_umap, columns=[\"x\", \"y\"]) umap_df[\"method\"] = \"UMAP\" umap_df[\"text\"] = texts  # Add text for UMAP pcaChart = alt.Chart(pca_df).mark_circle(size=20, opacity=0.5).encode(     x=\"x\",     y=\"y\",     tooltip=[\"text\", \"method\"] ).properties(     title=\"PCA of Sentence Embeddings\" ).interactive() umapChart = alt.Chart(umap_df).mark_circle(size=20, opacity=0.5).encode(     x=\"x\",     y=\"y\",     tooltip=[\"text\", \"method\"] ).properties(     title=\"UMAP of Sentence Embeddings\" ).interactive() pcaChart | umapChart Out[36]: <p>What if we want to cluster the tweets into topics? Let's try KMeans clustering on the SBERT embeddings.</p> In\u00a0[37]: Copied! <pre># What if we want to cluster the tweets into topics?\n# Let's try KMeans clustering on the SBERT embeddings.\n\nfrom sklearn.cluster import KMeans\n# Number of clusters\nn_clusters = 9\n\nkmeans = KMeans(n_clusters=n_clusters, random_state=0)\nkmeans.fit(embeddings)\n# Add cluster labels to the DataFrame\numap_df[\"cluster_index\"] = kmeans.labels_\n# Visualise clusters with UMAP\ncluster_chart = alt.Chart(umap_df).mark_circle(size=20, opacity=0.5).encode(\n    x=\"x\",\n    y=\"y\",\n    color=alt.Color(\"cluster_index:N\", scale=alt.Scale(scheme='category10')),\n    tooltip=[\"text\", \"method\", \"cluster_index\"]\n).properties(\n    title=f\"UMAP of Sentence Embeddings with {n_clusters} Clusters\"\n).interactive()\ncluster_chart\n</pre> # What if we want to cluster the tweets into topics? # Let's try KMeans clustering on the SBERT embeddings.  from sklearn.cluster import KMeans # Number of clusters n_clusters = 9  kmeans = KMeans(n_clusters=n_clusters, random_state=0) kmeans.fit(embeddings) # Add cluster labels to the DataFrame umap_df[\"cluster_index\"] = kmeans.labels_ # Visualise clusters with UMAP cluster_chart = alt.Chart(umap_df).mark_circle(size=20, opacity=0.5).encode(     x=\"x\",     y=\"y\",     color=alt.Color(\"cluster_index:N\", scale=alt.Scale(scheme='category10')),     tooltip=[\"text\", \"method\", \"cluster_index\"] ).properties(     title=f\"UMAP of Sentence Embeddings with {n_clusters} Clusters\" ).interactive() cluster_chart  Out[37]: <p>We can now use word clouds to visualise the clusters.</p> In\u00a0[38]: Copied! <pre># We can now use word clouds to visualise the clusters.\n\ndef generate_wordcloud_for_cluster(cluster_index, df,ax):\n    \"\"\"Generate a word cloud for a specific cluster index.\"\"\"\n    cluster_texts = df[df[\"cluster_index\"] == cluster_index][\"text\"].tolist()\n    text_blob = \" \".join(cluster_texts)\n    wc = WordCloud(width=800, height=400, background_color=\"white\").generate(text_blob)\n    \n    ax.imshow(wc, interpolation=\"bilinear\")\n    ax.axis(\"off\")\n    ax.set_title(f\"Cluster {cluster_index} Word\u2011Cloud\", fontsize=16)\n# Create subplots for each cluster\nfig, axes = plt.subplots(nrows=n_clusters, ncols=1, figsize=(20 * n_clusters, 20))\naxes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\nfor i in range(n_clusters):\n    generate_wordcloud_for_cluster(i, umap_df, axes[i])\nplt.tight_layout()\nplt.show()\n</pre> # We can now use word clouds to visualise the clusters.  def generate_wordcloud_for_cluster(cluster_index, df,ax):     \"\"\"Generate a word cloud for a specific cluster index.\"\"\"     cluster_texts = df[df[\"cluster_index\"] == cluster_index][\"text\"].tolist()     text_blob = \" \".join(cluster_texts)     wc = WordCloud(width=800, height=400, background_color=\"white\").generate(text_blob)          ax.imshow(wc, interpolation=\"bilinear\")     ax.axis(\"off\")     ax.set_title(f\"Cluster {cluster_index} Word\u2011Cloud\", fontsize=16) # Create subplots for each cluster fig, axes = plt.subplots(nrows=n_clusters, ncols=1, figsize=(20 * n_clusters, 20)) axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration for i in range(n_clusters):     generate_wordcloud_for_cluster(i, umap_df, axes[i]) plt.tight_layout() plt.show()  <p>However it is not very informative, as the clusters are not very distinct (except for those with different languages).</p> <p>Another option is using log odds ratios, which can help us identify the most distinctive words in each cluster. This is a more advanced technique described in https://www.cambridge.org/core/journals/political-analysis/article/fightin-words-lexical-feature-selection-and-evaluation-for-identifying-the-content-of-political-conflict/81B3703230D21620B81EB6E2266C7A66</p> <p>You can reuse this code to compute the log odds ratios for each cluster.</p> In\u00a0[46]: Copied! <pre># Nothing to see. How about we use log odds ratios to find the most distinctive words in each cluster?\nfrom collections import Counter, defaultdict\nimport math\n# check out the paper: # https://www.aclweb.org/anthology/P17-1003.pdf\ndef logodds(corpora_dic, bg_counter):\n    \"\"\" It calculates the log odds ratio of term i's frequency between \n    a target corpus and another corpus, with the prior information from\n    a background corpus. Inputs are:\n    \n    - a dictionary of Counter objects (corpora of our interest)\n    - a Counter objects (background corpus)\n    \n    Output is a dictionary of dictionaries. Each dictionary contains the log \n    odds ratio of each word. \n    \n    \"\"\"\n    corp_size = dict([(c, sum(corpora_dic[c].values())) for c in corpora_dic])\n    bg_size = sum(bg_counter.values())\n    result = dict([(c, {}) for c in corpora_dic])\n    \n    for name, c in corpora_dic.items():\n        for word in c:\n            # if 10 &gt; sum(1 for corpus in corpora_dic.values() if corpus[word]):\n            #    continue\n            \n            fi = c[word]\n            # 'fi' is the count of 'word' in the current corpus 'c' (target corpus).\n            fj = sum(co[word] for x, co in corpora_dic.items() if x != name)\n            # 'fj' is the total count of 'word' across all other corpora except the target.\n            fbg = bg_counter[word]\n            # 'fbg' is the count of 'word' in the background corpus.\n            ni = corp_size[name]\n            # 'ni' is the total number of words in the target corpus.\n            nj = sum(x for idx, x in corp_size.items() if idx != name)\n            # 'nj' is the total number of words across all corpora other than the target.\n            \n            nbg = bg_size  # 'nbg' represents the total number of words in the background corpus.\n            oddsratio = math.log(fi+fbg) - math.log(ni+nbg-(fi+fbg)) -\\\n                        math.log(fj+fbg) + math.log(nj+nbg-(fj+fbg))  # 'oddsratio' calculates the log odds ratio for 'word' using smoothed frequency counts.\n            std = 1.0 / (fi+fbg) + 1.0 / (fj+fbg)  # 'std' estimates the variance (denominator) used to standardize the log odds ratio.\n            z = oddsratio / math.sqrt(std)  # 'z' is the standardized z-score of the log odds ratio, indicating significance.\n            result[name][word] = z  # The z-score is stored in the result dictionary for the given corpus name and word.\n            \n    # Sort words by log-odds\n    grouped_sorted_ngrams = {key: sorted(entry.items(), key=lambda x: x[1], reverse=True)\n                             for key, entry in result.items()}\n    return grouped_sorted_ngrams\n</pre> # Nothing to see. How about we use log odds ratios to find the most distinctive words in each cluster? from collections import Counter, defaultdict import math # check out the paper: # https://www.aclweb.org/anthology/P17-1003.pdf def logodds(corpora_dic, bg_counter):     \"\"\" It calculates the log odds ratio of term i's frequency between      a target corpus and another corpus, with the prior information from     a background corpus. Inputs are:          - a dictionary of Counter objects (corpora of our interest)     - a Counter objects (background corpus)          Output is a dictionary of dictionaries. Each dictionary contains the log      odds ratio of each word.           \"\"\"     corp_size = dict([(c, sum(corpora_dic[c].values())) for c in corpora_dic])     bg_size = sum(bg_counter.values())     result = dict([(c, {}) for c in corpora_dic])          for name, c in corpora_dic.items():         for word in c:             # if 10 &gt; sum(1 for corpus in corpora_dic.values() if corpus[word]):             #    continue                          fi = c[word]             # 'fi' is the count of 'word' in the current corpus 'c' (target corpus).             fj = sum(co[word] for x, co in corpora_dic.items() if x != name)             # 'fj' is the total count of 'word' across all other corpora except the target.             fbg = bg_counter[word]             # 'fbg' is the count of 'word' in the background corpus.             ni = corp_size[name]             # 'ni' is the total number of words in the target corpus.             nj = sum(x for idx, x in corp_size.items() if idx != name)             # 'nj' is the total number of words across all corpora other than the target.                          nbg = bg_size  # 'nbg' represents the total number of words in the background corpus.             oddsratio = math.log(fi+fbg) - math.log(ni+nbg-(fi+fbg)) -\\                         math.log(fj+fbg) + math.log(nj+nbg-(fj+fbg))  # 'oddsratio' calculates the log odds ratio for 'word' using smoothed frequency counts.             std = 1.0 / (fi+fbg) + 1.0 / (fj+fbg)  # 'std' estimates the variance (denominator) used to standardize the log odds ratio.             z = oddsratio / math.sqrt(std)  # 'z' is the standardized z-score of the log odds ratio, indicating significance.             result[name][word] = z  # The z-score is stored in the result dictionary for the given corpus name and word.                  # Sort words by log-odds     grouped_sorted_ngrams = {key: sorted(entry.items(), key=lambda x: x[1], reverse=True)                              for key, entry in result.items()}     return grouped_sorted_ngrams  <p>We need two inputs for this function. One is a dictionary pointing to a Counter of tokens for each cluster (number of times each token appears in each cluster). The other is a dictionary pointing to a Counter of tokens for the entire dataset (number of times each token appears in the entire dataset). We can reuse the nltk_preprocess function to compute the tokens for each cluster and the entire dataset. Let's compute the log odds ratios for each cluster.</p> In\u00a0[47]: Copied! <pre># calculate log odds ratios for each cluster\ncorpora = defaultdict(Counter)\n\nbg_counter = Counter()\nfor i,(text, cluster_index) in enumerate(zip(umap_df[\"text\"], umap_df[\"cluster_index\"])):\n    tokens = nltk_preprocess(text)\n    corpora[cluster_index].update(tokens)\n    bg_counter.update(tokens)\n\n# Calculate log odds ratios\nlog_odds_results = logodds(corpora, bg_counter)\n# use wordcloud to visualise the log odds ratios\ndef generate_wordcloud_from_log_odds(log_odds_dict, ax):\n    \"\"\"Generate a word cloud from log odds ratios.\"\"\"\n    wc = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(log_odds_dict)\n    ax.imshow(wc, interpolation=\"bilinear\")\n    ax.axis(\"off\")\n# Create subplots for each cluster\nfig, axes = plt.subplots(nrows=n_clusters, ncols=1, figsize=(20 * n_clusters, 20))\naxes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\nfor i, (cluster_index, log_odds_dict) in enumerate(sorted(log_odds_results.items())):\n    generate_wordcloud_from_log_odds(dict(log_odds_dict), axes[i])\n    axes[i].set_title(f\"Cluster {cluster_index} Log Odds Word\u2011Cloud\", fontsize=16)\nplt.tight_layout()\nplt.show()\n</pre> # calculate log odds ratios for each cluster corpora = defaultdict(Counter)  bg_counter = Counter() for i,(text, cluster_index) in enumerate(zip(umap_df[\"text\"], umap_df[\"cluster_index\"])):     tokens = nltk_preprocess(text)     corpora[cluster_index].update(tokens)     bg_counter.update(tokens)  # Calculate log odds ratios log_odds_results = logodds(corpora, bg_counter) # use wordcloud to visualise the log odds ratios def generate_wordcloud_from_log_odds(log_odds_dict, ax):     \"\"\"Generate a word cloud from log odds ratios.\"\"\"     wc = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(log_odds_dict)     ax.imshow(wc, interpolation=\"bilinear\")     ax.axis(\"off\") # Create subplots for each cluster fig, axes = plt.subplots(nrows=n_clusters, ncols=1, figsize=(20 * n_clusters, 20)) axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration for i, (cluster_index, log_odds_dict) in enumerate(sorted(log_odds_results.items())):     generate_wordcloud_from_log_odds(dict(log_odds_dict), axes[i])     axes[i].set_title(f\"Cluster {cluster_index} Log Odds Word\u2011Cloud\", fontsize=16) plt.tight_layout() plt.show() <p>The results are much more informative, as they highlight the most distinctive words in each cluster.</p>"},{"location":"w11-text/class/#text-visualization-with-python","title":"Text Visualization with Python\u00b6","text":"<p>This notebook is a guided tour of common techniques for seeing text.</p> <p>We progress from the raw corpus to increasingly sophisticated representations:</p> <ol> <li><p>Exploratory visuals \u2013 word clouds &amp; token frequencies give a first\u2011glance feel.</p> </li> <li><p>Linguistic structure \u2013 part\u2011of\u2011speech tags, lemmas and named entities via spaCy.</p> </li> <li><p>Vector spaces \u2013 TF\u2013IDF, Word2Vec and Sentence\u2011BERT let us map text to numbers.</p> </li> <li><p>Dimensionality reduction \u2013 PCA and UMAP compress those high\u2011dimensional vectors so we can plot them in 2\u2011D.</p> </li> </ol> <p>Along the way we highlight why you might pick each tool, typical pitfalls, and how to interpret the resulting plots.</p>"},{"location":"w11-text/class/#0-prerequisites","title":"0 Prerequisites\u00b6","text":"<p>Make sure all required packages are installed before running the notebook.</p> <p>Uncomment the lines below if you need to install anything \u2013 they will work on Colab, JupyterLab, VS\u00a0Code or any environment that can execute shell commands.</p>"},{"location":"w11-text/class/#1-imports-runtime-setup","title":"1 Imports &amp; Runtime Setup\u00b6","text":"<p>Below we import:</p> Library Role nltk lightweight tokenisation, stop\u2011word lists, POS tags, lemmatiser spaCy industrial\u2011strength NLP for syntactic parses &amp; named entities wordcloud generates the word\u2011cloud image scikit\u2011learn TF\u2013IDF vectoriser &amp; PCA implementation UMAP non\u2011linear manifold projection that reveals local clusters gensim Word2Vec training sentence\u2011transformers pretrained SBERT sentence embeddings <p>We also download the small NLTK corpora needed for stop\u2011words, tokenisation and POS\u2011tagging.</p>"},{"location":"w11-text/class/#2-load-your-dataset","title":"2 Load Your Dataset\u00b6","text":"<p>For this demo we use the <code>ukraine_tweets.csv</code> dataset that contains a column <code>Text</code> with raw tweet bodies.</p> <p>Feel free to substitute your own dataset \u2013 just make sure to point <code>df['Text']</code> to the right column.</p>"},{"location":"w11-text/class/#3-wordcloud","title":"3 Word\u2011Cloud\u00b6","text":"<p>A word cloud is a very rough but engaging way to gauge term frequency. Use it only as an ice\u2011breaker \u2013 it is sensitive to stop\u2011word removal, case, and tokenisation nuances, and it completely ignores context.</p>"},{"location":"w11-text/class/#4-nltk-preprocessing","title":"4 NLTK Pre\u2011processing\u00b6","text":"<p>Cleaning text is crucial for downstream models. Steps we apply:</p> <ol> <li>Tokenise &amp; lowercase \u2013 breaks text into words and removes case differences.</li> <li>Filter \u2013 drop punctuation, digits and common English stop\u2011words.</li> <li>POS\u2011tag \u2013 assign grammatical roles so lemmatisation knows whether flies is a noun or a verb.</li> <li>Lemmatise \u2013 reduce inflected forms to their dictionary head\u2011words (e.g., running \u2192 run).</li> </ol> <p>But let's first remove links from the tweets, as they are not useful for our analysis.</p>"},{"location":"w11-text/class/#5-tfidf-vectors-pca-umap","title":"5 TF\u2013IDF Vectors \u279c\u00a0PCA &amp; UMAP\u00b6","text":"<p>Term Frequency\u2013Inverse Document Frequency weighs tokens so that common words are down\u2011weighted and rare (but potentially informative) words are emphasised.</p> <p>Once we have a matrix of size <code>n_docs\u00a0\u00d7\u00a0n_terms</code> (often sparse and very high\u2011dimensional), we project it to 2\u2011D:</p> <ul> <li><p>PCA \u2013 linear projection that maximises variance; good for global structure but may blur clusters.</p> </li> <li><p>UMAP \u2013 non\u2011linear; preserves local neighbourhoods and often reveals finer clusters at the expense of some distortion.</p> </li> </ul>"},{"location":"w11-text/class/#6-spacy-named-entity-recognition-dependency-parsing","title":"6 spaCy\u00a0\u25b6\u00a0Named\u00a0Entity Recognition &amp; Dependency Parsing\u00b6","text":"<p>spaCy ships with statistical models that identify named entities (people, organisations, locations\u2026) and syntactic dependencies.</p> <p>These views help answer questions like \u201cWho are the main actors?\u201d or \u201cHow are subjects and verbs connected?\u201d</p>"},{"location":"w11-text/class/#7-doc2vec-embeddings-pca-umap","title":"7 Doc2Vec Embeddings \u279c\u00a0PCA &amp; UMAP\u00b6","text":"<p>Doc2Vec learns vectors such that documents (or sentences) appearing in similar contexts lie close in space.</p> <p>By projecting those vectors we can inspect semantic clusters (city\u2011country, positive\u2011negative, etc.). Additionally, we can visualize these embeddings using PCA and UMAP to better understand the relationships between documents.</p>"},{"location":"w11-text/class/#8-sentencebert-for-documentlevel-semantics","title":"8 Sentence\u2011BERT for Document\u2011Level Semantics\u00b6","text":"<p>While Word2Vec treats each token separately, SBERT yields a single vector per document, capturing overall meaning.</p> <p>This is perfect for clustering tweets into topics, spotting outliers or feeding downstream classifiers.</p>"},{"location":"w12-networks/Communities/","title":"Community detection Examples","text":"In\u00a0[5]: Copied! <pre>import igraph as ig\nimport xnetwork as xn\nimport pandas as pd\nimport igraph as ig\nimport numpy as np\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n</pre> import igraph as ig import xnetwork as xn import pandas as pd import igraph as ig import numpy as np  import matplotlib as mpl import matplotlib.pyplot as plt from wordcloud import WordCloud <p>This finds the communities using the Leiden algorithm, which is a popular method for community detection in networks. It tries to optimize modularity, a measure of the strength of division of a network into communities. Nodes in the same community are more densely connected to each other than to nodes in other communities.</p> In\u00a0[44]: Copied! <pre>def leidenMemberships(g):\n    return g.community_leiden(\"modularity\").membership\n</pre> def leidenMemberships(g):     return g.community_leiden(\"modularity\").membership <p>Let's load the climate citation network.</p> In\u00a0[22]: Copied! <pre># Loading a citation network\ng = ig.Graph.Read_GML(\"../../Datasets/Networks/MAG-Climate-2020.gml\").simplify() # adjust path as needed\n# Let's make it undirected for simplicity\ng = g.as_undirected()\n#attributes:\nprint(g.vertex_attributes())\n</pre> # Loading a citation network g = ig.Graph.Read_GML(\"../../Datasets/Networks/MAG-Climate-2020.gml\").simplify() # adjust path as needed # Let's make it undirected for simplicity g = g.as_undirected() #attributes: print(g.vertex_attributes()) <pre>['id', 'KCoreAll', 'KCoreIN', 'doctype', 'papertitle', 'rank', 'year']\n</pre> <p>Let's record this as an attribute in the graph.</p> In\u00a0[45]: Copied! <pre>g.vs[\"Community\"] = leidenMemberships(g)\n</pre> g.vs[\"Community\"] = leidenMemberships(g) <p>Let's use the visualization function we defined earlier.</p> In\u00a0[25]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_igraph_with_matplotlib(\n    g,\n    layout,\n    node_size=300,\n    node_color=\"skyblue\",\n    node_alpha=1.0,\n    edge_color=\"gray\",\n    edge_width=1.0,\n    edge_alpha=0.7,\n    with_labels=False,\n    label_attr=\"label\",\n    label_font_size=10,\n    label_color=\"black\",\n    figsize=(6, 6),\n    dpi=100,\n    axis_off=True\n):\n    \"\"\"\n    Draw an igraph Graph `g` using matplotlib, given `layout`.\n    \n    Parameters\n    ----------\n    g : igraph.Graph\n      The graph to draw.\n    layout : Layout or sequence of (x, y)\n      An igraph Layout object or list/array of coordinate pairs.\n    node_size : float\n      Size of the nodes (passed to plt.scatter `s`).\n    node_color : color or list of colors\n      Node face color.\n    node_alpha : float\n      Node transparency (0.0 transparent, 1.0 opaque).\n    edge_color : color\n      Color for all edges.\n    edge_width : float\n      Line width for edges.\n    edge_alpha : float\n      Edge transparency.\n    with_labels : bool\n      Whether to draw vertex labels.\n    label_attr : str\n      Vertex attribute name to use for labels; if absent, vertex indices used.\n    label_font_size : float\n      Font size for labels.\n    label_color : color\n      Color for label text.\n    figsize : tuple\n      Matplotlib figure size.\n    dpi : int\n      Figure DPI.\n    axis_off : bool\n      If True, hides axes.\n    \"\"\"\n    # extract coordinates as an (N,2) array\n    coords = np.array(layout.coords) if hasattr(layout, \"coords\") else np.array(layout)\n    xs, ys = coords[:, 0], coords[:, 1]\n\n    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n\n    # draw edges\n    for src, tgt in g.get_edgelist():\n        x0, y0 = coords[src]\n        x1, y1 = coords[tgt]\n        ax.plot(\n            [x0, x1],\n            [y0, y1],\n            color=edge_color,\n            linewidth=edge_width,\n            alpha=edge_alpha,\n            zorder=1\n        )\n\n    # draw nodes\n    ax.scatter(\n        xs,\n        ys,\n        s=node_size,\n        c=node_color,\n        alpha=node_alpha,\n        zorder=2,\n        edgecolors=\"black\"\n    )\n\n    # draw labels\n    if with_labels:\n        if label_attr in g.vs.attribute_names():\n            labels = g.vs[label_attr]\n        else:\n            labels = [str(i) for i in range(g.vcount())]\n        for idx, label in enumerate(labels):\n            ax.text(\n                xs[idx],\n                ys[idx],\n                label,\n                fontsize=label_font_size,\n                color=label_color,\n                ha=\"center\",\n                va=\"center\",\n                zorder=3\n            )\n\n    if axis_off:\n        ax.set_axis_off()\n\n    plt.tight_layout()\n    plt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  def plot_igraph_with_matplotlib(     g,     layout,     node_size=300,     node_color=\"skyblue\",     node_alpha=1.0,     edge_color=\"gray\",     edge_width=1.0,     edge_alpha=0.7,     with_labels=False,     label_attr=\"label\",     label_font_size=10,     label_color=\"black\",     figsize=(6, 6),     dpi=100,     axis_off=True ):     \"\"\"     Draw an igraph Graph `g` using matplotlib, given `layout`.          Parameters     ----------     g : igraph.Graph       The graph to draw.     layout : Layout or sequence of (x, y)       An igraph Layout object or list/array of coordinate pairs.     node_size : float       Size of the nodes (passed to plt.scatter `s`).     node_color : color or list of colors       Node face color.     node_alpha : float       Node transparency (0.0 transparent, 1.0 opaque).     edge_color : color       Color for all edges.     edge_width : float       Line width for edges.     edge_alpha : float       Edge transparency.     with_labels : bool       Whether to draw vertex labels.     label_attr : str       Vertex attribute name to use for labels; if absent, vertex indices used.     label_font_size : float       Font size for labels.     label_color : color       Color for label text.     figsize : tuple       Matplotlib figure size.     dpi : int       Figure DPI.     axis_off : bool       If True, hides axes.     \"\"\"     # extract coordinates as an (N,2) array     coords = np.array(layout.coords) if hasattr(layout, \"coords\") else np.array(layout)     xs, ys = coords[:, 0], coords[:, 1]      fig, ax = plt.subplots(figsize=figsize, dpi=dpi)      # draw edges     for src, tgt in g.get_edgelist():         x0, y0 = coords[src]         x1, y1 = coords[tgt]         ax.plot(             [x0, x1],             [y0, y1],             color=edge_color,             linewidth=edge_width,             alpha=edge_alpha,             zorder=1         )      # draw nodes     ax.scatter(         xs,         ys,         s=node_size,         c=node_color,         alpha=node_alpha,         zorder=2,         edgecolors=\"black\"     )      # draw labels     if with_labels:         if label_attr in g.vs.attribute_names():             labels = g.vs[label_attr]         else:             labels = [str(i) for i in range(g.vcount())]         for idx, label in enumerate(labels):             ax.text(                 xs[idx],                 ys[idx],                 label,                 fontsize=label_font_size,                 color=label_color,                 ha=\"center\",                 va=\"center\",                 zorder=3             )      if axis_off:         ax.set_axis_off()      plt.tight_layout()     plt.show() <p>We can use wordclouds to visualize the words in the communities. This code will calculate the relative frequency of each word in the communities and then plot a word cloud for each community. The size of each word in the cloud corresponds to its frequency in the community subtracted by the frequency in the entire graph.</p> In\u00a0[47]: Copied! <pre>def lighten_color(color, amount=0.5):\n\t\"\"\"\n\tLightens the given color by multiplying (1-luminosity) by the given amount.\n\tInput can be matplotlib color string, hex string, or RGB tuple.\n\t\n\tExamples:\n\t&gt;&gt; lighten_color('g', 0.3)\n\t&gt;&gt; lighten_color('#F034A3', 0.6)\n\t&gt;&gt; lighten_color((.3,.55,.1), 0.5)\n\t\"\"\"\n\timport matplotlib.colors as mc\n\timport colorsys\n\ttry:\n\t\tc = mc.cnames[color]\n\texcept:\n\t\tc = color\n\tc = colorsys.rgb_to_hls(*mc.to_rgb(c))\n\treturn colorsys.hls_to_rgb(c[0], 1 - amount * (1 - c[1]), c[2])\n\n\ndef generateColorFunction(originalColor):\n\tdef lighten_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n\t\tc = lighten_color(originalColor,font_size/200*0.6+0.2+0.4*random.random())\n\t\treturn (int(c[0]*255),int(c[1]*255),int(c[2]*255))\n\treturn lighten_color_func\n\n\ndef sortByFrequency(arr):\n\ts = set(arr)\n\tkeys = {n: (-arr.count(n), arr.index(n)) for n in s}\n\treturn sorted(list(s), key=lambda n: keys[n])\n \n\nimport math\nimport random\n\n_styleColors = [\"#1f77b4\",\"#ff7f0e\",\"#2ca02c\",\"#d62728\",\"#9467bd\",\"#8c564b\",\"#e377c2\",\"#7f7f7f\",\"#bcbd22\",\"#17becf\",\"#aec7e8\",\"#ffbb78\",\"#98df8a\",\"#ff9896\",\"#c5b0d5\",\"#c49c94\",\"#f7b6d2\",\"#c7c7c7\",\"#dbdb8d\",\"#9edae5\"];\n\nmaxInternalWords = 10000\nmaxAllWords = 10000\nmaxCommunities = 6\n\ncommunities = g.vs[\"Community\"]\nsortedCommunities = sortByFrequency(communities)[0:maxCommunities]\nfig = plt.figure(figsize=(20,5*math.ceil(len(sortedCommunities)/2)))\nallTitles = \"\\n\".join(g.vs[\"papertitle\"])\nallFrequencies = WordCloud(max_words=maxAllWords).process_text(allTitles)\namask = np.zeros((500,1000),dtype='B')\namask[:10,:] = 255\namask[-10:,:] = 255\namask[:,:10] = 255\namask[:,-10:] = 255\nfor index,community in enumerate(sortedCommunities):\n\tcommunityColor = (_styleColors[index] if index&lt;len(_styleColors) else \"#aaaaaa\")\n\ttitles = \"\\n\".join([vertex[\"papertitle\"] for vertex in g.vs if vertex[\"Community\"]==community])\n\tplt.subplot(math.ceil(len(sortedCommunities)/2),2,index+1)\n\twc = WordCloud(background_color=\"white\", max_words=maxInternalWords, width=1000,height=500,\n\t\tmask=amask,contour_width=10, contour_color=communityColor,random_state=3,color_func=generateColorFunction(communityColor))\n\ttotalTitles = len(titles.split(\"\\n\"))\n\tinCommunityFrequency = wc.process_text(titles)\n\trelativeFrequencies = {key:frequency/totalTitles/(allFrequencies[key]-frequency+1) for key,frequency in inCommunityFrequency.items() if key in allFrequencies and allFrequencies[key]&gt;frequency}\n\twc.generate_from_frequencies(relativeFrequencies)\n\t\n\tplt.imshow(wc, interpolation='bilinear')\n\tplt.axis(\"off\")\n\t\t\nplt.tight_layout()\n# plt.savefig(\"wordcloud.pdf\")\nplt.show()\n# plt.close(fig)\n</pre>  def lighten_color(color, amount=0.5): \t\"\"\" \tLightens the given color by multiplying (1-luminosity) by the given amount. \tInput can be matplotlib color string, hex string, or RGB tuple. \t \tExamples: \t&gt;&gt; lighten_color('g', 0.3) \t&gt;&gt; lighten_color('#F034A3', 0.6) \t&gt;&gt; lighten_color((.3,.55,.1), 0.5) \t\"\"\" \timport matplotlib.colors as mc \timport colorsys \ttry: \t\tc = mc.cnames[color] \texcept: \t\tc = color \tc = colorsys.rgb_to_hls(*mc.to_rgb(c)) \treturn colorsys.hls_to_rgb(c[0], 1 - amount * (1 - c[1]), c[2])   def generateColorFunction(originalColor): \tdef lighten_color_func(word, font_size, position, orientation, random_state=None,**kwargs): \t\tc = lighten_color(originalColor,font_size/200*0.6+0.2+0.4*random.random()) \t\treturn (int(c[0]*255),int(c[1]*255),int(c[2]*255)) \treturn lighten_color_func   def sortByFrequency(arr): \ts = set(arr) \tkeys = {n: (-arr.count(n), arr.index(n)) for n in s} \treturn sorted(list(s), key=lambda n: keys[n])    import math import random  _styleColors = [\"#1f77b4\",\"#ff7f0e\",\"#2ca02c\",\"#d62728\",\"#9467bd\",\"#8c564b\",\"#e377c2\",\"#7f7f7f\",\"#bcbd22\",\"#17becf\",\"#aec7e8\",\"#ffbb78\",\"#98df8a\",\"#ff9896\",\"#c5b0d5\",\"#c49c94\",\"#f7b6d2\",\"#c7c7c7\",\"#dbdb8d\",\"#9edae5\"];  maxInternalWords = 10000 maxAllWords = 10000 maxCommunities = 6  communities = g.vs[\"Community\"] sortedCommunities = sortByFrequency(communities)[0:maxCommunities] fig = plt.figure(figsize=(20,5*math.ceil(len(sortedCommunities)/2))) allTitles = \"\\n\".join(g.vs[\"papertitle\"]) allFrequencies = WordCloud(max_words=maxAllWords).process_text(allTitles) amask = np.zeros((500,1000),dtype='B') amask[:10,:] = 255 amask[-10:,:] = 255 amask[:,:10] = 255 amask[:,-10:] = 255 for index,community in enumerate(sortedCommunities): \tcommunityColor = (_styleColors[index] if indexfrequency} \twc.generate_from_frequencies(relativeFrequencies) \t \tplt.imshow(wc, interpolation='bilinear') \tplt.axis(\"off\") \t\t plt.tight_layout() # plt.savefig(\"wordcloud.pdf\") plt.show() # plt.close(fig) <p>Let's now visualize the network with colors indicating the communities. We will use the LGL (Large Graph Layout) algorithm for layout, which is suitable for large graphs. It will take some time to compute the layout for large graphs.</p> In\u00a0[\u00a0]: Copied! <pre># color countries\nfrom collections import Counter\ncommunity2Index = {community:index for index,(community,_) in enumerate(Counter(g.vs[\"Community\"]).most_common(10))}\ncommunityColors = [mpl.cm.tab10(community2Index[community]) if community in community2Index else \"#cccccc\" for community in g.vs[\"Community\"]]\n# use lgl layout\nlayout = g.layout(\"lgl\")\n# plot the graph\nplot_igraph_with_matplotlib(\n    g,\n    layout,\n    node_size=20,\n    node_color=communityColors,\n    edge_color=\"gray\",\n    edge_alpha=0.05,\n    with_labels=False,\n    figsize=(12, 12),\n    dpi=100\n)\n</pre> # color countries from collections import Counter community2Index = {community:index for index,(community,_) in enumerate(Counter(g.vs[\"Community\"]).most_common(10))} communityColors = [mpl.cm.tab10(community2Index[community]) if community in community2Index else \"#cccccc\" for community in g.vs[\"Community\"]] # use lgl layout layout = g.layout(\"lgl\") # plot the graph plot_igraph_with_matplotlib(     g,     layout,     node_size=20,     node_color=communityColors,     edge_color=\"gray\",     edge_alpha=0.05,     with_labels=False,     figsize=(12, 12),     dpi=100 ) <p>You can also visualize these using Helios-Web, just save it as .xnet following the Visualize class materials.</p>"},{"location":"w12-networks/Communities/#community-detection-examples","title":"Community detection Examples\u00b6","text":"<p>Let's visualize the community structure of graphs using the Leiden algorithm built-in to <code>igraph</code>.</p>"},{"location":"w12-networks/FR-networkx/","title":"Fruchterman-Reingold algorithm","text":"<p>Let's import a few libraries.</p> In\u00a0[1]: Copied! <pre>import networkx as nx\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nimport igraph as ig\n\nfrom matplotlib.animation import FuncAnimation, writers\n%matplotlib widget\n</pre> import networkx as nx import matplotlib.pyplot as plt import numpy as np import math import igraph as ig  from matplotlib.animation import FuncAnimation, writers %matplotlib widget  <p>Let's create a geographic graph using the networkx library. This can be accomplished using the random_geometric_graph function. A geometric graph is a type of graph where nodes are placed in a geometric space, and edges are formed based on proximity.</p> In\u00a0[2]: Copied! <pre>#generating a geometric network\nnetwork = nx.random_geometric_graph(100,0.2)\n</pre> #generating a geometric network network = nx.random_geometric_graph(100,0.2)  <p>Let's convert it to igraph now.</p> In\u00a0[3]: Copied! <pre>g = ig.Graph.from_networkx(network)\n</pre> g = ig.Graph.from_networkx(network)  <p>Let's calculate the degrees and get the maximum degree of the graph. Let's get the number of nodes in the graph and the edges list.</p> In\u00a0[4]: Copied! <pre>degrees = np.array(g.degree())\nmaxDegrees = np.max(degrees)\nverticesCount = g.vcount()\nedges = g.get_edgelist()\n</pre>  degrees = np.array(g.degree()) maxDegrees = np.max(degrees) verticesCount = g.vcount() edges = g.get_edgelist()  <p>Let's define a few parameters for the Fruchterman-Reingold algorithm.</p> In\u00a0[5]: Copied! <pre>iterations = 500 # number of iterations to run the simulation\nviewSize = 50 # size of the view area\nviscosity = 0.15 # viscosity coefficient to reduce velocities (like water resistance)\nalpha = 0.5 # minimum distance to avoid division by zero in forces calculation\na = 0.001 # coefficient for attractive forces\nb = 1.0 # coefficient for repulsive forces\ndeltaT = 1.0 # time step for each iteration\n</pre> iterations = 500 # number of iterations to run the simulation viewSize = 50 # size of the view area viscosity = 0.15 # viscosity coefficient to reduce velocities (like water resistance) alpha = 0.5 # minimum distance to avoid division by zero in forces calculation a = 0.001 # coefficient for attractive forces b = 1.0 # coefficient for repulsive forces deltaT = 1.0 # time step for each iteration <p>We can initialize the positions of the nodes randomly according to the viewSize. Let's also initialize the velocities of the nodes to zero.</p> In\u00a0[10]: Copied! <pre>def resetPositions():\n    global positionsX, positionsY, velocitiesX, velocitiesY\n    positionsX = viewSize*np.random.random(verticesCount)-viewSize/2.0\n    positionsY = viewSize*np.random.random(verticesCount)-viewSize/2.0\n    velocitiesX = np.zeros(verticesCount)\n    velocitiesY = np.zeros(verticesCount)\n\nresetPositions()\n</pre> def resetPositions():     global positionsX, positionsY, velocitiesX, velocitiesY     positionsX = viewSize*np.random.random(verticesCount)-viewSize/2.0     positionsY = viewSize*np.random.random(verticesCount)-viewSize/2.0     velocitiesX = np.zeros(verticesCount)     velocitiesY = np.zeros(verticesCount)  resetPositions()  <p>So this is the code that iterates through the nodes and updates their positions based on the forces acting on them. The forces are calculated based on the repulsion between nodes and the attraction between connected nodes.</p> <p>We are using a very simple Euler integration method to update the positions of the nodes. The positions are updated based on the velocities, and the velocities are updated based on the forces acting on the nodes.</p> <p>The next position of node $i$ is defined as:</p> <p>$x_i(t+1) = x_i(t) + v_i(t+1) \\Delta t$</p> <p>where $\\Delta t$ is the time step, and $v_i(t)$ is the velocity at time $t$. The velocity is updated based on the forces acting on the nodes, which are calculated based on the repulsion and attraction forces.</p> <p>First we calculate the total force acting on each node, $F_i(t)$, which is the sum of the repulsion forces from all other nodes and the attraction forces from connected nodes. The repulsion force is inversely proportional to the square of the distance between nodes, while the attraction force is proportional to the distance between connected nodes.</p> <p>We calculate the velocities as:</p> <p>$v_i(t+1) = (1-\\gamma) v_i(t) + F_i(t+1) \\Delta t $</p> <p>where $\\gamma$ is a viscosity factor that reduces the velocity over time to prevent oscillations. The viscosity factor can be set to a small value, such as 0.1.</p> In\u00a0[11]: Copied! <pre>def iterate(iterationCount):\n    global positionsX,positionsY,velocitiesX,velocitiesY\n    for iteration in range(iterationCount):\n        forcesX = np.zeros(verticesCount)\n        forcesY = np.zeros(verticesCount)\n        #repulstive forces\n        for vertex1 in range(verticesCount):\n            for vertex2 in range(vertex1):\n                x1 = positionsX[vertex1]\n                y1 = positionsY[vertex1]\n                x2 = positionsX[vertex2]\n                y2 = positionsY[vertex2]\n                distance = math.sqrt((x2-x1)*(x2-x1) + (y2-y1)*(y2-y1)) + alpha\n                rx = (x2-x1)/distance\n                ry = (y2-y1)/distance\n                Fx = -b*rx/distance/distance\n                Fy = -b*ry/distance/distance\n                forcesX[vertex1] += Fx\n                forcesY[vertex1] += Fy\n                forcesX[vertex2] -= Fx\n                forcesY[vertex2] -= Fy\n        #attractive forces\n        for vFrom,vTo in edges:\n            x1 = positionsX[vFrom]\n            y1 = positionsY[vFrom]\n            x2 = positionsX[vTo]\n            y2 = positionsY[vTo]\n            distance = math.sqrt((x2-x1)*(x2-x1) + (y2-y1)*(y2-y1))\n            Rx = (x2-x1)\n            Ry = (y2-y1)\n            Fx = a*Rx*distance\n            Fy = a*Ry*distance\n            forcesX[vFrom] += Fx\n            forcesY[vFrom] += Fy\n            forcesX[vTo] -= Fx\n            forcesY[vTo] -= Fy\n            \n        # apply forces to velocities\n        velocitiesX+=forcesX*deltaT\n        velocitiesX*=(1.0-viscosity)\n        velocitiesY+=forcesY*deltaT\n        velocitiesY*=(1.0-viscosity)\n        # update positions\n        positionsX += velocitiesX*deltaT\n        positionsY += velocitiesY*deltaT\n</pre> def iterate(iterationCount):     global positionsX,positionsY,velocitiesX,velocitiesY     for iteration in range(iterationCount):         forcesX = np.zeros(verticesCount)         forcesY = np.zeros(verticesCount)         #repulstive forces         for vertex1 in range(verticesCount):             for vertex2 in range(vertex1):                 x1 = positionsX[vertex1]                 y1 = positionsY[vertex1]                 x2 = positionsX[vertex2]                 y2 = positionsY[vertex2]                 distance = math.sqrt((x2-x1)*(x2-x1) + (y2-y1)*(y2-y1)) + alpha                 rx = (x2-x1)/distance                 ry = (y2-y1)/distance                 Fx = -b*rx/distance/distance                 Fy = -b*ry/distance/distance                 forcesX[vertex1] += Fx                 forcesY[vertex1] += Fy                 forcesX[vertex2] -= Fx                 forcesY[vertex2] -= Fy         #attractive forces         for vFrom,vTo in edges:             x1 = positionsX[vFrom]             y1 = positionsY[vFrom]             x2 = positionsX[vTo]             y2 = positionsY[vTo]             distance = math.sqrt((x2-x1)*(x2-x1) + (y2-y1)*(y2-y1))             Rx = (x2-x1)             Ry = (y2-y1)             Fx = a*Rx*distance             Fy = a*Ry*distance             forcesX[vFrom] += Fx             forcesY[vFrom] += Fy             forcesX[vTo] -= Fx             forcesY[vTo] -= Fy                      # apply forces to velocities         velocitiesX+=forcesX*deltaT         velocitiesX*=(1.0-viscosity)         velocitiesY+=forcesY*deltaT         velocitiesY*=(1.0-viscosity)         # update positions         positionsX += velocitiesX*deltaT         positionsY += velocitiesY*deltaT  <p>Let's use matplotlib to visualize the graph. We will use the positions calculated by the Fruchterman-Reingold algorithm to draw the graph. We will use animation API to see the graph evolve over time.</p> In\u00a0[13]: Copied! <pre>from IPython.display import HTML, clear_output\nfrom matplotlib import rc\nfrom matplotlib.animation import FuncAnimation\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# make sure the JSHTML repr is enabled\nrc('animation', html='jshtml')\n\nresetPositions()  # reset positions before starting the animation\n\ndef displayEdges(edges, positionsX, positionsY):\n    linesX, linesY = [], []\n    for u, v in edges:\n        fx, fy = positionsX[u], positionsY[u]\n        tx, ty = positionsX[v], positionsY[v]\n        linesX += [fx, tx, None]\n        linesY += [fy, ty, None]\n    return linesX, linesY\n\ndef animate_graph(edges, positionsX, positionsY, degrees, maxDegrees,\n                  viewSize, iterate, framesCount=200):\n    # 1) clear previous Jupyter output\n    clear_output(wait=True)\n\n    # 2) create a fresh figure/axes\n    fig, ax = plt.subplots(figsize=(6,6))\n    ax.set_aspect('equal')\n    ax.set_xlim(-viewSize, viewSize)\n    ax.set_ylim(-viewSize, viewSize)\n\n    # initial drawing\n    linesX, linesY = displayEdges(edges, positionsX, positionsY)\n    pltEdges,   = ax.plot(linesX, linesY,   linewidth=1, color='#999999')\n    pltNodes = ax.scatter(positionsX, positionsY,\n                          s=degrees/maxDegrees*100 + 5,\n                          c='#222222', zorder=5)\n\n    def update(frame):\n        iterate(1)  # advance your simulation\n        lx, ly = displayEdges(edges, positionsX, positionsY)\n        pltEdges.set_data(lx, ly)\n        newPos = np.column_stack((positionsX, positionsY))\n        pltNodes.set_offsets(newPos)\n\n        # if your layout grows/shrinks over time:\n        ax.set_xlim(min(np.min(positionsX), -viewSize),\n                    max(np.max(positionsX),  viewSize))\n        ax.set_ylim(min(np.min(positionsY), -viewSize),\n                    max(np.max(positionsY),  viewSize))\n        return pltEdges, pltNodes  # needed for blitting\n\n    ani = FuncAnimation(fig, update, frames=framesCount,\n                        interval=33, blit=True)\n\n    # embed the animation, then close the figure so it won't persist\n    display(HTML(ani.to_jshtml()))\n    plt.close(fig)\n\n# finally, call it:\nanimate_graph(edges, positionsX, positionsY,\n              degrees, maxDegrees,\n              viewSize, iterate, framesCount=200)\n</pre> from IPython.display import HTML, clear_output from matplotlib import rc from matplotlib.animation import FuncAnimation import matplotlib.pyplot as plt import numpy as np  # make sure the JSHTML repr is enabled rc('animation', html='jshtml')  resetPositions()  # reset positions before starting the animation  def displayEdges(edges, positionsX, positionsY):     linesX, linesY = [], []     for u, v in edges:         fx, fy = positionsX[u], positionsY[u]         tx, ty = positionsX[v], positionsY[v]         linesX += [fx, tx, None]         linesY += [fy, ty, None]     return linesX, linesY  def animate_graph(edges, positionsX, positionsY, degrees, maxDegrees,                   viewSize, iterate, framesCount=200):     # 1) clear previous Jupyter output     clear_output(wait=True)      # 2) create a fresh figure/axes     fig, ax = plt.subplots(figsize=(6,6))     ax.set_aspect('equal')     ax.set_xlim(-viewSize, viewSize)     ax.set_ylim(-viewSize, viewSize)      # initial drawing     linesX, linesY = displayEdges(edges, positionsX, positionsY)     pltEdges,   = ax.plot(linesX, linesY,   linewidth=1, color='#999999')     pltNodes = ax.scatter(positionsX, positionsY,                           s=degrees/maxDegrees*100 + 5,                           c='#222222', zorder=5)      def update(frame):         iterate(1)  # advance your simulation         lx, ly = displayEdges(edges, positionsX, positionsY)         pltEdges.set_data(lx, ly)         newPos = np.column_stack((positionsX, positionsY))         pltNodes.set_offsets(newPos)          # if your layout grows/shrinks over time:         ax.set_xlim(min(np.min(positionsX), -viewSize),                     max(np.max(positionsX),  viewSize))         ax.set_ylim(min(np.min(positionsY), -viewSize),                     max(np.max(positionsY),  viewSize))         return pltEdges, pltNodes  # needed for blitting      ani = FuncAnimation(fig, update, frames=framesCount,                         interval=33, blit=True)      # embed the animation, then close the figure so it won't persist     display(HTML(ani.to_jshtml()))     plt.close(fig)  # finally, call it: animate_graph(edges, positionsX, positionsY,               degrees, maxDegrees,               viewSize, iterate, framesCount=200) Once Loop Reflect <p>Cool right? However this approach is not very efficient for large graphs, as it requires iterating through all pairs of nodes to calculate the forces. For larger graphs, more efficient algorithms such as the Barnes-Hut algorithm. Most of the libraries implementing the Fruchterman-Reingold algorithm use these optimizations to speed up the process.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"w12-networks/FR-networkx/#fruchterman-reingold-algorithm","title":"Fruchterman-Reingold algorithm\u00b6","text":"<p>In this notebook, we will implement a version of the Fruchterman-Reingold algorithm just for fun in python. The Fruchterman-Reingold algorithm is a force-directed algorithm for drawing graphs, which positions nodes in a way that visually represents the structure of the graph.</p> <p>This also demonstrates how to import networkx networks into igraph.</p> <p></p>"},{"location":"w12-networks/FR-networkx/#exercises","title":"Exercises\u00b6","text":"<p>Create a new version of the forces in which the degrees are part of the forces.</p>"},{"location":"w12-networks/Visualize/","title":"Visualizing networks","text":"In\u00a0[1]: Copied! <pre>import igraph as ig\nimport numpy as np\n</pre> import igraph as ig import numpy as np <p>Now let's load one of the networks in the Datasets/Networks folder. Let's load the EUPowerGrid network, which is a network of the European power grid.</p> <p>We are going to use the <code>simplify()</code> function to remove self-loops and multiple edges, which is a common preprocessing step for network analysis.</p> In\u00a0[2]: Copied! <pre># Loading EUPowerGrid network\ng_power = ig.Graph.Read_GML(\"../../Datasets/Networks/EUPowerGrid.gml\").simplify() # adjust the path as needed.\n</pre> # Loading EUPowerGrid network g_power = ig.Graph.Read_GML(\"../../Datasets/Networks/EUPowerGrid.gml\").simplify() # adjust the path as needed. <p>Let's see the attributes of the network:</p> In\u00a0[3]: Copied! <pre># Node attributes\nprint(g_power.vertex_attributes())\n</pre> # Node attributes print(g_power.vertex_attributes()) <pre>['id', 'name', 'Country']\n</pre> <p>We can calculate the layout using the layout function. Let's start with the circle layout with the order defined by the degree of the nodes.</p> <p>Note that the default plot function from igraph needs you to install cairo. However this library can be a bit tricky to install on some systems. So I will offer here an alternative way to plot the network using matplotlib.</p> In\u00a0[7]: Copied! <pre># Size of node changes with degree\nnode_degrees = np.array(g_power.degree())\nlayout = g_power.layout(\"circle\",order=node_degrees.argsort())\n# Plotting using the cairo backend (requires cairo installed)\n# ig.plot(g_power,\n#     layout=layout, \n#     vertex_size=node_degrees,\n#     )\n</pre> # Size of node changes with degree node_degrees = np.array(g_power.degree()) layout = g_power.layout(\"circle\",order=node_degrees.argsort()) # Plotting using the cairo backend (requires cairo installed) # ig.plot(g_power, #     layout=layout,  #     vertex_size=node_degrees, #     ) <p>Here is a nice matplotlib function to plot the network.</p> In\u00a0[8]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_igraph_with_matplotlib(\n    g,\n    layout,\n    node_size=300,\n    node_color=\"skyblue\",\n    node_alpha=1.0,\n    edge_color=\"gray\",\n    edge_width=1.0,\n    edge_alpha=0.7,\n    with_labels=False,\n    label_attr=\"label\",\n    label_font_size=10,\n    label_color=\"black\",\n    figsize=(6, 6),\n    dpi=100,\n    axis_off=True\n):\n    \"\"\"\n    Draw an igraph Graph `g` using matplotlib, given `layout`.\n    \n    Parameters\n    ----------\n    g : igraph.Graph\n      The graph to draw.\n    layout : Layout or sequence of (x, y)\n      An igraph Layout object or list/array of coordinate pairs.\n    node_size : float\n      Size of the nodes (passed to plt.scatter `s`).\n    node_color : color or list of colors\n      Node face color.\n    node_alpha : float\n      Node transparency (0.0 transparent, 1.0 opaque).\n    edge_color : color\n      Color for all edges.\n    edge_width : float\n      Line width for edges.\n    edge_alpha : float\n      Edge transparency.\n    with_labels : bool\n      Whether to draw vertex labels.\n    label_attr : str\n      Vertex attribute name to use for labels; if absent, vertex indices used.\n    label_font_size : float\n      Font size for labels.\n    label_color : color\n      Color for label text.\n    figsize : tuple\n      Matplotlib figure size.\n    dpi : int\n      Figure DPI.\n    axis_off : bool\n      If True, hides axes.\n    \"\"\"\n    # extract coordinates as an (N,2) array\n    coords = np.array(layout.coords) if hasattr(layout, \"coords\") else np.array(layout)\n    xs, ys = coords[:, 0], coords[:, 1]\n\n    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n\n    # draw edges\n    for src, tgt in g.get_edgelist():\n        x0, y0 = coords[src]\n        x1, y1 = coords[tgt]\n        ax.plot(\n            [x0, x1],\n            [y0, y1],\n            color=edge_color,\n            linewidth=edge_width,\n            alpha=edge_alpha,\n            zorder=1\n        )\n\n    # draw nodes\n    ax.scatter(\n        xs,\n        ys,\n        s=node_size,\n        c=node_color,\n        alpha=node_alpha,\n        zorder=2,\n        edgecolors=\"black\"\n    )\n\n    # draw labels\n    if with_labels:\n        if label_attr in g.vs.attribute_names():\n            labels = g.vs[label_attr]\n        else:\n            labels = [str(i) for i in range(g.vcount())]\n        for idx, label in enumerate(labels):\n            ax.text(\n                xs[idx],\n                ys[idx],\n                label,\n                fontsize=label_font_size,\n                color=label_color,\n                ha=\"center\",\n                va=\"center\",\n                zorder=3\n            )\n\n    if axis_off:\n        ax.set_axis_off()\n\n    plt.tight_layout()\n    plt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  def plot_igraph_with_matplotlib(     g,     layout,     node_size=300,     node_color=\"skyblue\",     node_alpha=1.0,     edge_color=\"gray\",     edge_width=1.0,     edge_alpha=0.7,     with_labels=False,     label_attr=\"label\",     label_font_size=10,     label_color=\"black\",     figsize=(6, 6),     dpi=100,     axis_off=True ):     \"\"\"     Draw an igraph Graph `g` using matplotlib, given `layout`.          Parameters     ----------     g : igraph.Graph       The graph to draw.     layout : Layout or sequence of (x, y)       An igraph Layout object or list/array of coordinate pairs.     node_size : float       Size of the nodes (passed to plt.scatter `s`).     node_color : color or list of colors       Node face color.     node_alpha : float       Node transparency (0.0 transparent, 1.0 opaque).     edge_color : color       Color for all edges.     edge_width : float       Line width for edges.     edge_alpha : float       Edge transparency.     with_labels : bool       Whether to draw vertex labels.     label_attr : str       Vertex attribute name to use for labels; if absent, vertex indices used.     label_font_size : float       Font size for labels.     label_color : color       Color for label text.     figsize : tuple       Matplotlib figure size.     dpi : int       Figure DPI.     axis_off : bool       If True, hides axes.     \"\"\"     # extract coordinates as an (N,2) array     coords = np.array(layout.coords) if hasattr(layout, \"coords\") else np.array(layout)     xs, ys = coords[:, 0], coords[:, 1]      fig, ax = plt.subplots(figsize=figsize, dpi=dpi)      # draw edges     for src, tgt in g.get_edgelist():         x0, y0 = coords[src]         x1, y1 = coords[tgt]         ax.plot(             [x0, x1],             [y0, y1],             color=edge_color,             linewidth=edge_width,             alpha=edge_alpha,             zorder=1         )      # draw nodes     ax.scatter(         xs,         ys,         s=node_size,         c=node_color,         alpha=node_alpha,         zorder=2,         edgecolors=\"black\"     )      # draw labels     if with_labels:         if label_attr in g.vs.attribute_names():             labels = g.vs[label_attr]         else:             labels = [str(i) for i in range(g.vcount())]         for idx, label in enumerate(labels):             ax.text(                 xs[idx],                 ys[idx],                 label,                 fontsize=label_font_size,                 color=label_color,                 ha=\"center\",                 va=\"center\",                 zorder=3             )      if axis_off:         ax.set_axis_off()      plt.tight_layout()     plt.show() In\u00a0[11]: Copied! <pre># Size of node changes with degree\nnode_degrees = np.array(g_power.degree())\nlayout = g_power.layout(\"circle\",order=node_degrees.argsort())\n# using the custom plotting function\nplot_igraph_with_matplotlib(\n    g_power,\n    layout=layout,\n    node_size=node_degrees * 20,  # Scale node size by degree\n    edge_alpha=0.05\n)\n</pre> # Size of node changes with degree node_degrees = np.array(g_power.degree()) layout = g_power.layout(\"circle\",order=node_degrees.argsort()) # using the custom plotting function plot_igraph_with_matplotlib(     g_power,     layout=layout,     node_size=node_degrees * 20,  # Scale node size by degree     edge_alpha=0.05 ) <p>Let's try another layout, the Fruchterman-Reingold layout, which is a force-directed layout. This layout is often used for visualizing networks as it tends to spread out the nodes nicely. We can color the nodes based on the country they belong to.</p> In\u00a0[18]: Copied! <pre># color countries\nfrom collections import Counter\nimport matplotlib as mpl\ncountry2Index = {country:index for index,(country,_) in enumerate(Counter(g_power.vs[\"Country\"]).most_common(10))}\ncountryColors = [mpl.cm.tab10(country2Index[country]) if country in country2Index else \"#888888\" for country in g_power.vs[\"Country\"]]\n# Size of node changes with degree\nnode_degrees = np.array(g_power.degree())\nlayout = g_power.layout(\"fruchterman_reingold\", niter= 2000)\n\n# Other supported layouts: drl, davidson_harel, circle, kamada_kawai, fruchterman_reingold, graphopt, mds\n\n# ig.plot(g_power,\n#     layout=layout,\n#     vertex_size=node_degrees,\n#     vertex_color=countryColors\n#     )\n\n# custom plotting with matplotlib\nplot_igraph_with_matplotlib(\n    g_power,\n    layout=layout,\n    node_size=node_degrees * 20,  # Scale node size by degree\n    node_color=countryColors,\n    figsize=(10, 10)\n)\n</pre> # color countries from collections import Counter import matplotlib as mpl country2Index = {country:index for index,(country,_) in enumerate(Counter(g_power.vs[\"Country\"]).most_common(10))} countryColors = [mpl.cm.tab10(country2Index[country]) if country in country2Index else \"#888888\" for country in g_power.vs[\"Country\"]] # Size of node changes with degree node_degrees = np.array(g_power.degree()) layout = g_power.layout(\"fruchterman_reingold\", niter= 2000)  # Other supported layouts: drl, davidson_harel, circle, kamada_kawai, fruchterman_reingold, graphopt, mds  # ig.plot(g_power, #     layout=layout, #     vertex_size=node_degrees, #     vertex_color=countryColors #     )  # custom plotting with matplotlib plot_igraph_with_matplotlib(     g_power,     layout=layout,     node_size=node_degrees * 20,  # Scale node size by degree     node_color=countryColors,     figsize=(10, 10) )  In\u00a0[20]: Copied! <pre>import xnetwork as xn\nxn.save(g_power, \"EUPowerGrid.xnet\")\n</pre> import xnetwork as xn xn.save(g_power, \"EUPowerGrid.xnet\") <p>Now open a browser window and drag and drop the .xnet file into the Helios-Web interface. From the bottom menus you can select different attributes.</p> <p>go to: https://heliosweb.io/docs/example/?advanced&amp;dark</p> <p>We are still working on the Helios-Web interface, so there are no controllers for the layout yet. If you know javascript, you can use helios-web as a library to create your own visualizations.</p> <p>More information on how to use Helios-Web can be found in the Helios-Web documentation.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Load a network from the Networks folder, explore it, and create a visualization</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"w12-networks/Visualize/#visualizing-networks","title":"Visualizing networks\u00b6","text":"<p>Let's showcase a few ways you can visualize networks using the igraph library. Let's first import a few libraries:</p>"},{"location":"w12-networks/Visualize/#using-helios-web","title":"Using Helios-Web\u00b6","text":"<p>You can try other layouts on igraph. However if you want to try more advanced visualizations, you can use Helios-Web. Helios-Web is a web-based visualization tool that allows you to visualize networks in a more interactive way. You can upload your network and try different layouts and styles.</p> <p>First you need to save the network to the .xnet format or .gml format.</p> <p>You may need to install the <code>xnetwork</code> package to save the network in the .xnet format. You can install it using pip:</p> <pre>pip install xnetwork\n</pre>"},{"location":"w12-networks/Visualize/#exercises","title":"Exercises\u00b6","text":"<p>Find the best layout for the EU power grid network.</p> <p>Hint: Check documentation for layouts in https://igraph.org/python/api/latest/igraph._igraph.GraphBase.html#layout_fruchterman_reingold</p>"},{"location":"w12-networks/assignment_w12_networks/","title":"Homework 12 - Networks","text":"In\u00a0[\u00a0]: Copied! <pre># YOUR CODE STARTS HERE\n</pre> # YOUR CODE STARTS HERE <p><code>YOUR DISCUSSION HERE</code></p>"},{"location":"w12-networks/assignment_w12_networks/#homework-12-networks","title":"Homework 12 - Networks\u00b6","text":"<p>In this homework, create a visualization of a network of your choice. You are free to use any software or library to create a simple network visualization. You don't need to use this notebook or even Python, but you can if you want. The goal is to explore the data and find interesting patterns or clusters visually. If you need references, you can look at our class hands-on materials.</p>"},{"location":"w12-networks/assignment_w12_networks/#instructions","title":"Instructions\u00b6","text":"<ol> <li><p>Project Setup:</p> <ul> <li>Choose a software or library for network visualization. Some popular options include:<ul> <li>Python libraries: igraph, NetworkX, Matplotlib</li> <li>JavaScript libraries: D3.js, Sigma.js</li> <li>Other tools: Gephi, Cytoscape</li> </ul> </li> </ul> </li> <li><p>Choose a Dataset:</p> <ul> <li>Choose a network dataset that interests you. Or any of the datasets provided in the <code>Datasets/Networks</code> folder in this repository.</li> </ul> </li> <li><p>Convert to the desired network format if needed:</p> <ul> <li>If your dataset is not already in a network format (e.g., edge list, adjacency matrix), convert it to the appropriate format. You can use libraries like Pandas to manipulate your data into the required structure. You can also use <code>igraph</code> or <code>NetworkX</code> to create the network from your data.</li> </ul> </li> <li><p>Create at least one visualization of that network:</p> <ul> <li>You can create a simple visualization using the chosen library or software.</li> </ul> </li> <li><p>Documentation and Discussion:</p> <ul> <li>Comment your code and add markdown explanations for each part of your analysis. Discuss the insights you gained from the visualizations. If you do not find any insights, explain the limitations of the dataset or the visualization techniques used.</li> </ul> </li> <li><p>Submission:</p> <ul> <li>Ensure your notebook is complete and all cells are executed without errors.</li> <li>Save your notebook and export as either PDF or HTML. If the visualizations using altair are not being shown in the html, submit a separated version with altair html. Refer to: https://altair-viz.github.io/getting_started/starting.html#publishing-your-visualization (you can use the <code>chart.save('chart_file.html')</code> method).</li> <li>Submit to Canvas.</li> </ul> </li> </ol>"},{"location":"w12-networks/assignment_w12_networks/#discussion","title":"Discussion\u00b6","text":"<p>Discuss briefly the results of the dimension reduction methods you applied. What do you observe? Do the reduced dimensions capture any structure of the data? How do the two methods compare? Are there any interesting patterns or clusters in the data that can be observed visually?</p>"},{"location":"w12-networks/igraph-basics/","title":"igraph basics (python library)","text":"In\u00a0[2]: Copied! <pre>import igraph as ig\n</pre> import igraph as ig <p>Let's generate a random graph using the Erdos-Renyi model, which is a simple model for generating random graphs. In this model, we specify the number of vertices and the probability of an edge between any two vertices.</p> In\u00a0[4]: Copied! <pre># Generating a random network\ng_random = ig.Graph.Erdos_Renyi(n=15, p=0.2, directed=False, loops=False)\n</pre> # Generating a random network g_random = ig.Graph.Erdos_Renyi(n=15, p=0.2, directed=False, loops=False) <p>Cool, now we have a random graph with 15 vertices/nodes and a probability of connection of 0.3 between any two vertices. Let's see what igraph prints.</p> In\u00a0[7]: Copied! <pre>print(g_random)\n</pre> print(g_random) <pre>IGRAPH U--- 15 20 --\n+ edges:\n 0 --  2  4 11 13 14    5 --  7               10 --\n 1 --  2  6  7          6 --  1  2  8 11      11 --  0  6\n 2 --  0  1  6  9       7 --  1  5 12 14      12 --  4  7 13\n 3 --                   8 --  4  6            13 --  0  9 12\n 4 --  0  8  9 12       9 --  2  4 13         14 --  0  7\n</pre> <p>Ok it is difficult to understand. But internally igraph uses a adjacency list to represent the graph. Let's print the adjacency list of the graph. The first two numbers are the number of vertices and edges, followed by the adjacency list.</p> <p>For instance an entry <code>0 --  2  4 11 13 14</code> means that vertex 0 is connected to vertices 2, 4, 11, 13, and 14.</p> <p>We can access the nodes and edges of the graph using the <code>vs</code> and <code>es</code> attributes, respectively. A node may have attributes associated with it. One of the attributes that is built-in is <code>index</code>, which is the index of the node in the graph. We can access it like this:</p> In\u00a0[9]: Copied! <pre># Nodes\n[node.index for node in g_random.vs]\n</pre> # Nodes [node.index for node in g_random.vs] Out[9]: <pre>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</pre> <p>We can access the edges of the graph using the <code>es</code> attribute. Each edge has an source and target vertex, which can be accessed using the <code>source</code> and <code>target</code> attributes.</p> In\u00a0[11]: Copied! <pre># Edges list\n[(edge.source, edge.target) for edge in g_random.es]\n</pre> # Edges list [(edge.source, edge.target) for edge in g_random.es] Out[11]: <pre>[(0, 2),\n (1, 2),\n (0, 4),\n (1, 6),\n (2, 6),\n (1, 7),\n (5, 7),\n (4, 8),\n (6, 8),\n (2, 9),\n (4, 9),\n (0, 11),\n (6, 11),\n (4, 12),\n (7, 12),\n (0, 13),\n (9, 13),\n (12, 13),\n (0, 14),\n (7, 14)]</pre> <p>or simply:</p> In\u00a0[12]: Copied! <pre>g_random.get_edgelist()\n</pre> g_random.get_edgelist() Out[12]: <pre>[(0, 2),\n (1, 2),\n (0, 4),\n (1, 6),\n (2, 6),\n (1, 7),\n (5, 7),\n (4, 8),\n (6, 8),\n (2, 9),\n (4, 9),\n (0, 11),\n (6, 11),\n (4, 12),\n (7, 12),\n (0, 13),\n (9, 13),\n (12, 13),\n (0, 14),\n (7, 14)]</pre> <p>There are several methods to calculate properties of the graph, such as the degree of each vertex. The degree of a vertex is the number of edges connected to it. We can calculate the degree of each vertex using the <code>degree</code> method:</p> In\u00a0[13]: Copied! <pre># Calculate degree of nodes\ng_random.degree()\n</pre> # Calculate degree of nodes g_random.degree() Out[13]: <pre>[5, 3, 4, 0, 4, 1, 4, 4, 2, 3, 0, 2, 3, 3, 2]</pre> <p>You can also set a node attribute using the <code>vs</code> attribute. For example, let's store the double of the degree of each vertex as an attribute called <code>degree doubled</code>:</p> In\u00a0[14]: Copied! <pre># Setting node attributes\ng_random.vs[\"degree doubled\"] = [2*d for d in g_random.degree()]\nprint(g_random)\n</pre> # Setting node attributes g_random.vs[\"degree doubled\"] = [2*d for d in g_random.degree()] print(g_random) <pre>IGRAPH U--- 15 20 --\n+ attr: degree doubled (v)\n+ edges:\n 0 --  2  4 11 13 14    5 --  7               10 --\n 1 --  2  6  7          6 --  1  2  8 11      11 --  0  6\n 2 --  0  1  6  9       7 --  1  5 12 14      12 --  4  7 13\n 3 --                   8 --  4  6            13 --  0  9 12\n 4 --  0  8  9 12       9 --  2  4 13         14 --  0  7\n</pre> <p>We can get the neighbors of a vertex using the <code>neighbors</code> method. For example, let's get the neighbors of vertex 0:</p> In\u00a0[15]: Copied! <pre># Getting neighbors of a node\ng_random.neighbors(0)\n</pre> # Getting neighbors of a node g_random.neighbors(0) Out[15]: <pre>[2, 4, 11, 13, 14]</pre> <p>igraph also works well with numpy arrays.</p> In\u00a0[16]: Copied! <pre>import numpy as np\ng_random.vs[\"random number\"] = np.random.random(g_random.vcount())\n</pre> import numpy as np g_random.vs[\"random number\"] = np.random.random(g_random.vcount()) <p>Attributes can also be set for edges. For example, let's set the weight of each edge to a random value between 0 and 10:</p> In\u00a0[17]: Copied! <pre># Setting an random weight attribute to edges\ng_random.es[\"weight\"] = 10*np.random.random(g_random.ecount())\nprint(g_random)\n</pre> # Setting an random weight attribute to edges g_random.es[\"weight\"] = 10*np.random.random(g_random.ecount()) print(g_random) <pre>IGRAPH U-W- 15 20 --\n+ attr: degree doubled (v), random number (v), weight (e)\n+ edges:\n 0 --  2  4 11 13 14    5 --  7               10 --\n 1 --  2  6  7          6 --  1  2  8 11      11 --  0  6\n 2 --  0  1  6  9       7 --  1  5 12 14      12 --  4  7 13\n 3 --                   8 --  4  6            13 --  0  9 12\n 4 --  0  8  9 12       9 --  2  4 13         14 --  0  7\n</pre> <p>One of the most used file formats for graphs is gml (Graph Modelling Language). We can save the graph to a gml file using the <code>write_gml</code> method:</p> In\u00a0[\u00a0]: Copied! <pre># Saving as gml\ng_random.write_gml(\"random_network.gml\")\n</pre> # Saving as gml g_random.write_gml(\"random_network.gml\") <p>And you can load it back using the <code>Graph.Read_GML</code> method:</p> In\u00a0[\u00a0]: Copied! <pre># Loading as gml\ng_random_loaded = ig.Graph.Read_GML(\"random_network.gml\")\nprint(g_random_loaded)\n</pre> # Loading as gml g_random_loaded = ig.Graph.Read_GML(\"random_network.gml\") print(g_random_loaded) <p>If you plan to use Helios-Web, you can use the xnetwork package to convert igraph graphs to xnetwork file format. Then you can go to heliosweb.io, select a network configuration and drag and drop the .xnet file there to visualize the graph.</p> In\u00a0[\u00a0]: Copied! <pre># Saving xnet (useful for the Helios-Web application)\nimport xnetwork as xn\nxn.save(g_random, \"random_network.xnet\")\n</pre> # Saving xnet (useful for the Helios-Web application) import xnetwork as xn xn.save(g_random, \"random_network.xnet\") <p>You can load the xnetwork file using <code>xn.load</code>:</p> In\u00a0[\u00a0]: Copied! <pre>g_random_from_xnet = xn.load(\"random_network.xnet\")\nprint(g_random_from_xnet)\n</pre> g_random_from_xnet = xn.load(\"random_network.xnet\") print(g_random_from_xnet) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Create a Barabasi-Albert network of similar size and degree, and plot the degree distrib. check https://igraph.readthedocs.io/ .</p> <p>Hint: <code>ig.Graph.Barabasi</code></p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Generate a plot of avg. clustering coefficient and shortest path length along p in [0,1] for a 1D Watts-Strogatz network. Use at least <code>nei=2</code>.</p> <p>hint: <code>ig.Graph.Watts_Strogatz</code></p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Choose a network or model and create a plot of degree (first level) and second level degree (number of connections from first neighborhood to the second)  (no need to optimize)</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"w12-networks/igraph-basics/#igraph-basics-python-library","title":"igraph basics (python library)\u00b6","text":"<p>iGraph is a library for creating and manipulating graphs and networks. It is widely used in network analysis, social network analysis, and graph theory.</p>"},{"location":"w12-networks/igraph-basics/#installation","title":"Installation\u00b6","text":"<p>To install the igraph library in Python, you can use pip:</p> <pre>pip install python-igraph\n</pre> <p>You can also check the igraph tutorial at https://igraph.org/python/doc/tutorial/tutorial.html</p> <p>Let's start with some basic examples of how to use igraph in Python. But first let's import it.</p>"},{"location":"w12-networks/igraph-basics/#exercises","title":"Exercises\u00b6","text":"<p>Create a larger random network and plot the degree distribution.</p> <p>Hint: <code>ig.Graph.Erdos_Renyi</code></p>"}]}